Common Market Law Review,http://arxiv.org/abs/physics/0612068v2,"Topological Properties of the Minimal Spanning Tree in Korean and
  American Stock Markets","We investigate a factor that can affect the number of links of a specific
stock in a network between stocks created by the minimal spanning tree (MST)
method, by using individual stock data listed on the S&P500 and KOSPI. Among
the common factors mentioned in the arbitrage pricing model (APM), widely
acknowledged in the financial field, a representative market index is
established as a possible factor. We found that the correlation distribution,
$\rho_{ij}$, of 400 stocks taken from the S&P500 index shows a very similar
with that of the Korean stock market and those deviate from the correlation
distribution of time series removed a nonlinearity by the surrogate method. We
also shows that the degree distribution of the MSTs for both stock markets
follows a power-law distribution with the exponent $\zeta \sim$ 2.1, while the
degree distribution of the time series eliminated a nonlinearity follows an
exponential distribution with the exponent, $\delta \sim 0.77$. Furthermore the
correlation, $\rho_{iM}$, between the degree k of individual stock, $i$, and
the market index, $M$, follows a power-law distribution, $<\rho_{iM}(k) > \sim
k^{\gamma}$, with the exponent $\gamma_{\textrm{S&P500}} \approx 0.16$ and
$\gamma_{\textrm{KOSPI}} \approx 0.14$, respectively. Thus, regardless of the
markets, the indivisual stocks closely related to the common factor in the
market, the market index, are likely to be located around the center of the
network between stocks, while those weakly related to the market index are
likely to be placed in the outside.",
Common Market Law Review,http://arxiv.org/abs/1201.6655v1,Learning Performance of Prediction Markets with Kelly Bettors,"In evaluating prediction markets (and other crowd-prediction mechanisms),
investigators have repeatedly observed a so-called ""wisdom of crowds"" effect,
which roughly says that the average of participants performs much better than
the average participant. The market price---an average or at least aggregate of
traders' beliefs---offers a better estimate than most any individual trader's
opinion. In this paper, we ask a stronger question: how does the market price
compare to the best trader's belief, not just the average trader. We measure
the market's worst-case log regret, a notion common in machine learning theory.
To arrive at a meaningful answer, we need to assume something about how traders
behave. We suppose that every trader optimizes according to the Kelly criteria,
a strategy that provably maximizes the compound growth of wealth over an
(infinite) sequence of market interactions. We show several consequences.
First, the market prediction is a wealth-weighted average of the individual
participants' beliefs. Second, the market learns at the optimal rate, the
market price reacts exactly as if updating according to Bayes' Law, and the
market prediction has low worst-case log regret to the best individual
participant. We simulate a sequence of markets where an underlying true
probability exists, showing that the market converges to the true objective
frequency as if updating a Beta distribution, as the theory predicts. If agents
adopt a fractional Kelly criteria, a common practical variant, we show that
agents behave like full-Kelly agents with beliefs weighted between their own
and the market's, and that the market price converges to a time-discounted
frequency. Our analysis provides a new justification for fractional Kelly
betting, a strategy widely used in practice for ad-hoc reasons. Finally, we
propose a method for an agent to learn her own optimal Kelly fraction.",
Common Market Law Review,http://arxiv.org/abs/1112.2251v1,"Recommendation systems: a joint analysis of technical aspects with
  marketing implications","In 2010, Web users ordered, only in Amazon, 73 items per second and massively
contribute reviews about their consuming experience. As the Web matures and
becomes social and participatory, collaborative filters are the basic
complement in searching online information about people, events and products.
In Web 2.0, what connected consumers create is not simply content (e.g.
reviews) but context. This new contextual framework of consumption emerges
through the aggregation and collaborative filtering of personal preferences
about goods in the Web in massive scale. More importantly, facilitates
connected consumers to search and navigate the complex Web more effectively and
amplifies incentives for quality. The objective of the present article is to
jointly review the basic stylized facts of relevant research in recommendation
systems in computer and marketing studies in order to share some common
insights. After providing a comprehensive definition of goods and Users in the
Web, we describe a classification of recommendation systems based on two
families of criteria: how recommendations are formed and input data
availability. The classification is presented under a common minimal matrix
notation and is used as a bridge to related issues in the business and
marketing literature. We focus our analysis in the fields of one-to-one
marketing, network-based marketing Web merchandising and atmospherics and their
implications in the processes of personalization and adaptation in the Web.
Market basket analysis is investigated in context of recommendation systems.
Discussion on further research refers to the business implications and
technological challenges of recommendation systems.",
Common Market Law Review,http://arxiv.org/abs/1201.3851v1,Combinatorial Modelling and Learning with Prediction Markets,"Combining models in appropriate ways to achieve high performance is commonly
seen in machine learning fields today. Although a large amount of combinatorial
models have been created, little attention is drawn to the commons in different
models and their connections. A general modelling technique is thus worth
studying to understand model combination deeply and shed light on creating new
models. Prediction markets show a promise of becoming such a generic, flexible
combinatorial model. By reviewing on several popular combinatorial models and
prediction market models, this paper aims to show how the market models can
generalise different combinatorial stuctures and how they implement these
popular combinatorial models in specific conditions. Besides, we will see among
different market models, Storkey's \emph{Machine Learning Markets} provide more
fundamental, generic modelling mechanisms than the others, and it has a
significant appeal for both theoretical study and application.",
Common Market Law Review,http://arxiv.org/abs/physics/0510038v2,"A common origin of the power law distributions in models of market and
  earthquake","We show that there is a common mode of origin for the power laws observed in
two different models: (i) the Pareto law for the distribution of money among
the agents with random saving propensities in an ideal gas-like market model
and (ii) the Gutenberg-Richter law for the distribution of overlaps in a
fractal-overlap model for earthquakes. We find that the power laws appear as
the asymptotic forms of ever-widening log-normal distributions for the agents'
money and the overlap magnitude respectively. The identification of the generic
origin of the power laws helps in better understanding and in developing
generalized views of phenomena in such diverse areas as economics and
geophysics.",Physica A 381 (2007) 377-382
Common Market Law Review,http://arxiv.org/abs/physics/0507136v2,Ideal-Gas Like Markets: Effect of Savings,"We discuss the ideal gas like models of a trading market. The effect of
savings on the distribution have been thoroughly reviewed. The market with
fixed saving factors leads to a Gamma-like distribution. In a market with
quenched random saving factors for its agents we show that the steady state
income ($m$) distribution $P(m)$ in the model has a power law tail with Pareto
index $\nu$ equal to unity. We also discuss the detailed numerical results on
this model. We analyze the distribution of mutual money difference and also
develop a master equation for the time development of $P(m)$. Precise solutions
are then obtained in some special cases.",
Common Market Law Review,http://arxiv.org/abs/0806.4466v1,Klein - Gordon equation for market wealth operations,"In this paper the modified Klein - Gordon equation for market processes is
proposed and solved. It is argued that the oscillations in market propagate
with the light velocity. The initial pulse in the market is damped and for very
large time diffused according to the Fourier law.",
Common Market Law Review,http://arxiv.org/abs/1505.02766v1,Features of transformation of marketing in e-commerce,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.","Omsk Scientific Bulletin. 2013. No 1 (115). P. 55-58. ISSN
  1813-8225"
Common Market Law Review,http://arxiv.org/abs/1811.00612v1,"Zipf's, Heaps' and Taylor's laws are determined by the expansion into
  the adjacent possible","Zipf's, Heaps' and Taylor's laws are ubiquitous in many different systems
where innovation processes are at play. Together, they represent a compelling
set of stylized facts regarding the overall statistics, the innovation rate and
the scaling of fluctuations for systems as diverse as written texts and cities,
ecological systems and stock markets. Many modeling schemes have been proposed
in literature to explain those laws, but only recently a modeling framework has
been introduced that accounts for the emergence of those laws without deducing
the emergence of one of the laws from the others or without ad hoc assumptions.
This modeling framework is based on the concept of adjacent possible space and
its key feature of being dynamically restructured while its boundaries get
explored, i.e., conditional to the occurrence of novel events. Here, we
illustrate this approach and show how this simple modelling framework,
instantiated through a modified Polya's urn model, is able reproduce Zipf's,
Heaps' and Taylor's laws within a unique self-consistent scheme. In addition
the same modelling scheme embraces other less common evolutionary laws (Hoppe's
model and Dirichlet processes) as particular cases.","Entropy 2018, 20(10), 752"
Common Market Law Review,http://arxiv.org/abs/physics/0701171v2,"A case study of speculative financial bubbles in the South African stock
  market 2003-2006","We tested 45 indices and common stocks traded in the South African stock
market for the possible existence of a bubble over the period from Jan. 2003 to
May 2006. A bubble is defined by a faster-than-exponential acceleration with
significant log-periodic oscillations. The faster-than-exponential acceleration
characteristics are tested with several different metrics, including
nonlinearity on the logarithm of the price and power law fits. The log-periodic
properties are investigated in detail using the first-order log-periodic
power-law (LPPL) formula, the parametric detrending method, the
$(H,q)$-analysis, and the second-order Weierstrass-type model, resulting in a
consistent and robust estimation of the fundamental angular log-frequency
$\omega_1 =7\pm 2$, in reasonable agreement with previous estimations on many
other bubbles in developed and developing markets. Sensitivity tests of the
estimated critical times and of the angular log-frequency are performed by
varying the first date and the last date of the stock price time series. These
tests show that the estimated parameters are robust. With the insight of 6
additional month of data since the analysis was performed, we observe that many
of the stocks on the South Africa market experienced an abrupt drop mid-June
2006, which is compatible with the predicted $t_c$ for several of the stocks,
but not all. This suggests that the mini-crash that occurred around mid-June of
2006 was only a partial correction, which has resumed into a renewed bubbly
acceleration bound to end some times in 2007, similarly to what happened on the
S&P500 US market from Oct. 1997 to Aug. 1998.","Physica A 388 (6), 869-880 (2009)"
Common Market Law Review,http://arxiv.org/abs/1508.07272v1,"Market Formation as Transitive Closure: the Evolving Pattern of Trade in
  Music","Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.",
Common Market Law Review,http://arxiv.org/abs/physics/0606005v1,"On the gap between an empirical distribution and an exponential
  distribution of waiting times for price changes in a financial market","We analyze waiting times for price changes in a foreign currency exchange
rate. Recent empirical studies of high frequency financial data support that
trades in financial markets do not follow a Poisson process and the waiting
times between trades are not exponentially distributed. Here we show that our
data is well approximated by a Weibull distribution rather than an exponential
distribution in a non-asymptotic regime. Moreover, we quantitatively evaluate
how much an empirical data is far from an exponential distribution using a
Weibull fit. Finally, we discuss a phase transition between a Weibull-law and a
power-law in the asymptotic long waiting time regime.",
Common Market Law Review,http://arxiv.org/abs/1604.01672v1,Market Share Analysis with Brand Effect,"In this paper, we investigate the effect of brand in market competition.
Specifically, we propose a variant Hotelling model where companies and
customers are represented by points in an Euclidean space, with axes being
product features. $N$ companies compete to maximize their own profits by
optimally choosing their prices, while each customer in the market, when
choosing sellers, considers the sum of product price, discrepancy between
product feature and his preference, and a company's brand name, which is
modeled by a function of its market area of the form $-\beta\cdot\text{(Market
Area)}^q$, where $\beta$ captures the brand influence and $q$ captures how
market share affects the brand. By varying the parameters $\beta$ and $q$, we
derive existence results of Nash equilibrium and equilibrium market prices and
shares. In particular, we prove that pure Nash equilibrium always exists when
$q=0$ for markets with either one and two dominating features, and it always
exists in a single dominating feature market when market affects brand name
linearly, i.e., $q=1$. Moreover, we show that at equilibrium, a company's price
is proportional to its market area over the competition intensity with its
neighbors, a result that quantitatively reconciles the common belief of a
company's pricing power. We also study an interesting ""wipe out"" phenomenon
that only appears when $q>0$, which is similar to the ""undercut"" phenomenon in
the Hotelling model, where companies may suddenly lose the entire market area
with a small price increment. Our results offer novel insight into market
pricing and positioning under competition with brand effect.",
Common Market Law Review,http://arxiv.org/abs/0706.0641v1,Information diffusion epidemics in social networks,"The dynamics of information dissemination in social networks is of paramount
importance in processes such as rumors or fads propagation, spread of product
innovations or ""word-of-mouth"" communications. Due to the difficulty in
tracking a specific information when it is transmitted by people, most
understanding of information spreading in social networks comes from models or
indirect measurements. Here we present an integrated experimental and
theoretical framework to understand and quantitatively predict how and when
information spreads over social networks. Using data collected in Viral
Marketing campaigns that reached over 31,000 individuals in eleven European
markets, we show the large degree of variability of the participants' actions,
despite them being confronted with the common task of receiving and forwarding
the same piece of information. This have a profound effect on information
diffusion: Firstly, most of the transmission takes place due to super-spreading
events which would be considered extraordinary in population-average models.
Secondly, due to the different way individuals schedule information
transmission we observe a slowing down of the spreading of information in
social networks that happens in logarithmic time. Quantitative description of
the experiments is possible through an stochastic branching process which
corroborates the importance of heterogeneity. Since high variability of both
the intensity and frequency of human responses are found in many other
activities, our findings are pertinent to many other human driven diffusion
processes like rumors, fads, innovations or news which has important
consequences for organizations management, communications, marketing or
electronic social communities.","Physical Review Letters 103, 038702 (2009)"
Common Market Law Review,http://arxiv.org/abs/1303.2675v1,"A legal perspective of E-business and E-marketing for small and medium
  enterprises","Electronic businesses are witnessing enormous growth as more and more people
are switching to online platforms. The widespread use of Internet has opened
new channels to operate trade for many businesses. Also electronic marketing
has become a proven channel of passing on the word to the customers. Legal and
ethical issues quickly become an area of concern. In this research
recommendations are made to harmonize IT and Internet Laws. A novel approach is
proposed to promote legal risk management culture in organizations. It begins
with revising current state of regulations surrounding eBusinesses and
electronic marketing. The proposed approach offers risk management by
considering risk mitigation strategy, educating people and use of information
technology. Monitoring compliance requirements are met by reviewing the latest
changes in regulations and rewarding the employees who ensures the successful
implementation of the strategy.",
Common Market Law Review,http://arxiv.org/abs/1209.5625v1,Managing Complex Structured Data In a Fast Evolving Environment,"Criminal data comes in a variety of formats, mandated by state, federal, and
international standards. Specifying the data in a unified fashion is necessary
for any system that intends to integrate with state, federal, and international
law enforcement agencies. However, the contents, format, and structure of the
data is highly inconsistent across jurisdictions, and each datum requires
different ways of being printed, transmitted, and displayed. The goal was to
design a system that is unified in its approach to specify data, and is
amenable to future ""unknown unknowns"". We have developed a domain-specific
language in Common Lisp which allows the specification of complex data with
evolving formats and structure, and is inter-operable with the Common Lisp
language. The resultant system has enabled the easy handling of complex
evolving information in the general criminal data environment and has made it
possible to manage and extend the system in a high-paced market. The language
has allowed the principal product of Secure Outcomes Inc. to enjoy success with
over 50 users throughout the United States.",
Common Market Law Review,http://arxiv.org/abs/physics/0607014v3,Inverse cubic law of index fluctuation distribution in Indian markets,"One of the principal statistical features characterizing the activity in
financial markets is the distribution of fluctuations in market indicators such
as the index. While the developed stock markets, e.g., the New York Stock
Exchange (NYSE) have been found to show heavy-tailed return distribution with a
characteristic power-law exponent, the universality of such behavior has been
debated, particularly in regard to emerging markets. Here we investigate the
distribution of several indices from the Indian financial market, one of the
largest emerging markets in the world. We have used tick-by-tick data from the
National Stock Exchange (NSE), as well as, daily closing data from both NSE and
Bombay Stock Exchange (BSE). We find that the cumulative distributions of index
returns have long tails consistent with a power-law having exponent \alpha
\approx 3, at time-scales of both 1 min and 1 day. This ``inverse cubic law''
is quantitatively similar to what has been observed in developed markets,
thereby providing strong evidence of universality in the behavior of market
fluctuations.","Physica A, 387 (2008) 2055-2065"
Common Market Law Review,http://arxiv.org/abs/1910.01615v1,Fair Division of Goods with Market Values,"Inheritances, divorces or liquidations of companies require that a common
asset is divided among the entitled parties. Legal methods usually consider the
market value of goods, while fair division procedures take into account the
parties' preferences expressed as cardinal utilities. We combine the two
practices to define two procedures that optimally allocate goods with market
values to people with preferences.",
Common Market Law Review,http://arxiv.org/abs/1009.0309v1,A Complexity View of Markets with Social Influence,"In this paper, inspired by the work of Megiddo on the formation of
preferences and strategic analysis, we consider an early market model studied
in the field of economic theory, in which each trader's utility may be
influenced by the bundles of goods obtained by her social neighbors. The goal
of this paper is to understand and characterize the impact of social influence
on the complexity of computing and approximating market equilibria.
  We present complexity-theoretic and algorithmic results for approximating
market equilibria in this model with focus on two concrete influence models
based on the traditional linear utility functions. Recall that an Arrow-Debreu
market equilibrium in a conventional exchange market with linear utility
functions can be computed in polynomial time by convex programming. Our
complexity results show that even a bounded-degree, planar influence network
can significantly increase the difficulty of equilibrium computation even in
markets with only a constant number of goods. Our algorithmic results suggest
that finding an approximate equilibrium in markets with hierarchical influence
networks might be easier than that in markets with arbitrary neighborhood
structures. By demonstrating a simple market with a constant number of goods
and a bounded-degree, planar influence graph whose equilibrium is PPAD-hard to
approximate, we also provide a counterexample to a common belief, which we
refer to as the myth of a constant number of goods, that equilibria in markets
with a constant number of goods are easy to compute or easy to approximate.",
Common Market Law Review,http://arxiv.org/abs/physics/0605247v1,"The Power (Law) of Indian Markets: Analysing NSE and BSE trading
  statistics","The nature of fluctuations in the Indian financial market is analyzed in this
paper. We have looked at the price returns of individual stocks, with
tick-by-tick data from the National Stock Exchange (NSE) and daily closing
price data from both NSE and the Bombay Stock Exchange (BSE), the two largest
exchanges in India. We find that the price returns in Indian markets follow a
fat-tailed cumulative distribution, consistent with a power law having exponent
$\alpha \sim 3$, similar to that observed in developed markets. However, the
distributions of trading volume and the number of trades have a different
nature than that seen in the New York Stock Exchange (NYSE). Further, the price
movement of different stocks are highly correlated in Indian markets.","Econophysics of Stock and Other Markets (Springer, Milan, 2006) p.
  24-34"
Common Market Law Review,http://arxiv.org/abs/physics/0504143v1,"The law of large numbers for completely random behavior of market
  participants. Quantum economics","In this paper, we briefly discuss a mathematical concept that can be used in
economics.",
Common Market Law Review,http://arxiv.org/abs/cs/0109108v1,"Spectrum auctions, pricing and network expansion in wireless
  telecommunications","This paper examines the effects of licensing conditions, in particular of
spectrum fees, on the pricing and diffusion of mobile communications services.
Seemingly exorbitant sums paid for 3G licenses in the UK, Germany in 2000 and
similarly high fees paid by U.S. carriers in the re-auctioning of PCS licenses
early in 2001 raised concerns as to the impacts of the market entry regime on
the mobile communications market.
  The evidence from the GSM and PCS markets reviewed in this paper suggests
that market entry fees do indeed influence the subsequent development of the
market. We discuss three potential transmission channels by which license fees
can influence the price and quantity of service sold in a wireless market: an
increase in average cost, an increase in incremental costs, and impacts of sunk
costs on the emerging market structure.
  From this conceptual debate, an empirical model is developed and tested using
cross-sectional data for the residential mobile voice market. We utilize a
structural equation approach, modeling the supply and demand relationships
subject to the constraint that supply equals demand. The results confirm the
existence of a positive effect of license fees on the cost of supply. However,
we also find that higher market concentration has a positive effect on the
overall supply in the market, perhaps supporting a Schumpeterian view that a
certain degree of market concentration facilitates efficiency.",
Common Market Law Review,http://arxiv.org/abs/1902.09277v1,"Design of auction-based approach for market clearing in peer-to-peer
  market platform","This paper designs a market platform for Peer-to-Peer (P2P) energy trading in
Transactive Energy (TE) systems, where prosumers and consumers actively
participate in the market as seller or buyer to trade energy. An auction-based
approach is used for market clearing in the proposed platform and a review of
different types of auction is performed. The appropriate auction approach for
market clearing in the proposed platform is designed. The proposed auction
mechanism is implemented in three steps namely determination, allocation and
payment. This paper identifies important P2P market clearing performance
indices, which are used to compare and contrast the designed auction with
different types of auction mechanisms. Comparative studies demonstrate the
efficacy of the proposed auction mechanism for market clearing in the P2P
platform.",
Common Market Law Review,http://arxiv.org/abs/physics/0606213v3,Self-organization of price fluctuation distribution in evolving markets,"Financial markets can be seen as complex systems in non-equilibrium steady
state, one of whose most important properties is the distribution of price
fluctuations. Recently, there have been assertions that this distribution is
qualitatively different in emerging markets as compared to developed markets.
Here we analyse both high-frequency tick-by-tick as well as daily closing price
data to show that the price fluctuations in the Indian stock market, one of the
largest emerging markets, have a distribution that is identical to that
observed for developed markets (e.g., NYSE). In particular, the cumulative
distribution has a long tail described by a power law with an exponent $\alpha
\approx 3$. Also, we study the historical evolution of this distribution over
the period of existence of the National Stock Exchange (NSE) of India, which
coincided with the rapid transformation of the Indian economy due to
liberalization, and show that this power law tail has been present almost
throughout. We conclude that the ``inverse cubic law'' is a truly universal
feature of a financial market, independent of its stage of development or the
condition of the underlying economy.","Europhys. Lett., 77, 58004 (2007)"
Common Market Law Review,http://arxiv.org/abs/0708.2053v2,Fluctuation scaling in complex systems: Taylor's law and beyond,"Complex systems consist of many interacting elements which participate in
some dynamical process. The activity of various elements is often different and
the fluctuation in the activity of an element grows monotonically with the
average activity. This relationship is often of the form ""$fluctuations \approx
const.\times average^\alpha$"", where the exponent $\alpha$ is predominantly in
the range $[1/2, 1]$. This power law has been observed in a very wide range of
disciplines, ranging from population dynamics through the Internet to the stock
market and it is often treated under the names \emph{Taylor's law} or
\emph{fluctuation scaling}. This review attempts to show how general the above
scaling relationship is by surveying the literature, as well as by reporting
some new empirical data and model calculations. We also show some basic
principles that can underlie the generality of the phenomenon. This is followed
by a mean-field framework based on sums of random variables. In this context
the emergence of fluctuation scaling is equivalent to some corresponding limit
theorems. In certain physical systems fluctuation scaling can be related to
finite size scaling.","Advances in Physics 57, 89-142 (2008)"
cross-border merger,http://arxiv.org/abs/1011.0534v1,Merger Dynamics in Three-Agent Games,"We present the effect of mergers in the dynamics of the three-agent model
studied by Ben-Naim, Kahng and Kim and by Rador and Mungan. Mergers are
possible in three-agent games because two agents can combine forces against the
third player and thus increase their probability to win a competition. We
implement mergers in this three-agent model via resolving merger and no-merger
units of competition in terms of a two-agent unit. This way one needs only a
single parameter which we have called the competitiveness parameter. We have
presented an analytical solution in the fully competitive limit. In this limit
the score distribution of agents is stratified and self-similar.","Eur. Phys. J. B 83, 289 - 299, 2011"
cross-border merger,http://arxiv.org/abs/1210.3893v1,Merger of Long Vortex Filaments,"This fluid dynamics video demonstrates the merger of long vortex filaments is
shown experimentally. Two counter-rotating vortices are generated using in a
tank with very high aspect ratio. PIV demonstrates the merger of the vortices
within a single orbit.",
cross-border conversion,http://arxiv.org/abs/0804.2373v1,Fast Conversion Algorithms for Orthogonal Polynomials,"We discuss efficient conversion algorithms for orthogonal polynomials. We
describe a known conversion algorithm from an arbitrary orthogonal basis to the
monomial basis, and deduce a new algorithm of the same complexity for the
converse operation.",
cross-border conversion,http://arxiv.org/abs/1906.06893v1,"Interconnected Question Generation with Coreference Alignment and
  Conversation Flow Modeling","We study the problem of generating interconnected questions in
question-answering style conversations. Compared with previous works which
generate questions based on a single sentence (or paragraph), this setting is
different in two major aspects: (1) Questions are highly conversational. Almost
half of them refer back to conversation history using coreferences. (2) In a
coherent conversation, questions have smooth transitions between turns. We
propose an end-to-end neural model with coreference alignment and conversation
flow modeling. The coreference alignment modeling explicitly aligns coreferent
mentions in conversation history with corresponding pronominal references in
generated questions, which makes generated questions interconnected to
conversation history. The conversation flow modeling builds a coherent
conversation by starting questioning on the first few sentences in a text
passage and smoothly shifting the focus to later parts. Extensive experiments
show that our system outperforms several baselines and can generate highly
conversational questions. The code implementation is released at
https://github.com/Evan-Gao/conversational-QG.",
cross-border conversion,http://arxiv.org/abs/1402.0586v1,Topic Segmentation and Labeling in Asynchronous Conversations,"Topic segmentation and labeling is often considered a prerequisite for
higher-level conversation analysis and has been shown to be useful in many
Natural Language Processing (NLP) applications. We present two new corpora of
email and blog conversations annotated with topics, and evaluate annotator
reliability for the segmentation and labeling tasks in these asynchronous
conversations. We propose a complete computational framework for topic
segmentation and labeling in asynchronous conversations. Our approach extends
state-of-the-art methods by considering a fine-grained structure of an
asynchronous conversation, along with other conversational features by applying
recent graph-based methods for NLP. For topic segmentation, we propose two
novel unsupervised models that exploit the fine-grained conversational
structure, and a novel graph-theoretic supervised model that combines lexical,
conversational and topic features. For topic labeling, we propose two novel
(unsupervised) random walk models that respectively capture conversation
specific clues from two different sources: the leading sentences and the
fine-grained conversational structure. Empirical evaluation shows that the
segmentation and the labeling performed by our best models beat the
state-of-the-art, and are highly correlated with human annotations.","Journal Of Artificial Intelligence Research, Volume 47, pages
  521-573, 2013"
cross-border conversion,http://arxiv.org/abs/1901.06525v1,"What Makes a Good Conversation? Challenges in Designing Truly
  Conversational Agents","Conversational agents promise conversational interaction but fail to deliver.
Efforts often emulate functional rules from human speech, without considering
key characteristics that conversation must encapsulate. Given its potential in
supporting long-term human-agent relationships, it is paramount that HCI
focuses efforts on delivering this promise. We aim to understand what people
value in conversation and how this should manifest in agents. Findings from a
series of semi-structured interviews show people make a clear dichotomy between
social and functional roles of conversation, emphasising the long-term dynamics
of bond and trust along with the importance of context and relationship stage
in the types of conversations they have. People fundamentally questioned the
need for bond and common ground in agent communication, shifting to more
utilitarian definitions of conversational qualities. Drawing on these findings
we discuss key challenges for conversational agent design, most notably the
need to redefine the design parameters for conversational agent interaction.",
cross-border conversion,http://arxiv.org/abs/1911.02707v1,Conversation Generation with Concept Flow,"Human conversations naturally evolve around related entities and connected
concepts, while may also shift from topic to topic. This paper presents
ConceptFlow, which leverages commonsense knowledge graphs to explicitly model
such conversation flows for better conversation response generation.
ConceptFlow grounds the conversation inputs to the latent concept space and
represents the potential conversation flow as a concept flow along the
commonsense relations. The concept is guided by a graph attention mechanism
that models the possibility of the conversation evolving towards different
concepts. The conversation response is then decoded using the encodings of both
utterance texts and concept flows, integrating the learned conversation
structure in the concept space. Our experiments on Reddit conversations
demonstrate the advantage of ConceptFlow over previous commonsense aware dialog
models and fine-tuned GPT-2 models, while using much fewer parameters but with
explicit modeling of conversation structures.",
cross-border conversion,http://arxiv.org/abs/1312.0455v1,Radix Conversion for IEEE754-2008 Mixed Radix Floating-Point Arithmetic,"Conversion between binary and decimal floating-point representations is
ubiquitous. Floating-point radix conversion means converting both the exponent
and the mantissa. We develop an atomic operation for FP radix conversion with
simple straight-line algorithm, suitable for hardware design. Exponent
conversion is performed with a small multiplication and a lookup table. It
yields the correct result without error. Mantissa conversion uses a few
multiplications and a small lookup table that is shared amongst all types of
conversions. The accuracy changes by adjusting the computing precision.",
cross-border conversion,http://arxiv.org/abs/1906.11604v1,"Gated Embeddings in End-to-End Speech Recognition for
  Conversational-Context Fusion","We present a novel conversational-context aware end-to-end speech recognizer
based on a gated neural network that incorporates
conversational-context/word/speech embeddings. Unlike conventional speech
recognition models, our model learns longer conversational-context information
that spans across sentences and is consequently better at recognizing long
conversations. Specifically, we propose to use the text-based external word
and/or sentence embeddings (i.e., fastText, BERT) within an end-to-end
framework, yielding a significant improvement in word error rate with better
conversational-context representation. We evaluated the models on the
Switchboard conversational speech corpus and show that our model outperforms
standard end-to-end speech recognition models.",
cross-border conversion,http://arxiv.org/abs/1905.11553v2,Target-Guided Open-Domain Conversation,"Many real-world open-domain conversation applications have specific goals to
achieve during open-ended chats, such as recommendation, psychotherapy,
education, etc. We study the problem of imposing conversational goals on
open-domain chat agents. In particular, we want a conversational system to chat
naturally with human and proactively guide the conversation to a designated
target subject. The problem is challenging as no public data is available for
learning such a target-guided strategy. We propose a structured approach that
introduces coarse-grained keywords to control the intended content of system
responses. We then attain smooth conversation transition through turn-level
supervised learning, and drive the conversation towards the target with
discourse-level constraints. We further derive a keyword-augmented conversation
dataset for the study. Quantitative and human evaluations show our system can
produce meaningful and effective conversations, significantly improving over
other approaches.",
cross-border conversion,http://arxiv.org/abs/1910.07514v1,Designing Style Matching Conversational Agents,"Advances in machine intelligence have enabled conversational interfaces that
have the potential to radically change the way humans interact with machines.
However, even with the progress in the abilities of these agents, there remain
critical gaps in their capacity for natural interactions. One limitation is
that the agents are often monotonic in behavior and do not adapt to their
partner. We built two end-to-end conversational agents: a voice-based agent
that can engage in naturalistic, multi-turn dialogue and align with the
interlocutor's conversational style, and a 2nd, expressive, embodied
conversational agent (ECA) that can recognize human behavior during open-ended
conversations and automatically align its responses to the visual and
conversational style of the other party. The embodied conversational agent
leverages multimodal inputs to produce rich and perceptually valid vocal and
facial responses (e.g., lip syncing and expressions) during the conversation.
Based on empirical results from a set of user studies, we highlight several
significant challenges in building such systems and provide design guidelines
for multi-turn dialogue interactions using style adaptation for future
research.",
cross-border conversion,http://arxiv.org/abs/1208.5951v1,Single Photon Adiabatic Wavelength Conversion,"Adiabatic wavelength conversion is experimentally demonstrated at a single
photon power-level using an integrated Silicon ring resonator. This approach
allows conversion of a photon to arbitrary wavelengths with no energy or phase
matching constraints. The conversion is inherently low-noise and efficient with
greater than 10% conversion efficiencies for wavelength changes up to 0.5nm,
more than twenty times the resonators line-width. The observed wavelength
change and efficiency agrees well with theory and bright coherent light
demonstrations. These results will enable integrated quantum optical wavelength
conversion for application ranging from wavelength-multiplexed quantum networks
to frequency bin entanglement.",
cross-border conversion,http://arxiv.org/abs/1802.04425v1,"""How Was Your Weekend?"" A Generative Model of Phatic Conversation","Unspoken social rules, such as those that govern choosing a proper discussion
topic and when to change discussion topics, guide conversational behaviors. We
propose a computational model of conversation that can follow or break such
rules, with participant agents that respond accordingly. Additionally, we
demonstrate an application of the model: the Experimental Social Tutor (EST), a
first step toward a social skills training tool that generates human-readable
conversation and a conversational guideline at each point in the dialogue.
Finally, we discuss the design and results of a pilot study evaluating the EST.
Results show that our model is capable of producing conversations that follow
social norms.",
cross-border conversion,http://arxiv.org/abs/1808.03915v1,Addressee and Response Selection for Multilingual Conversation,"Developing conversational systems that can converse in many languages is an
interesting challenge for natural language processing. In this paper, we
introduce multilingual addressee and response selection. In this task, a
conversational system predicts an appropriate addressee and response for an
input message in multiple languages. A key to developing such multilingual
responding systems is how to utilize high-resource language data to compensate
for low-resource language data. We present several knowledge transfer methods
for conversational systems. To evaluate our methods, we create a new
multilingual conversation dataset. Experiments on the dataset demonstrate the
effectiveness of our methods.",
cross-border conversion,http://arxiv.org/abs/1902.07763v1,"Optomechanically amplified wavelength conversion in diamond
  microcavities","Efficient, low noise conversion between different colors of light is a
necessary tool for interfacing quantum optical technologies that have different
operating wavelengths. Optomechanically mediated wavelength conversion and
amplification is a potential method for realizing this technology, and is
demonstrated here in microdisks fabricated from single crystal diamond -- a
material that can host a wide range of quantum emitters. Frequency up--
conversion is demonstrated with internal conversion efficiency of $\sim$ 45\%
using both narrow and broadband probe fields, and optomechanical frequency
conversion with amplification is demonstrated in the optical regime for the
first time.",
cross-border conversion,http://arxiv.org/abs/1909.13443v1,Using Conversational Agents To Support Learning By Teaching,"Conversational agents are becoming increasingly popular for supporting and
facilitating learning. Conventional pedagogical agents are designed to play the
role of human teachers by giving instructions to the students. In this paper,
we investigate the use of conversational agents to support the
'learning-by-teaching' paradigm where the agent receives instructions from
students. In particular, we introduce Curiosity Notebook: an educational
application that harvests conversational interventions to facilitate students'
learning. Recognizing such interventions can not only help in engaging students
within learning interactions, but also provide a deeper insight into the
intricacies involved in designing conversational agents for educational
purposes.",
cross-border conversion,http://arxiv.org/abs/1909.01362v1,"Trouble on the Horizon: Forecasting the Derailment of Online
  Conversations as they Develop","Online discussions often derail into toxic exchanges between participants.
Recent efforts mostly focused on detecting antisocial behavior after the fact,
by analyzing single comments in isolation. To provide more timely notice to
human moderators, a system needs to preemptively detect that a conversation is
heading towards derailment before it actually turns toxic. This means modeling
derailment as an emerging property of a conversation rather than as an isolated
utterance-level event.
  Forecasting emerging conversational properties, however, poses several
inherent modeling challenges. First, since conversations are dynamic, a
forecasting model needs to capture the flow of the discussion, rather than
properties of individual comments. Second, real conversations have an unknown
horizon: they can end or derail at any time; thus a practical forecasting model
needs to assess the risk in an online fashion, as the conversation develops. In
this work we introduce a conversational forecasting model that learns an
unsupervised representation of conversational dynamics and exploits it to
predict future derailment as the conversation develops. By applying this model
to two new diverse datasets of online conversations with labels for antisocial
events, we show that it outperforms state-of-the-art systems at forecasting
derailment.",
cross-border conversion,http://arxiv.org/abs/1605.04462v3,"Large-scale Analysis of Counseling Conversations: An Application of
  Natural Language Processing to Mental Health","Mental illness is one of the most pressing public health issues of our time.
While counseling and psychotherapy can be effective treatments, our knowledge
about how to conduct successful counseling conversations has been limited due
to lack of large-scale data with labeled outcomes of the conversations. In this
paper, we present a large-scale, quantitative study on the discourse of
text-message-based counseling conversations. We develop a set of novel
computational discourse analysis methods to measure how various linguistic
aspects of conversations are correlated with conversation outcomes. Applying
techniques such as sequence-based conversation models, language model
comparisons, message clustering, and psycholinguistics-inspired word frequency
analyses, we discover actionable conversation strategies that are associated
with better conversation outcomes.",
cross-border conversion,http://arxiv.org/abs/1709.04734v1,Perspectives for Evaluating Conversational AI,"Conversational AI systems are becoming famous in day to day lives. In this
paper, we are trying to address the following key question: To identify whether
design, as well as development efforts for search oriented conversational AI
are successful or not.It is tricky to define 'success' in the case of
conversational AI and equally tricky part is to use appropriate metrics for the
evaluation of conversational AI. We propose four different perspectives namely
user experience, information retrieval, linguistic and artificial intelligence
for the evaluation of conversational AI systems. Additionally, background
details of conversational AI systems are provided including desirable
characteristics of personal assistants, differences between chatbot and an AI
based personal assistant. An importance of personalization and how it can be
achieved is explained in detail. Current challenges in the development of an
ideal conversational AI (personal assistant) are also highlighted along with
guidelines for achieving personalized experience for users.",
cross-border conversion,http://arxiv.org/abs/1906.02738v2,"Conversing by Reading: Contentful Neural Conversation with On-demand
  Machine Reading","Although neural conversation models are effective in learning how to produce
fluent responses, their primary challenge lies in knowing what to say to make
the conversation contentful and non-vacuous. We present a new end-to-end
approach to contentful neural conversation that jointly models response
generation and on-demand machine reading. The key idea is to provide the
conversation model with relevant long-form text on the fly as a source of
external knowledge. The model performs QA-style reading comprehension on this
text in response to each conversational turn, thereby allowing for more focused
integration of external knowledge than has been possible in prior approaches.
To support further research on knowledge-grounded conversation, we introduce a
new large-scale conversation dataset grounded in external web pages (2.8M
turns, 7.4M sentences of grounding). Both human evaluation and automated
metrics show that our approach results in more contentful responses compared to
a variety of previous methods, improving both the informativeness and diversity
of generated output.",
cross-border conversion,http://arxiv.org/abs/1712.07229v1,"Attentive Memory Networks: Efficient Machine Reading for Conversational
  Search","Recent advances in conversational systems have changed the search paradigm.
Traditionally, a user poses a query to a search engine that returns an answer
based on its index, possibly leveraging external knowledge bases and
conditioning the response on earlier interactions in the search session. In a
natural conversation, there is an additional source of information to take into
account: utterances produced earlier in a conversation can also be referred to
and a conversational IR system has to keep track of information conveyed by the
user during the conversation, even if it is implicit.
  We argue that the process of building a representation of the conversation
can be framed as a machine reading task, where an automated system is presented
with a number of statements about which it should answer questions. The
questions should be answered solely by referring to the statements provided,
without consulting external knowledge. The time is right for the information
retrieval community to embrace this task, both as a stand-alone task and
integrated in a broader conversational search setting.
  In this paper, we focus on machine reading as a stand-alone task and present
the Attentive Memory Network (AMN), an end-to-end trainable machine reading
algorithm. Its key contribution is in efficiency, achieved by having an
hierarchical input encoder, iterating over the input only once. Speed is an
important requirement in the setting of conversational search, as gaps between
conversational turns have a detrimental effect on naturalness. On 20 datasets
commonly used for evaluating machine reading algorithms we show that the AMN
achieves performance comparable to the state-of-the-art models, while using
considerably fewer computations.","Proceedings of 1st International Workshop on Conversational
  Approaches to Information Retrieval, Tokyo, Japan, August 11, 2017 (CAIR'17)"
cross-border conversion,http://arxiv.org/abs/1805.00188v3,"Response Ranking with Deep Matching Networks and External Knowledge in
  Information-seeking Conversation Systems","Intelligent personal assistant systems with either text-based or voice-based
conversational interfaces are becoming increasingly popular around the world.
Retrieval-based conversation models have the advantages of returning fluent and
informative responses. Most existing studies in this area are on open domain
""chit-chat"" conversations or task / transaction oriented conversations. More
research is needed for information-seeking conversations. There is also a lack
of modeling external knowledge beyond the dialog utterances among current
conversational models. In this paper, we propose a learning framework on the
top of deep neural matching networks that leverages external knowledge for
response ranking in information-seeking conversation systems. We incorporate
external knowledge into deep neural models with pseudo-relevance feedback and
QA correspondence knowledge distillation. Extensive experiments with three
information-seeking conversation data sets including both open benchmarks and
commercial data show that, our methods outperform various baseline methods
including several deep text matching models and the state-of-the-art method on
response selection in multi-turn conversations. We also perform analysis over
different response types, model variations and ranking examples. Our models and
research findings provide new insights on how to utilize external knowledge
with deep neural models for response selection and have implications for the
design of the next generation of information-seeking conversation systems.",
cross-border conversion,http://arxiv.org/abs/1701.08251v2,"Image-Grounded Conversations: Multimodal Context for Natural Question
  and Response Generation","The popularity of image sharing on social media and the engagement it creates
between users reflects the important role that visual context plays in everyday
conversations. We present a novel task, Image-Grounded Conversations (IGC), in
which natural-sounding conversations are generated about a shared image. To
benchmark progress, we introduce a new multiple-reference dataset of
crowd-sourced, event-centric conversations on images. IGC falls on the
continuum between chit-chat and goal-directed conversation models, where visual
grounding constrains the topic of conversation to event-driven utterances.
Experiments with models trained on social media data show that the combination
of visual and textual context enhances the quality of generated conversational
turns. In human evaluation, the gap between human performance and that of both
neural and retrieval architectures suggests that multi-modal IGC presents an
interesting challenge for dialogue research.",
cross-border conversion,http://arxiv.org/abs/1706.06987v1,A Generative Model of Group Conversation,"Conversations with non-player characters (NPCs) in games are typically
confined to dialogue between a human player and a virtual agent, where the
conversation is initiated and controlled by the player. To create richer, more
believable environments for players, we need conversational behavior to reflect
initiative on the part of the NPCs, including conversations that include
multiple NPCs who interact with one another as well as the player. We describe
a generative computational model of group conversation between agents, an
abstract simulation of discussion in a small group setting. We define
conversational interactions in terms of rules for turn taking and interruption,
as well as belief change, sentiment change, and emotional response, all of
which are dependent on agent personality, context, and relationships. We
evaluate our model using a parameterized expressive range analysis, observing
correlations between simulation parameters and features of the resulting
conversations. This analysis confirms, for example, that character
personalities will predict how often they speak, and that heterogeneous groups
of characters will generate more belief change.",
cross-border conversion,http://arxiv.org/abs/1712.08636v1,Find the Conversation Killers: a Predictive Study of Thread-ending Posts,"How to improve the quality of conversations in online communities has
attracted considerable attention recently. Having engaged, urbane, and reactive
online conversations has a critical effect on the social life of Internet
users. In this study, we are particularly interested in identifying a post in a
multi-party conversation that is unlikely to be further replied to, which
therefore kills that thread of the conversation. For this purpose, we propose a
deep learning model called the ConverNet. ConverNet is attractive due to its
capability of modeling the internal structure of a long conversation and its
appropriate encoding of the contextual information of the conversation, through
effective integration of attention mechanisms. Empirical experiments on
real-world datasets demonstrate the effectiveness of the proposal model. For
the widely concerned topic, our analysis also offers implications for improving
the quality and user experience of online conversations.",
cross-border conversion,http://arxiv.org/abs/1801.03622v1,Topic-based Evaluation for Conversational Bots,"Dialog evaluation is a challenging problem, especially for non task-oriented
dialogs where conversational success is not well-defined. We propose to
evaluate dialog quality using topic-based metrics that describe the ability of
a conversational bot to sustain coherent and engaging conversations on a topic,
and the diversity of topics that a bot can handle. To detect conversation
topics per utterance, we adopt Deep Average Networks (DAN) and train a topic
classifier on a variety of question and query data categorized into multiple
topics. We propose a novel extension to DAN by adding a topic-word attention
table that allows the system to jointly capture topic keywords in an utterance
and perform topic classification. We compare our proposed topic based metrics
with the ratings provided by users and show that our metrics both correlate
with and complement human judgment. Our analysis is performed on tens of
thousands of real human-bot dialogs from the Alexa Prize competition and
highlights user expectations for conversational bots.",Nips.Workshop.ConversationalAI 2017-12-08
cross-border conversion,http://arxiv.org/abs/1805.04625v2,Strong Converse using Change of Measure Arguments,"The strong converse for a coding theorem shows that the optimal asymptotic
rate possible with vanishing error cannot be improved by allowing a fixed
error. Building on a method introduced by Gu and Effros for centralized coding
problems, we develop a general and simple recipe for proving strong converse
that is applicable for distributed problems as well. Heuristically, our proof
of strong converse mimics the standard steps for proving a weak converse,
except that we apply those steps to a modified distribution obtained by
conditioning the original distribution on the event that no error occurs. A key
component of our recipe is the replacement of the hard Markov constraints
implied by the distributed nature of the problem with a soft information cost
using a variational formula introduced by Oohama. We illustrate our method by
providing a short proof of the strong converse for the Wyner-Ziv problem and
strong converse theorems for interactive function computation, common
randomness and secret key agreement, and the wiretap channel; the latter three
strong converse problems were open prior to this work.",
cross-border conversion,http://arxiv.org/abs/1808.07042v2,CoQA: A Conversational Question Answering Challenge,"Humans gather information by engaging in conversations involving a series of
interconnected questions and answers. For machines to assist in information
gathering, it is therefore essential to enable them to answer conversational
questions. We introduce CoQA, a novel dataset for building Conversational
Question Answering systems. Our dataset contains 127k questions with answers,
obtained from 8k conversations about text passages from seven diverse domains.
The questions are conversational, and the answers are free-form text with their
corresponding evidence highlighted in the passage. We analyze CoQA in depth and
show that conversational questions have challenging phenomena not present in
existing reading comprehension datasets, e.g., coreference and pragmatic
reasoning. We evaluate strong conversational and reading comprehension models
on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points
behind human performance (88.8%), indicating there is ample room for
improvement. We launch CoQA as a challenge to the community at
http://stanfordnlp.github.io/coqa/",
cross-border conversion,http://arxiv.org/abs/1812.10008v1,Alpha-conversion for lambda terms with explicit weakenings,"Using explicit weakenings, we can define alpha-conversion by simple equations
without any mention of free variables.",
cross-border conversion,http://arxiv.org/abs/1905.05412v2,BERT with History Answer Embedding for Conversational Question Answering,"Conversational search is an emerging topic in the information retrieval
community. One of the major challenges to multi-turn conversational search is
to model the conversation history to answer the current question. Existing
methods either prepend history turns to the current question or use complicated
attention mechanisms to model the history. We propose a conceptually simple yet
highly effective approach referred to as history answer embedding. It enables
seamless integration of conversation history into a conversational question
answering (ConvQA) model built on BERT (Bidirectional Encoder Representations
from Transformers). We first explain our view that ConvQA is a simplified but
concrete setting of conversational search, and then we provide a general
framework to solve ConvQA. We further demonstrate the effectiveness of our
approach under this framework. Finally, we analyze the impact of different
numbers of history turns under different settings to provide new insights into
conversation history modeling in ConvQA.",
cross-border conversion,http://arxiv.org/abs/1905.12188v1,"Exploiting Persona Information for Diverse Generation of Conversational
  Responses","In human conversations, due to their personalities in mind, people can easily
carry out and maintain the conversations. Giving conversational context with
persona information to a chatbot, how to exploit the information to generate
diverse and sustainable conversations is still a non-trivial task. Previous
work on persona-based conversational models successfully make use of predefined
persona information and have shown great promise in delivering more realistic
responses. And they all learn with the assumption that given a source input,
there is only one target response. However, in human conversations, there are
massive appropriate responses to a given input message. In this paper, we
propose a memory-augmented architecture to exploit persona information from
context and incorporate a conditional variational autoencoder model together to
generate diverse and sustainable conversations. We evaluate the proposed model
on a benchmark persona-chat dataset. Both automatic and human evaluations show
that our model can deliver more diverse and more engaging persona-based
responses than baseline approaches.",
cross-border conversion,http://arxiv.org/abs/1910.01335v1,"Topic-aware Pointer-Generator Networks for Summarizing Spoken
  Conversations","Due to the lack of publicly available resources, conversation summarization
has received far less attention than text summarization. As the purpose of
conversations is to exchange information between at least two interlocutors,
key information about a certain topic is often scattered and spanned across
multiple utterances and turns from different speakers. This phenomenon is more
pronounced during spoken conversations, where speech characteristics such as
backchanneling and false-starts might interrupt the topical flow. Moreover,
topic diffusion and (intra-utterance) topic drift are also more common in
human-to-human conversations. Such linguistic characteristics of dialogue
topics make sentence-level extractive summarization approaches used in spoken
documents ill-suited for summarizing conversations. Pointer-generator networks
have effectively demonstrated its strength at integrating extractive and
abstractive capabilities through neural modeling in text summarization. To the
best of our knowledge, to date no one has adopted it for summarizing
conversations. In this work, we propose a topic-aware architecture to exploit
the inherent hierarchical structure in conversations to further adapt the
pointer-generator model. Our approach significantly outperforms competitive
baselines, achieves more efficient learning outcomes, and attains more robust
performance.",
cross-border conversion,http://arxiv.org/abs/physics/0009070v1,Nuclear spin conversion in formaldehyde,"Theoretical model of the nuclear spin conversion in formaldehyde (H2CO) has
been developed. The conversion is governed by the intramolecular spin-rotation
mixing of molecular ortho and para states. The rate of conversion has been
found equal 1.4*10^{-4}~1/s*Torr. Temperature dependence of the spin conversion
has been predicted to be weak in the wide temperature range T=200-900 K.","J. Molecular Structure v.599, 337 (2001)"
cross-border conversion,http://arxiv.org/abs/1110.2550v1,"Suppression of FM-to-AM conversion in third-harmonic generation at the
  retracing point of a crystal","FM-to-AM conversion can cause many negative effects (e.g., reducing of margin
against damage to the optics, and etc.) on performances of third-harmonic
conversion system. In this letter, the FM-to-AM conversion effect in
third-harmonic generation is investigated both at and away from the retracing
point of type-II KDP crystal. Obtained results indicate that the FM-to-AM
conversion can be suppressed effectively when the crystal works at the
retracing point.","Optics Letters, 34(24), 3848-3850 (2009)"
cross-border conversion,http://arxiv.org/abs/1301.1429v1,Adaptation of fictional and online conversations to communication media,"Conversations allow the quick transfer of short bits of information and it is
reasonable to expect that changes in communication medium affect how we
converse. Using conversations in works of fiction and in an online social
networking platform, we show that the utterance length of conversations is
slowly shortening with time but adapts more strongly to the constraints of the
communication medium. This indicates that the introduction of any new medium of
communication can affect the way natural language evolves.","Eur. Phys. J. B, vol. 85, no. 12, pp. 1-7, 2012"
cross-border conversion,http://arxiv.org/abs/1710.07388v1,"Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation
  Models","Building a persona-based conversation agent is challenging owing to the lack
of large amounts of speaker-specific conversation data for model training. This
paper addresses the problem by proposing a multi-task learning approach to
training neural conversation models that leverages both conversation data
across speakers and other types of data pertaining to the speaker and speaker
roles to be modeled. Experiments show that our approach leads to significant
improvements over baseline model quality, generating responses that capture
more precisely speakers' traits and speaking styles. The model offers the
benefits of being algorithmically simple and easy to implement, and not relying
on large quantities of data representing specific individual speakers.",
cross-border conversion,http://arxiv.org/abs/1805.02275v1,"Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid
  Approach","We propose a novel coherence model for written asynchronous conversations
(e.g., forums, emails), and show its applications in coherence assessment and
thread reconstruction tasks. We conduct our research in two steps. First, we
propose improvements to the recently proposed neural entity grid model by
lexicalizing its entity transitions. Then, we extend the model to asynchronous
conversations by incorporating the underlying conversational structure in the
entity grid representation and feature computation. Our model achieves state of
the art results on standard coherence assessment tasks in monologue and
conversations outperforming existing models. We also demonstrate its
effectiveness in reconstructing thread structures.",
cross-border conversion,http://arxiv.org/abs/1809.00344v1,"Contextual Neural Model for Translating Bilingual Multi-Speaker
  Conversations","Recent works in neural machine translation have begun to explore document
translation. However, translating online multi-speaker conversations is still
an open problem. In this work, we propose the task of translating Bilingual
Multi-Speaker Conversations, and explore neural architectures which exploit
both source and target-side conversation histories for this task. To initiate
an evaluation for this task, we introduce datasets extracted from Europarl v7
and OpenSubtitles2016. Our experiments on four language-pairs confirm the
significance of leveraging conversation history, both in terms of BLEU and
manual evaluation.",
cross-border conversion,http://arxiv.org/abs/1904.01664v1,Mirroring to Build Trust in Digital Assistants,"We describe experiments towards building a conversational digital assistant
that considers the preferred conversational style of the user. In particular,
these experiments are designed to measure whether users prefer and trust an
assistant whose conversational style matches their own. To this end we
conducted a user study where subjects interacted with a digital assistant that
responded in a way that either matched their conversational style, or did not.
Using self-reported personality attributes and subjects' feedback on the
interactions, we built models that can reliably predict a user's preferred
conversational style.",
cross-border conversion,http://arxiv.org/abs/1208.4324v1,"Characterizing Pedophile Conversations on the Internet using Online
  Grooming","Cyber-crime targeting children such as online pedophile activity are a major
and a growing concern to society. A deep understanding of predatory chat
conversations on the Internet has implications in designing effective solutions
to automatically identify malicious conversations from regular conversations.
We believe that a deeper understanding of the pedophile conversation can result
in more sophisticated and robust surveillance systems than majority of the
current systems relying only on shallow processing such as simple word-counting
or key-word spotting.
  In this paper, we study pedophile conversations from the perspective of
online grooming theory and perform a series of linguistic-based empirical
analysis on several pedophile chat conversations to gain useful insights and
patterns. We manually annotated 75 pedophile chat conversations with six stages
of online grooming and test several hypothesis on it. The results of our
experiments reveal that relationship forming is the most dominant online
grooming stage in contrast to the sexual stage. We use a widely used
word-counting program (LIWC) to create psycho-linguistic profiles for each of
the six online grooming stages to discover interesting textual patterns useful
to improve our understanding of the online pedophile phenomenon. Furthermore,
we present empirical results that throw light on various aspects of a pedophile
conversation such as probability of state transitions from one stage to
another, distribution of a pedophile chat conversation across various online
grooming stages and correlations between pre-defined word categories and online
grooming stages.",
cross-border conversion,http://arxiv.org/abs/1812.09321v2,Multiple topic identification in telephone conversations,"This paper deals with the automatic analysis of conversations between a
customer and an agent in a call centre of a customer care service. The purpose
of the analysis is to hypothesize themes about problems and complaints
discussed in the conversation. Themes are defined by the application
documentation topics. A conversation may contain mentions that are irrelevant
for the application purpose and multiple themes whose mentions may be
interleaved portions of a conversation that cannot be well defined. Two methods
are proposed for multiple theme hypothesization. One of them is based on a
cosine similarity measure using a bag of features extracted from the entire
conversation. The other method introduces the concept of thematic density
distributed around specific word positions in a conversation. In addition to
automatically selected words, word bi-grams with possible gaps between
successive words are also considered and selected. Experimental results show
that the results obtained with the proposed methods outperform the results
obtained with support vector machines on the same data. Furthermore, using the
theme skeleton of a conversation from which thematic densities are derived, it
will be possible to extract components of an automatic conversation report to
be used for improving the service performance. Index Terms: multi-topic audio
document classification, hu-man/human conversation analysis, speech analytics,
distance bigrams","Interspeech, Aug 2013, Lyon, France"
cross-border conversion,http://arxiv.org/abs/1906.05572v2,Proactive Human-Machine Conversation with Explicit Conversation Goals,"Though great progress has been made for human-machine conversation, current
dialogue system is still in its infancy: it usually converses passively and
utters words more as a matter of response, rather than on its own initiatives.
In this paper, we take a radical step towards building a human-like
conversational agent: endowing it with the ability of proactively leading the
conversation (introducing a new topic or maintaining the current topic). To
facilitate the development of such conversation systems, we create a new
dataset named DuConv where one acts as a conversation leader and the other acts
as the follower. The leader is provided with a knowledge graph and asked to
sequentially change the discussion topics, following the given conversation
goal, and meanwhile keep the dialogue as natural and engaging as possible.
DuConv enables a very challenging task as the model needs to both understand
dialogue and plan over the given knowledge graph. We establish baseline results
on this dataset (about 270K utterances and 30k dialogues) using several
state-of-the-art models. Experimental results show that dialogue models that
plan over the knowledge graph can make full use of related knowledge to
generate more diverse multi-turn conversations. The baseline systems along with
the dataset are publicly available",
cross-border conversion,http://arxiv.org/abs/1908.09456v1,Attentive History Selection for Conversational Question Answering,"Conversational question answering (ConvQA) is a simplified but concrete
setting of conversational search. One of its major challenges is to leverage
the conversation history to understand and answer the current question. In this
work, we propose a novel solution for ConvQA that involves three aspects.
First, we propose a positional history answer embedding method to encode
conversation history with position information using BERT in a natural way.
BERT is a powerful technique for text representation. Second, we design a
history attention mechanism (HAM) to conduct a ""soft selection"" for
conversation histories. This method attends to history turns with different
weights based on how helpful they are on answering the current question. Third,
in addition to handling conversation history, we take advantage of multi-task
learning (MTL) to do answer prediction along with another essential
conversation task (dialog act prediction) using a uniform model architecture.
MTL is able to learn more expressive and generic representations to improve the
performance of ConvQA. We demonstrate the effectiveness of our model with
extensive experimental evaluations on QuAC, a large-scale ConvQA dataset. We
show that position information plays an important role in conversation history
modeling. We also visualize the history attention and provide new insights into
conversation history understanding.",
cross-border conversion,http://arxiv.org/abs/cs/0205055v1,"Practical Strategies for Integrating a Conversation Analyst in an
  Iterative Design Process","We present a case study of an iterative design process that includes a
conversation analyst. We discuss potential benefits of conversation analysis
for design, and we describe our strategies for integrating the conversation
analyst in the design process. Since the analyst on our team had no previous
exposure to design or engineering, and none of the other members of our team
had any experience with conversation analysis, we needed to build a foundation
for our interaction. One of our key strategies was to pair the conversation
analyst with a designer in a highly interactive collaboration. Our tactics have
been effective on our project, leading to valuable results that we believe we
could not have obtained using another method. We hope that this paper can serve
as a practical guide to those interested in establishing a productive and
efficient working relationship between a conversation analyst and the other
members of a design team.","Proc. ACM Conf. on Designing Interactive Systems, London, UK, June
  2002, 19-28. ACM Press."
cross-border conversion,http://arxiv.org/abs/cs/0304002v1,"The Mad Hatter&acute;s Cocktail Party: A Social Mobile Audio Space
  Supporting Multiple Simultaneous Conversations","This paper presents a mobile audio space intended for use by gelled social
groups. In face-to-face interactions in such social groups, conversational
floors change frequently, e.g., two participants split off to form a new
conversational floor, a participant moves from one conversational floor to
another, etc. To date, audio spaces have provided little support for such
dynamic regroupings of participants, either requiring that the participants
explicitly specify with whom they wish to talk or simply presenting all
participants as though they are in a single floor. By contrast, the audio space
described here monitors participant behavior to identify conversational floors
as they emerge. The system dynamically modifies the audio delivered to each
participant to enhance the salience of the participants with whom they are
currently conversing. We report a user study of the system, focusing on
conversation analytic results.","Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems, Ft.
  Lauderdale, FL, Apr. 2003, 425-432. ACM Press."
cross-border conversion,http://arxiv.org/abs/1010.2943v1,The roundtable: an abstract model of conversation dynamics,"Is it possible to abstract a formal mechanism originating schisms and
governing the size evolution of social conversations? In this work a
constructive solution to such problem is proposed: an abstract model of a
generic N-party turn-taking conversation. The model develops from simple yet
realistic assumptions derived from experimental evidence, abstracts from
conversation content and semantics while including topological information, and
is driven by stochastic dynamics. We find that a single mechanism - namely the
dynamics of conversational party's individual fitness, as related to
conversation size - controls the development of the self-organized schisming
phenomenon. Potential generalizations of the model - including individual
traits and preferences, memory effects and more elaborated conversational
topologies - may find important applications also in other fields of research,
where dynamically-interacting and networked agents play a fundamental role.","Journal of Artificial Societies and Social Simulation 13, 4 (2010)"
cross-border conversion,http://arxiv.org/abs/1411.0319v3,"Achievable and Converse bounds over a general channel and general
  decoding metric","Achievable and converse bounds for general channels and mismatched decoding
are derived. The direct (achievable) bound is derived using random coding and
the analysis is tight up to factor 2. The converse is given in term of the
achievable bound and the factor between them is given. This gives performance
of the best rate-R code with possible mismatched decoding metric over a general
channel, up to the factor that is identified. In the matched case we show that
the converse equals the minimax meta-converse of Polyanskiy et al.",
cross-border conversion,http://arxiv.org/abs/1307.6319v1,"Kinetic simulation of the O-X conversion process in dense magnetized
  plasmas","The ordinary-extraordinary-Bernstein (O-X-B) double conversion is considered
and simulated with a kinetic particle model vs full wave model for parameters
of the TJ-II stellarator. This simulation has been done with the
particle-in-cell code, XOOPIC (X11-based object-oriented particle-incell).
XOOPIC is able to model the non-monotonic density and magnetic profile of
TJ-II. The first step of conversion, O-X conversion, is observed clearly. By
applying some optimizations such as increasing the number of computational
particles in the region of the X-B conversion, the simulation of the second
step is also possible. By considering the electric and magnetic components of
launched and reflected waves, the O-mode wave and the X-mode wave can be easily
detected. Via considering the power of launched O-mode wave and converted
X-mode wave, the efficiency of O-X conversion for the best theoretical launch
angle is obtained, which is in good agreement with previous computed
efficiencies via full-wave simulations. For the optimum angle of 47? between
the wave-vector of the incident O-mode wave and the external magnetic field,
the conversion efficiency is 66%.",
cross-border conversion,http://arxiv.org/abs/1405.6824v1,"When Politicians Talk: Assessing Online Conversational Practices of
  Political Parties on Twitter","Assessing political conversations in social media requires a deeper
understanding of the underlying practices and styles that drive these
conversations. In this paper, we present a computational approach for assessing
online conversational practices of political parties. Following a deductive
approach, we devise a number of quantitative measures from a discussion of
theoretical constructs in sociological theory. The resulting measures make
different - mostly qualitative - aspects of online conversational practices
amenable to computation. We evaluate our computational approach by applying it
in a case study. In particular, we study online conversational practices of
German politicians on Twitter during the German federal election 2013. We find
that political parties share some interesting patterns of behavior, but also
exhibit some unique and interesting idiosyncrasies. Our work sheds light on (i)
how complex cultural phenomena such as online conversational practices are
amenable to quantification and (ii) the way social media such as Twitter are
utilized by political parties.",
cross-border conversion,http://arxiv.org/abs/1510.02527v1,"Efficient and low-noise single-photon-level frequency conversion
  interfaces using silicon nanophotonics","Optical frequency conversion has applications ranging from tunable light
sources to telecommunications-band interfaces for quantum information science.
Here, we demonstrate efficient, low-noise frequency conversion on a
nanophotonic chip through four-wave-mixing Bragg scattering in compact
(footprint < 0.5 x 10^-4 cm^2) Si3N4 microring resonators. We investigate three
frequency conversion configurations: (1) spectral translation over a few
nanometers within the 980 nm band, (2) upconversion from 1550 nm to 980 nm, and
(3) downconversion from 980 nm to 1550 nm. With conversion efficiencies ranging
from 25 % for the first process to > 60 % for the last two processes, a signal
conversion bandwidth > 1 GHz, < 60 mW of continuous-wave pump power needed, and
background noise levels between a few fW and a few pW, these devices are
suitable for quantum frequency conversion of single photon states from InAs
quantum dots. Simulations based on coupled mode equations and the
Lugiato-Lefever equation are used to model device performance, and show
quantitative agreement with measurements.","Nature Photonics, Vol. 10, pp. 406-414 (2016)"
cross-border conversion,http://arxiv.org/abs/1412.3511v1,"Optimizing optical Bragg scattering for single-photon frequency
  conversion","We develop a systematic theory for optimising single-photon frequency
conversion using optical Bragg scattering. The efficiency and phase-matching
conditions for the desired Bragg scattering conversion as well as spurious
scattering and modulation instability are identified. We find that third-order
dispersion can suppress unwanted processes, while dispersion above the fourth
order limits the maximum conversion efficiency. We apply the optimisation
conditions to frequency conversion in highly nonlinear fiber, silicon nitride
waveguides and silicon nanowires. Efficient conversion is confirmed using full
numerical simulations. These design rules will assist the development of
efficient quantum frequency conversion between multicolour single photon
sources for integration in complex quantum networks.","Phys. Rev. A 91, 013837 (2015)"
cross-border conversion,http://arxiv.org/abs/1604.04358v1,"StalemateBreaker: A Proactive Content-Introducing Approach to Automatic
  Human-Computer Conversation","Existing open-domain human-computer conversation systems are typically
passive: they either synthesize or retrieve a reply provided a human-issued
utterance. It is generally presumed that humans should take the role to lead
the conversation and introduce new content when a stalemate occurs, and that
the computer only needs to ""respond."" In this paper, we propose
StalemateBreaker, a conversation system that can proactively introduce new
content when appropriate. We design a pipeline to determine when, what, and how
to introduce new content during human-computer conversation. We further propose
a novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between
conversation context and candidate replies. Experiments show that both the
content-introducing approach and the reranking algorithm are effective. Our
full StalemateBreaker model outperforms a state-of-the-practice conversation
system by +14.4% p@1 when a stalemate occurs.",
cross-border conversion,http://arxiv.org/abs/1609.09028v2,"Stance Classification in Rumours as a Sequential Task Exploiting the
  Tree Structure of Social Media Conversations","Rumour stance classification, the task that determines if each tweet in a
collection discussing a rumour is supporting, denying, questioning or simply
commenting on the rumour, has been attracting substantial interest. Here we
introduce a novel approach that makes use of the sequence of transitions
observed in tree-structured conversation threads in Twitter. The conversation
threads are formed by harvesting users' replies to one another, which results
in a nested tree-like structure. Previous work addressing the stance
classification task has treated each tweet as a separate unit. Here we analyse
tweets by virtue of their position in a sequence and test two sequential
classifiers, Linear-Chain CRF and Tree CRF, each of which makes different
assumptions about the conversational structure. We experiment with eight
Twitter datasets, collected during breaking news, and show that exploiting the
sequential structure of Twitter conversations achieves significant improvements
over the non-sequential methods. Our work is the first to model Twitter
conversations as a tree structure in this manner, introducing a novel way of
tackling NLP tasks on Twitter conversations.",
cross-border conversion,http://arxiv.org/abs/1609.09547v1,"Competitive Propagation: Models, Asymptotic Behavior and Multi-stage
  Games","In this paper we propose a class of propagation models for multiple competing
products over a social network. We consider two propagation mechanisms: social
conversion and self conversion, corresponding, respectively, to endogenous and
exogenous factors. A novel concept, the product-conversion graph, is proposed
to characterize the interplay among competing products. According to the
chronological order of social and self conversions, we develop two Markov-chain
models and, based on the independence approximation, we approximate them with
two respective difference equations systems.
  Theoretical analysis on these two approximation models reveals the dependency
of the systems' asymptotic behavior on the structures of both the
product-conversion graph and the social network, as well as the initial
condition. In addition to the theoretical work, accuracy of the independence
approximation and the asymptotic behavior of the Markov-chain model are
investigated via numerical analysis, for the case where social conversion
occurs before self conversion.
  Finally, we propose a class of multi-player and multi-stage competitive
propagation games and discuss the seeding-quality trade-off, as well as the
allocation of seeding resources among the individuals. We investigate the
unique Nash equilibrium at each stage and analyze the system's behavior when
every player is adopting the policy at the Nash equilibrium.",
cross-border conversion,http://arxiv.org/abs/1709.05411v1,"Combining Search with Structured Data to Create a More Engaging User
  Experience in Open Domain Dialogue","The greatest challenges in building sophisticated open-domain conversational
agents arise directly from the potential for ongoing mixed-initiative
multi-turn dialogues, which do not follow a particular plan or pursue a
particular fixed information need. In order to make coherent conversational
contributions in this context, a conversational agent must be able to track the
types and attributes of the entities under discussion in the conversation and
know how they are related. In some cases, the agent can rely on structured
information sources to help identify the relevant semantic relations and
produce a turn, but in other cases, the only content available comes from
search, and it may be unclear which semantic relations hold between the search
results and the discourse context. A further constraint is that the system must
produce its contribution to the ongoing conversation in real-time. This paper
describes our experience building SlugBot for the 2017 Alexa Prize, and
discusses how we leveraged search and structured data from different sources to
help SlugBot produce dialogic turns and carry on conversations whose length
over the semi-finals user evaluation period averaged 8:17 minutes.",Search-Oriented Conversational AI (SCAI) 2017
cross-border conversion,http://arxiv.org/abs/1712.09943v3,Toward Continual Learning for Conversational Agents,"While end-to-end neural conversation models have led to promising advances in
reducing hand-crafted features and errors induced by the traditional complex
system architecture, they typically require an enormous amount of data due to
the lack of modularity. Previous studies adopted a hybrid approach with
knowledge-based components either to abstract out domain-specific information
or to augment data to cover more diverse patterns. On the contrary, we propose
to directly address the problem using recent developments in the space of
continual learning for neural models. Specifically, we adopt a
domain-independent neural conversational model and introduce a novel neural
continual learning algorithm that allows a conversational agent to accumulate
skills across different tasks in a data-efficient way. To the best of our
knowledge, this is the first work that applies continual learning to
conversation systems. We verified the efficacy of our method through a
conversational skill transfer from either synthetic dialogs or human-human
dialogs to human-computer conversations in a customer support domain.",
cross-border conversion,http://arxiv.org/abs/1801.08693v1,"Improved Finite Blocklength Converses for Slepian-Wolf Coding via Linear
  Programming","A new finite blocklength converse for the Slepian- Wolf coding problem is
presented which significantly improves on the best known converse for this
problem, due to Miyake and Kanaya [2]. To obtain this converse, an extension of
the linear programming (LP) based framework for finite blocklength point-
to-point coding problems from [3] is employed. However, a direct application of
this framework demands a complicated analysis for the Slepian-Wolf problem. An
analytically simpler approach is presented wherein LP-based finite blocklength
converses for this problem are synthesized from point-to-point lossless source
coding problems with perfect side-information at the decoder. New finite
blocklength metaconverses for these point-to-point problems are derived by
employing the LP-based framework, and the new converse for Slepian-Wolf coding
is obtained by an appropriate combination of these converses.",
cross-border conversion,http://arxiv.org/abs/1802.09055v1,"Domain Specific Design Patterns: Designing For Conversational User
  Interfaces","Designing conversational user interface experience is complicated because
conversation comes with many expectations. When these expectations are met, we
feel the interface is natural, but once violated, we feel something is amiss.
The last decade witnessed human language technologies and behaviours to enable
humans converse with software using spoken dialogue to access, create and
process information. Less is known about the practicalities of designing
chatbot interactions. In this paper, we introduce the nature of conversational
user interfaces (CUIs) and describe the underlying technologies they are based
on. Moreover, we define guidelines for designing conversational interfaces in
various domains. This paper particularly focuses on classifying the elements
and techniques used in CUI design patterns. After concluding certain challenges
with CUI, we discuss important features and chatbot states to be considered in
CUI design for specific domain. We envisage this study to support CUI
researchers to design tailored chatbots applicable into certain domain and
improve the current state of research challenges in the field of Artificial
Intelligence and conversational agents.",
cross-border conversion,http://arxiv.org/abs/1805.04335v1,Added mass: a complex face of tidal conversion,"This paper revisits the problem of tidal conversion at a ridge in a uniformly
stratified fluid of limited depth using measurements of complex-valued added
mass. When the height of a sub-marine ridge is non negligible with respect to
the depth of the water, the tidal conversion can be enhanced in the
supercritical regime or reduced in the subcritical regime with respect to the
large depth situation. Tidal conversion can even be null for some specific
cases. Here, we study experimentally the influence of finite depth on the added
mass coefficients for three diffierent ridge shapes. We first show that at low
forcing frequency the tidal conversion is weakly enhanced by shallow depth for
a semi-circular ridge. In addition, added mass coefficients measured for a
vertical ridge show strong similarities with the ones obtained for the
semi-circular ridge. Nevertheless, the enhancement of the tidal conversion at
low forcing frequency for the vertical ridge has not been observed, in contrast
with its supercritical shape. Finally, we provide the experimental evidence of
a lack of tidal conversion due to the specific shape of a ridge for certain
depth and frequency tuning.","Journal of Fluid Mechanics 831, 101-127 (2017)"
cross-border conversion,http://arxiv.org/abs/1806.06411v1,Measuring Semantic Coherence of a Conversation,"Conversational systems have become increasingly popular as a way for humans
to interact with computers. To be able to provide intelligent responses,
conversational systems must correctly model the structure and semantics of a
conversation. We introduce the task of measuring semantic (in)coherence in a
conversation with respect to background knowledge, which relies on the
identification of semantic relations between concepts introduced during a
conversation. We propose and evaluate graph-based and machine learning-based
approaches for measuring semantic coherence using knowledge graphs, their
vector space embeddings and word embedding models, as sources of background
knowledge. We demonstrate how these approaches are able to uncover different
coherence patterns in conversations on the Ubuntu Dialogue Corpus.",
cross-border conversion,http://arxiv.org/abs/1808.05366v1,"Strong Converse for Hypothesis Testing Against Independence over a
  Two-Hop Network","By proving a strong converse, we strengthen the weak converse result by
Salehkalaibar, Wigger and Wang (2017) concerning hypothesis testing against
independence over a two-hop network with communication constraints. Our proof
follows by judiciously combining two recently proposed techniques for proving
strong converse theorems, namely the strong converse technique via reverse
hypercontractivity by Liu, van Handel, and Verd\'u (2017) and the strong
converse technique by Tyagi and Watanabe (2018), in which the authors used a
change-of-measure technique and replaced hard Markov constraints with soft
information costs. The techniques used in our paper can also be applied to
prove strong converse theorems for other multiterminal hypothesis testing
against independence problems.",
cross-border conversion,http://arxiv.org/abs/1901.11462v1,"Exploring the context of recurrent neural network based conversational
  agents","Conversational agents have begun to rise both in the academic (in terms of
research) and commercial (in terms of applications) world. This paper
investigates the task of building a non-goal driven conversational agent, using
neural network generative models and analyzes how the conversation context is
handled. It compares a simpler Encoder-Decoder with a Hierarchical Recurrent
Encoder-Decoder architecture, which includes an additional module to model the
context of the conversation using previous utterances information. We found
that the hierarchical model was able to extract relevant context information
and include them in the generation of the output. However, it performed worse
(35-40%) than the simple Encoder-Decoder model regarding both grammatically
correct output and meaningful response. Despite these results, experiments
demonstrate how conversations about similar topics appear close to each other
in the context space due to the increased frequency of specific topic-related
words, thus leaving promising directions for future research and how the
context of a conversation can be exploited.",
cross-border conversion,http://arxiv.org/abs/1902.05224v1,Conversion from RLBWT to LZ77,"Converting a compressed format of a string into another compressed format
without an explicit decompression is one of the central research topics in
string processing. We discuss the problem of converting the run-length
Burrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases
of the reversed string. The first results with Policriti and Prezza's
conversion algorithm [Algorithmica 2018] were $O(n \log r)$ time and $O(r)$
working space for length of the string $n$, number of runs $r$ in the RLBWT,
and number of LZ77 phrases $z$. Recent results with Kempa's conversion
algorithm [SODA 2019] are $O(n / \log n + r \log^{9} n + z \log^{9} n)$ time
and $O(n / \log_{\sigma} n + r \log^{8} n)$ working space for the alphabet size
$\sigma$ of the RLBWT. In this paper, we present a new conversion algorithm by
improving Policriti and Prezza's conversion algorithm where dynamic data
structures for general purpose are used. We argue that these dynamic data
structures can be replaced and present new data structures for faster
conversion. The time and working space of our conversion algorithm with new
data structures are $O(n \min \{ \log \log n, \sqrt{\frac{\log r}{\log\log r}}
\})$ and $O(r)$, respectively.",
cross-border conversion,http://arxiv.org/abs/1904.02760v2,An End-to-End Conversational Style Matching Agent,"We present an end-to-end voice-based conversational agent that is able to
engage in naturalistic multi-turn dialogue and align with the interlocutor's
conversational style. The system uses a series of deep neural network
components for speech recognition, dialogue generation, prosodic analysis and
speech synthesis to generate language and prosodic expression with qualities
that match those of the user. We conducted a user study (N=30) in which
participants talked with the agent for 15 to 20 minutes, resulting in over 8
hours of natural interaction data. Users with high consideration conversational
styles reported the agent to be more trustworthy when it matched their
conversational style. Whereas, users with high involvement conversational
styles were indifferent. Finally, we provide design guidelines for multi-turn
dialogue interactions using conversational style adaptation.","Proceedings of the 19th ACM International Conference on
  Intelligent Virtual Agents (2019) 111-118"
cross-border conversion,http://arxiv.org/abs/1905.06716v1,"A Method to Discover Digital Collaborative Conversations in Business
  Collaborations","Many companies have a suite of digital tools, such as Enterprise Social
Networks, conferencing and document sharing software, and email, to facilitate
collaboration among employees. During, or at the end of a collaboration,
documents are often produced. People who were not involved in the initial
collaboration often have difficulties understanding parts of its content
because they are lacking the overall context. We argue there is valuable
contextual and collaborative knowledge contained in these tools (content and
use) that can be used to understand the document. Our goal is to rebuild the
conversations that took place over a messaging service and their links with a
digital conferencing tool during document production. The novelty in our
approach is to combine several conversation-threading methods to identify
interesting links between distinct conversations. Specifically we combine
header-field information with social, temporal and semantic proximities. Our
findings suggest the messaging service and conferencing tool are used in a
complementary way. The primary results confirm that combining different
conversation threading approaches is efficient to detect and construct
conversation threads from distinct digital conversations concerning the same
document.","10th International Joint Conference on Knowledge Discovery,
  Knowledge Engineering and Knowledge Management, May 2018, Seville, Spain"
cross-border conversion,http://arxiv.org/abs/1905.08796v1,Acoustic-to-Word Models with Conversational Context Information,"Conversational context information, higher-level knowledge that spans across
sentences, can help to recognize a long conversation. However, existing speech
recognition models are typically built at a sentence level, and thus it may not
capture important conversational context information. The recent progress in
end-to-end speech recognition enables integrating context with other available
information (e.g., acoustic, linguistic resources) and directly recognizing
words from speech. In this work, we present a direct acoustic-to-word,
end-to-end speech recognition model capable of utilizing the conversational
context to better process long conversations. We evaluate our proposed approach
on the Switchboard conversational speech corpus and show that our system
outperforms a standard end-to-end speech recognition system.",
cross-border conversion,http://arxiv.org/abs/1907.10235v1,"Predicting Different Types of Conversions with Multi-Task Learning in
  Online Advertising","Conversion prediction plays an important role in online advertising since
Cost-Per-Action (CPA) has become one of the primary campaign performance
objectives in the industry. Unlike click prediction, conversions have different
types in nature, and each type may be associated with different decisive
factors. In this paper, we formulate conversion prediction as a multi-task
learning problem, so that the prediction models for different types of
conversions can be learned together. These models share feature
representations, but have their specific parameters, providing the benefit of
information-sharing across all tasks. We then propose Multi-Task Field-weighted
Factorization Machine (MT-FwFM) to solve these tasks jointly. Our experiment
results show that, compared with two state-of-the-art models, MT-FwFM improve
the AUC by 0.74% and 0.84% on two conversion types, and the weighted AUC across
all conversion types is also improved by 0.50%.",SIGKDD 2019
cross-border conversion,http://arxiv.org/abs/1908.00059v1,"GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for
  Conversational Machine Comprehension","Conversational machine reading comprehension (MRC) has proven significantly
more challenging compared to traditional MRC since it requires better
utilization of conversation history. However, most existing approaches do not
effectively capture conversation history and thus have trouble handling
questions involving coreference or ellipsis. We propose a novel graph neural
network (GNN) based model, namely GraphFlow, which captures conversational flow
in the dialog. Specifically, we first propose a new approach to dynamically
construct a question-aware context graph from passage text at each turn. We
then present a novel flow mechanism to model the temporal dependencies in the
sequence of context graphs. The proposed GraphFlow model shows superior
performance compared to existing state-of-the-art methods. For instance,
GraphFlow outperforms two recently proposed models on the CoQA benchmark
dataset: FlowQA by 2.3% and SDNet by 0.7% on F1 score, respectively. In
addition, visualization experiments show that our proposed model can better
mimic the human reasoning process for conversational MRC compared to existing
models.",
cross-border conversion,http://arxiv.org/abs/1909.10743v1,"An Empirical Study of Content Understanding in Conversational Question
  Answering","With a lot of work about context-free question answering systems, there is an
emerging trend of conversational question answering models in the natural
language processing field. Thanks to the recently collected datasets, including
QuAC and CoQA, there has been more work on conversational question answering,
and some models have achieved competitive performance on both datasets.
However, to best of our knowledge, two important questions for conversational
comprehension research have not been well studied: 1) How well can the
benchmark dataset reflect models' content understanding? 2) Do the models
utilize the conversation content well? To investigate the two questions, we
design different training settings, testing settings, as well as an attack to
verify the models' capability of content understanding on QuAC and CoQA. The
results indicate some potential hazards of using QuAC and CoQA for
conversational comprehension research. Our analysis also sheds some light one
both models and datasets. Given deeper investigation of the task, it is
believed that this work is beneficial to the future progress of conversation
comprehension.",
cross-border conversion,http://arxiv.org/abs/1910.04980v2,"Emotion Recognition in Conversations with Transfer Learning from
  Generative Conversation Modeling","Recognizing emotions in conversations is a challenging task due to the
presence of contextual dependencies governed by self- and inter-personal
influences. Recent approaches have focused on modeling these dependencies
primarily via supervised learning. However, purely supervised strategies demand
large amounts of annotated data, which is lacking in most of the available
corpora in this task. To tackle this challenge, we look at transfer learning
approaches as a viable alternative. Given the large amount of available
conversational data, we investigate whether generative conversational models
can be leveraged to transfer affective knowledge for the target task of
detecting emotions in context. We propose an approach where we first train a
neural dialogue model and then perform parameter transfer to initiate our
target model. Apart from the traditional pre-trained sentence encoders, we also
incorporate parameter transfer from the recurrent components that model
inter-sentence context across the whole conversation. Based on this idea, we
perform several experiments across multiple datasets and find improvement in
performance and robustness against limited training data. Our models also
achieve better validation performances in significantly fewer epochs. Overall,
we infer that knowledge acquired from dialogue generators can indeed help
recognize emotions in conversations.",
cross-border conversion,http://arxiv.org/abs/1606.01582v1,"Quantum frequency conversion and strong coupling of photonic modes using
  four-wave mixing in integrated microresonators","Single photon-level quantum frequency conversion has recently been
demonstrated using silicon nitride microring resonators. The resonance
enhancement offered by such systems enables high-efficiency translation of
quantum states of light across wide frequency ranges at sub-watt pump powers.
Using a quantum-mechanical Hamiltonian formalism, we present a detailed
theoretical analysis of the conversion dynamics in these systems, and show that
they are capable of converting single- and multi-photon quantum states.
Analytic formulas for the conversion efficiency, spectral conversion
probability density, and pump power requirements are derived which are in good
agreement with previous theoretical and experimental results. We show that with
only modest improvement to the state of the art, efficiencies exceeding 95% are
achievable using less than 100 mW of pump power. At the critical driving
strength that yields maximum conversion efficiency, the spectral conversion
probability density is shown to exhibit a flat-topped peak, indicating a range
of insensitivity to the spectrum of a single photon input. Two alternate
theoretical approaches are presented to study the conversion dynamics: a
dressed mode approach that yields a better intuitive picture of the conversion
process, and a study of the temporal dynamics of the participating modes in the
resonator, which uncovers a regime of Rabi-like coherent oscillations of single
photons between two different frequency modes. This oscillatory regime arises
from the strong coupling of distinct frequency modes mediated by coherent
pumps.","Phys. Rev. A 94, 023810 (2016)"
cross-border conversion,http://arxiv.org/abs/1706.07440v2,End-to-end Conversation Modeling Track in DSTC6,"End-to-end training of neural networks is a promising approach to automatic
construction of dialog systems using a human-to-human dialog corpus. Recently,
Vinyals et al. tested neural conversation models using OpenSubtitles. Lowe et
al. released the Ubuntu Dialogue Corpus for researching unstructured multi-turn
dialogue systems. Furthermore, the approach has been extended to accomplish
task oriented dialogs to provide information properly with natural
conversation. For example, Ghazvininejad et al. proposed a knowledge grounded
neural conversation model [3], where the research is aiming at combining
conversational dialogs with task-oriented knowledge using unstructured data
such as Twitter data for conversation and Foursquare data for external
knowledge.However, the task is still limited to a restaurant information
service, and has not yet been tested with a wide variety of dialog tasks. In
addition, it is still unclear how to create intelligent dialog systems that can
respond like a human agent.
  In consideration of these problems, we proposed a challenge track to the 6th
dialog system technology challenges (DSTC6) using human-to-human dialog data to
mimic human dialog behaviors. The focus of the challenge track is to train
end-to-end conversation models from human-to-human conversation and accomplish
end-to-end dialog tasks in various situations assuming a customer service, in
which a system plays a role of human agent and generates natural and
informative sentences in response to user's questions or comments given dialog
context.",
cross-border conversion,http://arxiv.org/abs/1708.03044v1,"""Is there anything else I can help you with?"": Challenges in Deploying
  an On-Demand Crowd-Powered Conversational Agent","Intelligent conversational assistants, such as Apple's Siri, Microsoft's
Cortana, and Amazon's Echo, have quickly become a part of our digital life.
However, these assistants have major limitations, which prevents users from
conversing with them as they would with human dialog partners. This limits our
ability to observe how users really want to interact with the underlying
system. To address this problem, we developed a crowd-powered conversational
assistant, Chorus, and deployed it to see how users and workers would interact
together when mediated by the system. Chorus sophisticatedly converses with end
users over time by recruiting workers on demand, which in turn decide what
might be the best response for each user sentence. Up to the first month of our
deployment, 59 users have held conversations with Chorus during 320
conversational sessions. In this paper, we present an account of Chorus'
deployment, with a focus on four challenges: (i) identifying when conversations
are over, (ii) malicious users and workers, (iii) on-demand recruiting, and
(iv) settings in which consensus is not enough. Our observations could assist
the deployment of crowd-powered conversation systems and crowd-powered systems
in general.",
cross-border division,http://arxiv.org/abs/1101.2798v1,On Logical Extension of Algebraic Division,"Basic arithmetic is the cornerstone of mathematics and computer sciences. In
arithmetic, 'division by zero' is an undefined operation and any attempt at
extending logic for algebraic division to incorporate division by zero has
resulted in paradoxes and fallacies. However, there is no proven theorem or
mathematical logic that suggests that, defining logic for division by zero
would result in break-down of theory. Basing on this motivation, in this paper,
we attempt at logically defining a solution for 'division by zero' problem.",
cross-border division,http://arxiv.org/abs/1006.3961v1,An electrostatic model for biological cell division,"Probably the most fundamental processes for biological systems is their
ability to create themselves through the use of cell division and cell
differentiation. In this work a simple physical model is proposed for
biological cell division. The model consists of a positive ionic gradient
across the cell membrane, and concentration of charge at the nodes of the
spindle and on the chromosomes. A simple calculation, based on Coulomb's Law,
shows that under such circumstances a chromosome will tend to break up to its
constituent chromatids and that the chromatids will be separated by a distance
that is an order of thirty percent of the distance between the spindle nodes.
Further repulsion between the nodes will tend to stretch the cell and
eventually break the cell membrane between the separated chromatids, leading to
cell division. The importance of this work is in continuing the understanding
of the electromagnetic basis of cell division and providing it with an
analytical model. A central implication of this and other studies is to give
theoretical support to the notion that cell division can be manipulated by
electromagnetic means. Requirements on the ingredients of more sophisticated
models for biological cell division will also discussed.
  Keywords: Charge Separation, Cell Division, Mitosis",
cross-border division,http://arxiv.org/abs/1405.0878v1,Market Coupling as the Universal Algorithm to Assess Zonal Divisions,"Adopting a zonal structure of electricity market requires specification of
zones' borders. In this paper we use social welfare as the measure to assess
quality of various zonal divisions. The social welfare is calculated by Market
Coupling algorithm. The analyzed divisions are found by the usage of extended
Locational Marginal Prices (LMP) methodology presented in paper [1], which
takes into account variable weather conditions. The offered method of
assessment of a proposed division of market into zones is however not limited
to LMP approach but can evaluate the social welfare of divisions obtained by
any methodology.",
cross-border division,http://arxiv.org/abs/1702.05825v1,Sustainable Fair Division,"In this paper, I summarize our work on online fair division. In particular, I
present two models for online fair division: (1) one existing model for fair
division in food banks and (2) one new model for fair division of deceased
organs to patients. I further discuss simple mechanisms for these models that
allocate the resources as they arrive to agents. In practice, agents are often
risk-averse having imperfect information. Within this assumption, I report
several interesting axiomatic and complexity results for these mechanisms and
conclude with future work.",
cross-border division,http://arxiv.org/abs/1707.02871v1,How to cut a cake with a gram matrix,"In this article we study the problem of fair division. In particular we study
a notion introduced by J. Barbanel that generalizes super envy-free fair
division. We give a new proof of his result. Our approach allows us to give an
explicit bound for this kind of fair division. Furthermore, we also give a
theoretical answer to an open problem posed by Barbanel in 1996. Roughly
speaking, this question is: how can we decide if there exists a fair division
satisfying some inequalities constraints? Furthermore, when all the measures
are given with piecewise constant density functions then we show how to
construct effectively such a fair division.",
cross-border division,http://arxiv.org/abs/1804.03833v2,Don't cry to be the first!Symmetric fair division exist,"In this article we study a cake cutting problem. More precisely, we study
symmetric fair division algorithms, that is to say we study algorithms where
the order of the players do not influence the value obtained by each player. In
the first part of the article, we give a symmetric and envy-free fair division
algorithm. More precisely, we show how to get a symmetric and envy-free fair
division algorithm from an envy-free division algorithm. In the second part, we
give a proportional and symmetric fair division algorithm with a complexity in
O(n 3) in the Robertson-Webb model of complexity. This algorithm is based on
Kuhn's algorithm. Furthermore, our study has led us to introduce a new notion:
aristotelian fair division. This notion is an interpretation of Aristotle's
principle: give equal shares to equal people. We conclude this article with a
discussion and some questions about the Robertson-Webb model of computation.",
company law package,http://arxiv.org/abs/physics/0506066v1,Pareto index induced from the scale of companies,"Employing profits data of Japanese companies in 2002 and 2003, we confirm
that Pareto's law and the Pareto index are derived from the law of detailed
balance and Gibrat's law. The last two laws are observed beyond the region
where Pareto's law holds. By classifying companies into job categories, we find
that companies in a small scale job category have more possibilities of growing
than those in a large scale job category. This kinematically explains that the
Pareto index for the companies in the small scale job class is larger than that
for the companies in the large scale job class.",
company law package,http://arxiv.org/abs/physics/9708013v1,Self-pinching of a relativistic electron bunch in a drift tube,"Electron bunches with charge densities $\rho$ of the order of $10^2$ to
$10^3$ [nC/cm$^3$], energies between $20.$ and $100.$ [MeV], peak current
$>100$ [A], bunch lengths between 0.3 and 1.8 [cm], and bunch charge of 2.0 to
$20.$ [nC] are relevant to the design of Free Electron Lasers and future linear
colliders. In this paper we present the results of numerical simulations
performed with a particle in a cell (pic) code of an electron bunch in a drift
tube. The electron bunch has cylindrical symmetry with the $z$-axis oriented in
the direction of motion. The charge density distribution is constant in the
radial and Gaussian in the longitudinal direction, respectively. The electron
bunch experiences both a radial pinch in the middle of the pulse, corresponding
to the peak electron density, and a significant growth of the correlated
emittance. This behavior is explained, and an approximate scaling law is
identified. Comparisons of the results from the pic and PARMELA codes are
presented.",
company law package,http://arxiv.org/abs/1312.2069v1,"Applying the Apriori algorithm for investigating the relationships
  between demographic characteristics of Iranian top 100 enterprises and the
  strcture of their commercial website","This study was conducted with the main aim to investigate the relationships
between demographic characteristics of companies and the facilities required
for their commercial websites. The research samples are the top 100 Iranian
companies as ranked by the Iranian Industrial Management Institute; the method
applied is datamining, using Association Rules throught the Apriori algorithms.
To collect the data, an aithor-modified check list has been utilized, coverig
the three areas of faclities within commercial websites, i.e. fundamental,
information-providing, and service-delivering facilities. having extracted the
association rules between the mentioned two sets of variables, 68 rules with a
confidence rate of 90% and above were obtained, and based on their significance
were classified into two groups of must-have and should-have requirements; a
recommended package of facilities is hitherto offered to other companies which
intend to enter e-commerce through their commerical websites with regards to
each company's unique demographic characteristics.","International Journal of Data Mining & Knowledge Management
  Process (IJDKP) Vol.3, No.6, November 2013"
company law package,http://arxiv.org/abs/1811.08660v1,"The Unwanted Sharing Economy: An Analysis of Cookie Syncing and User
  Transparency under GDPR","The European General Data Protection Regulation (GDPR), which went into
effect in May 2018, leads to important changes in this area: companies are now
required to ask for users' consent before collecting and sharing personal data
and by law users now have the right to gain access to the personal information
collected about them.
  In this paper, we study and evaluate the effect of the GDPR on the online
advertising ecosystem. In a first step, we measure the impact of the
legislation on the connections (regarding cookie syncing) between third-parties
and show that the general structure how the entities are arranged is not
affected by the GDPR. However, we find that the new regulation has a
statistically significant impact on the number of connections, which shrinks by
around 40%. Furthermore, we analyze the right to data portability by evaluating
the subject access right process of popular companies in this ecosystem and
observe differences between the processes implemented by the companies and how
they interpret the new legislation. We exercised our right of access under GDPR
with 36 companies that had tracked us online. Although 32 companies (89%) we
inquired replied within the period defined by law, only 21 (58%) finished the
process by the deadline set in the GDPR. Our work has implications regarding
the implementation of privacy law as well as what online tracking companies
should do to be more compliant with the new regulation.",
company law package,http://arxiv.org/abs/physics/0702248v1,"The uniqueness of company size distribution function from tent-shaped
  growth rate distribution","We report the proof that the extension of Gibrat's law in the middle scale
region is unique and the probability distribution function (pdf) is also
uniquely derived from the extended Gibrat's law and the law of detailed
balance. In the proof, two approximations are employed. The pdf of growth rate
is described as tent-shaped exponential functions and the value of the origin
of the growth rate distribution is constant. These approximations are confirmed
in profits data of Japanese companies 2003 and 2004. The resultant profits pdf
fits with the empirical data with high accuracy. This guarantees the validity
of the approximations.",
company law package,http://arxiv.org/abs/physics/0503177v1,Shareholding Networks in Japan,"The Japanese shareholding network existing at the end of March 2002 is
studied empirically. The network is constructed from 2,303 listed companies and
53 non-listed financial institutions. We consider this network as a directed
graph by drawing edges from shareholders to stock corporations. The lengths of
the shareholder lists vary with the companies, and the most comprehensive lists
contain the top 30 shareholders. Consequently, the distribution of incoming
edges has an upper bound, while that of outgoing edges has no bound. The
distribution of outgoing degrees is well explained by the power law function
with an exponential tail. The exponent in the power law range is gamma=1.7. To
understand these features from the viewpoint of a company's growth, we consider
the correlations between the outgoing degree and the company's age, profit, and
total assets.",
company law package,http://arxiv.org/abs/1907.06538v1,"Patterns of Effort Contribution and Demand and User Classification based
  on Participation Patterns in NPM Ecosystem","Background: Open source requires participation of volunteer and commercial
developers (users) in order to deliver functional high-quality components.
Developers both contribute effort in the form of patches and demand effort from
the component maintainers to resolve issues reported against it. Aim: Identify
and characterize patterns of effort contribution and demand throughout the open
source supply chain and investigate if and how these patterns vary with
developer activity; identify different groups of developers; and predict
developers' company affiliation based on their participation patterns. Method:
1,376,946 issues and pull-requests created for 4433 NPM packages with over
10,000 monthly downloads and full (public) commit activity data of the 272,142
issue creators is obtained and analyzed and dependencies on NPM packages are
identified. Fuzzy c-means clustering algorithm is used to find the groups among
the users based on their effort contribution and demand patterns, and Random
Forest is used as the predictive modeling technique to identify their company
affiliations. Result: Users contribute and demand effort primarily from
packages that they depend on directly with only a tiny fraction of
contributions and demand going to transitive dependencies. A significant
portion of demand goes into packages outside the users' respective supply
chains (constructed based on publicly visible version control data). Three and
two different groups of users are observed based on the effort demand and
effort contribution patterns respectively. The Random Forest model used for
identifying the company affiliation of the users gives a AUC-ROC value of 0.68.
Conclusion: Our results give new insights into effort demand and supply at
different parts of the supply chain of the NPM ecosystem and its users and
suggests the need to increase visibility further upstream.",
company law package,http://arxiv.org/abs/physics/0512124v2,Re-examination of the size distribution of firms,"In this paper we address the question of the size distribution of firms. To
this aim, we use the Bloomberg database comprising multinational firms within
the years 1995-2003, and analyze the data of the sales and the total assets of
the separate financial statement of the Japanese and the US companies, and make
a comparison of the size distributions between the Japanese companies and the
US companies. We find that (i) the size distribution of the US firms is
approximately log-normal, in agreement with Gibrat's observation (Gibrat 1931),
and in contrast (ii) the size distribution of the Japanese firms is clearly not
log-normal, and the upper tail of the size distribution follows the Pareto law.
It agree with the predictions of the Simon model (Simon 1955). Key words: the
size distribution of firms, the Gibrat's law, and the Pareto law",Evolutionary and Institutional Economics Review 2-2 (2006)
company law package,http://arxiv.org/abs/physics/0508178v1,Derivation of the distribution from extended Gibrat's law,"Employing profits data of Japanese companies in 2002 and 2003, we identify
the non-Gibrat's law which holds in the middle profits region. From the law of
detailed balance in all regions, Gibrat's law in the high region and the
non-Gibrat's law in the middle region, we kinematically derive the profits
distribution function in the high and middle range uniformly. The distribution
function accurately fits with empirical data without any fitting parameter.",
company law package,http://arxiv.org/abs/physics/0503017v1,Financial Networks in the Korean Stock Exchange Market,"We investigate the financial network in the Korean stock exchange (KSE)
market, using both numerical simulations and scaling arguments. We estimate the
cross-correlation on the stock price exchanges of all companies listed on the
the Korean stock exchange market, where all companies are fully connected via
weighted links, by introducing a weighted random graph. The degree distribution
and the edge density are discussed numerically from the market graph, and the
statistical analysis for the degree distribution of vertices is particularly
found to approximately follow the power law.",
company law package,http://arxiv.org/abs/1309.5351v1,Human Resource Management System,"The paper titled HUMAN RESOURCE MANAGEMENT SYSTEM is basically concerned with
managing the Administrator of HUMAN RESOURCE Department in a company. A Human
Resource Management System, refers to the systems and processes at the
intersection between human resource management and information technology. It
merges HRM as a discipline and in particular its basic HR activities and
processes with the information technology field, whereas the programming of
data processing systems evolved into standardized routines and packages of
enterprise resource planning software. The main objective of this paper is to
reduce the effort of Administrator to keep the daily events such as attendance,
projects, works, appointments, etc. This paper deals with the process of
identifying the employees, recording their attendance hourly and calculating
their effective payable hours or days. This paper should maintain the records
of each and every employee and their time spend in to company, which can be
used for performance appraisal. Based on that transfer, removal, promotion can
be done.",
company law package,http://arxiv.org/abs/1909.02674v1,"Willing Buyer, Willing Seller: Personal Data Trade as a Service","There is an increased sensitivity by people about how companies collect
information about them, and how this information is packaged, used and sold.
This perceived lack of control is highlighted by the helplessness of users of
various platforms in managing or halting what data is collected from/about
them. In a future where users have wrested control of their data and have the
autonomy to decide what information is collected, how it is used and most
importantly, how much it is worth, a new market emerges. This design fiction
considers possible steps prescient companies would take to meet these demands,
such as providing third-party subscription platforms offering personal data
trade as a service. These services would provide a means for transparent
transactions that preserve an owner's control over their data; allowing them to
individually make decisions about what data they avail for sale, and the amount
of compensation they would accept in trade.",
company law package,http://arxiv.org/abs/1808.04617v2,"Joint Ground and Aerial Package Delivery Services: A Stochastic
  Optimization Approach","Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a
promising mode of fast, energy-efficient, and cost-effective package delivery.
A considerable number of works have studied different aspects of drone package
delivery service by a supplier, one of which is delivery planning. However,
existing works addressing the planning issues consider a simple case of perfect
delivery without service interruption, e.g., due to accident which is common
and realistic. Therefore, this paper introduces the joint ground and aerial
delivery service optimization and planning (GADOP) framework. The framework
explicitly incorporates uncertainty of drone package delivery, i.e., takeoff
and breakdown conditions. The GADOP framework aims to minimize the total
delivery cost given practical constraints, e.g., traveling distance limit.
Specifically, we formulate the GADOP framework as a three-stage stochastic
integer programming model. To deal with the high complexity issue of the
problem, a decomposition method is adopted. Then, the performance of the GADOP
framework is evaluated by using two data sets including Solomon benchmark suite
and the real data from one of the Singapore logistics companies. The
performance evaluation clearly shows that the GADOP framework can achieve
significantly lower total payment than that of the baseline methods which do
not take uncertainty into account.",Transactions on Intelligent Transportation Systems 2018
company law package,http://arxiv.org/abs/0807.0014v1,"Empirical Tests of Zipf's law Mechanism In Open Source Linux
  Distribution","The evolution of open source software projects in Linux distributions offers
a remarkable example of a growing complex self-organizing adaptive system,
exhibiting Zipf's law over four full decades. We present three tests of the
usually assumed ingredients of stochastic growth models that have been
previously conjectured to be at the origin of Zipf's law: (i) the growth
observed between successive releases of the number of in-directed links of
packages obeys Gibrat's law of proportional growth; (ii) the average growth
increment of the number of in-directed links of packages over a time interval
$\Delta t$ is proportional to $\Delta t$, while its standard deviation is
proportional to $\sqrt{\Delta t}$; (iii) the distribution of the number of
in-directed links of new packages appearing in evolving versions of Debian
Linux distributions has a tail thinner than Zipf's law, with an exponent which
converges to the Zipf's law value 1 as the time $\Delta t$ between releases
increases.","Physical Review Letters 101, 218701 (2008)"
company law package,http://arxiv.org/abs/physics/0502005v1,Heterogeneous Economic Networks,"The Japanese shareholding network at the end of March 2002 is studied. To
understand the characteristics of this network intuitively, we visualize it as
a directed graph and an adjacency matrix. Especially detailed features of
networks concerned with the automobile industry sector are discussed by using
the visualized networks. The shareholding network is also considered as an
undirected graph, because many quantities characterizing networks are defined
for undirected cases. For this undirected shareholding network, we show that a
degree distribution is well fitted by a power law function with an exponential
tail. The exponent in the power law range is gamma=1.8. We also show that the
spectrum of this network follows asymptotically the power law distribution with
the exponent delta=2.6. By comparison with gamma and delta, we find a scaling
relation delta=2gamma-1. The reason why this relation holds is attributed to
the local tree-like structure of networks. To clarify this structure, the
correlation between degrees and clustering coefficients is considered. We show
that this correlation is negative and fitted by the power law function with the
exponent alpha=1.1. This guarantees the local tree-like structure of the
network and suggests the existence of a hierarchical structure. We also show
that the degree correlation is negative and follows the power law function with
the exponent nu=0.8. This indicates a degree-nonassortative network, in which
hubs are not directly connected with each other. To understand these features
of the network from the viewpoint of a company's growth, we consider the
correlation between the degree and the company's total assets and age. It is
clarified that the degree and the company's total assets correlate strongly,
but the degree and the company's age have no correlation.",
company law package,http://arxiv.org/abs/0805.0922v1,High Density Through Silicon Via (TSV),"The Through Silicon Via (TSV) process developed by Silex provides down to 30
micrometers pitch for through wafer connections in up to 600 micrometers thick
substrates. Integrated with MEMS designs it enables significantly reduced die
size and true ""Wafer Level Packaging"" - features that are particularly
important in consumer market applications. The TSV technology also enables
integration of advanced interconnect functions in optical MEMS, sensors and
microfluidic devices. In addition the Via technology opens for very interesting
possibilities considering integration with CMOS processing. With several
companies using the process already today, qualified volume manufacturing in
place and a line-up of potential users, the process is becoming a standard in
the MEMS industry. We provide a introduction to the via formation process and
also present some on the novel solutions made available by the technology.","Dans Symposium on Design, Test, Integration and Packaging of
  MEMS/MOEMS - DTIP 2008, Nice : France (2008)"
company law package,http://arxiv.org/abs/1502.03663v1,MPPC versus MRS APD in two-phase Cryogenic Avalanche Detectors,"Two-phase Cryogenic Avalanche Detectors (CRADs) with combined THGEM/GAPD
multiplier have become an emerging potential technique for dark matter search
and coherent neutrino-nucleus scattering experiments. In such a multiplier the
THGEM hole avalanches are optically recorded in the Near Infrared (NIR) using a
matrix of Geiger-mode APDs (GAPDs). To select the proper sensor, the
performances of six GAPD types manufactured by different companies, namely by
Hamamatsu (MPPCs), CPTA (MRS APDs) and SensL (SiPMs), have been comparatively
studied at cryogenic temperatures when operated in two-phase CRADs in Ar at 87
K. While the GAPDs with ceramic packages failed to operate properly at
cryogenic temperatures, those with plastic packages, namely MPPC S10931-100P
and MRS APD 149-35, showed satisfactory performances at 87 K. In addition, MPPC
S10931-100P turned out to be superior in terms of the higher detection
efficiency, lower nose rate, lower pixel quenching resistor and better
characteristics reproducibility.",2015 JINST 10 P04013
company law package,http://arxiv.org/abs/1806.03688v1,"LexNLP: Natural language processing and information extraction for legal
  and regulatory texts","LexNLP is an open source Python package focused on natural language
processing and machine learning for legal and regulatory text. The package
includes functionality to (i) segment documents, (ii) identify key text such as
titles and section headings, (iii) extract over eighteen types of structured
information like distances and dates, (iv) extract named entities such as
companies and geopolitical entities, (v) transform text into features for model
training, and (vi) build unsupervised and supervised models such as word
embedding or tagging models. LexNLP includes pre-trained models based on
thousands of unit tests drawn from real documents available from the SEC EDGAR
database as well as various judicial and regulatory proceedings. LexNLP is
designed for use in both academic research and industrial applications, and is
distributed at https://github.com/LexPredict/lexpredict-lexnlp.",
company law package,http://arxiv.org/abs/physics/0612104v1,"New version of PLNoise: a package for exact numerical simulation of
  power-law noises","In a recent paper I have introduced a package for the exact simulation of
power-law noises and other colored noises (E. Milotti, Comput. Phys. Commun.
{\bf 175} (2006) 212): in particular the algorithm generates $1/f^\alpha$
noises with $0 < \alpha \leq 2$. Here I extend the algorithm to generate
$1/f^\alpha$ noises with $2 < \alpha \leq 4$ (black noises). The method is
exact in the sense that it produces a sampled process with a theoretically
guaranteed range-limited power-law spectrum for any arbitrary sequence of
sampling intervals, i.e., the sampling times may be unevenly spaced.",
company law package,http://arxiv.org/abs/physics/0607217v1,"The uniqueness of the profits distribution function in the middle scale
  region","We report the proof that the expression of extended Gibrat's law is unique
and the probability distribution function (pdf) is also uniquely derived from
the law of detailed balance and the extended Gibrat's law. In the proof, two
approximations are employed that the pdf of growth rate is described as
tent-shaped exponential functions and that the value of the origin of growth
rate is constant. These approximations are confirmed in profits data of
Japanese companies 2003 and 2004. The resultant profits pdf fits with the
empirical data with high accuracy. This guarantees the validity of the
approximations.",
company law package,http://arxiv.org/abs/physics/0504045v1,Change of ownership networks in Japan,"As complex networks in economics, we consider Japanese shareholding networks
as they existed in 1985, 1990, 1995, 2000, 2002, and 2003. In this study, we
use as data lists of shareholders for companies listed on the stock market or
on the over-the-counter market. The lengths of the shareholder lists vary with
the companies, and we use lists for the top 20 shareholders. We represent these
shareholding networks as a directed graph by drawing arrows from shareholders
to stock corporations. Consequently, the distribution of incoming edges has an
upper bound, while that of outgoing edges has no bound. This representation
shows that for all years the distributions of outgoing degrees can be well
explained by the power law function with an exponential tail. The exponent
depends on the year and the country, while the power law shape is maintained
universally. We show that the exponent strongly correlates with the long-term
shareholding rate and the cross-shareholding rate.",
company law package,http://arxiv.org/abs/1811.01077v2,Dynamic Pricing under a Static Calendar,"This work is motivated by our collaboration with a large Consumer Packaged
Goods (CPG) company. We have found that while they appreciate the advantages of
dynamic pricing, they deem it operationally much easier to plan out a static
price calendar in advance.
  In this paper, we investigate the efficacy of static control policies for
dynamic revenue management problems. In these problems, a firm has limited
inventory to sell over a finite time horizon where demand is known but
stochastic. We consider both pricing and assortment controls, and derive simple
static policies in the form of a price calendar or a planned sequence of
assortments, respectively. We show that our policies are within 1-1/e
(approximately 0.63) of the optimum under stationary (IID) demand, and 1/2 of
optimum under non-stationary demand, with both guarantees approaching 1 if the
starting inventory is large.
  A main contribution of this work is developing a system of tools for
establishing best-possible performance guarantees relative to linear
programming relaxations: in the stationary setting, structural properties about
static policies which provide a complete characterization of tight bounds; and
in the non-stationary setting, an adaptation of the prophet inequalities from
optimal stopping theory to pricing and assortment problems.
  Finally, we demonstrate on data from the CPG company that our simple price
calendars are effective.",
company law package,http://arxiv.org/abs/1305.0215v3,Powerlaw: a Python package for analysis of heavy-tailed distributions,"Power laws are theoretically interesting probability distributions that are
also frequently used to describe empirical data. In recent years effective
statistical methods for fitting power laws have been developed, but appropriate
use of these techniques requires significant programming and statistical
insight. In order to greatly decrease the barriers to using good statistical
methods for fitting power law distributions, we developed the powerlaw Python
package. This software package provides easy commands for basic fitting and
statistical analysis of distributions. Notably, it also seeks to support a
variety of user needs by being exhaustive in the options available to the user.
The source code is publicly available and easily extensible.",PLoS ONE 9(1): e85777
company law package,http://arxiv.org/abs/1006.4555v1,Law-Aware Access Control and its Information Model,"Cross-border access to a variety of data such as market information,
strategic information, or customer-related information defines the daily
business of many global companies, including financial institutions. These
companies are obliged by law to keep a data processing legal for all offered
services. They need to fulfill different security objectives specified by the
legislation. Therefore, they control access to prevent unauthorized users from
using data. Those security objectives, for example confidentiality or secrecy,
are often defined in the eXtensible Access Control Markup Language that
promotes interoperability between different systems. In this paper, we show the
necessity of incorporating the requirements of legislation into access control.
Based on the work flow in a banking scenario we describe a variety of available
contextual information and their interrelations. Different from other access
control systems our main focus is on law-compliant cross-border data access. By
including legislation directly into access decisions, this lawfulness can be
ensured. We also describe our information model to demonstrate how these
policies can be implemented into an existing network and how the components and
contextual information interrelate. Finally, we outline an event flow for a
request made from a remote user exemplifying how such a system decides about
access.","Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN
  2151-9617"
company law package,http://arxiv.org/abs/1806.06718v1,"On the decentralized navigation of multiple packages on transportation
  networks","We investigate by numerical simulation and finite-size analysis the impact of
long-range shortcuts on a spatially embedded transportation network. Our
networks are built from two-dimensional ($d=2$) square lattices to be improved
by the addition of long-range shortcuts added with probability $P(r_{ij})\sim
r_{ij}^{-\alpha}$ [J. M. Kleinberg, Nature 406, 845 (2000)]. Considering those
improved networks, we performed numerical simulation of multiple discrete
package navigation and found a limit for the amount of packages flowing through
the network. Such limit is characterized by a critical probability of creating
packages $p_{c}$, where above this value a transition to a congested state
occurs. Moreover, $p_{c}$ is found to follow a power-law, $p_{c}\sim
L^{-\gamma}$, where $L$ is the network size. Our results indicate the presence
of an optimal value of $\alpha_{\rm min}\approx1.7$, where the parameter
$\gamma$ reaches its minimum value and the networks are more resilient to
congestion for larger system sizes.","Phys. Rev. E 98, 032306 (2018)"
company law package,http://arxiv.org/abs/1309.2086v1,"High-level robot programming based on CAD: dealing with unpredictable
  environments","Purpose - The purpose of this paper is to present a CAD-based human-robot
interface that allows non-expert users to teach a robot in a manner similar to
that used by human beings to teach each other.
  Design/methodology/approach - Intuitive robot programming is achieved by
using CAD drawings to generate robot programs off-line. Sensory feedback allows
minimization of the effects of uncertainty, providing information to adjust the
robot paths during robot operation.
  Findings - It was found that it is possible to generate a robot program from
a common CAD drawing and run it without any major concerns about calibration or
CAD model accuracy.
  Research limitations/implications - A limitation of the proposed system has
to do with the fact that it was designed to be used for particular
technological applications.
  Practical implications - Since most manufacturing companies have CAD packages
in their facilities today, CAD-based robot programming may be a good option to
program robots without the need for skilled robot programmers.
  Originality/value - The paper proposes a new CAD-based robot programming
system. Robot programs are directly generated from a CAD drawing running on a
commonly available 3D CAD package (Autodesk Inventor) and not from a
commercial, computer aided robotics (CAR) software, making it a simple CAD
integrated solution. This is a low-cost and low-setup time system where no
advanced robot programming skills are required to operate it. In summary, robot
programs are generated with a high-level of abstraction from the robot
language.","Industrial Robot: An International Journal, Vol. 39 Iss: 3, 2012,
  pp.294 - 303"
company law package,http://arxiv.org/abs/1606.00888v1,The Power of Fair Information Practices - A Control Agency Approach,"Most companies' new business practices are based on customer data. These
practices have raised privacy concerns because of the associated risks. Privacy
laws require companies to gain customer consent before using their information,
which stands as the biggest roadblock to monetise this asset. Privacy
literature suggests that reducing privacy concerns and building trust may
increase individuals' intention to authorise the use of personal information.
Fair information practices (FIPs) are potential means to achieve this goal.
However, there is lack of empirical evidence on the mechanisms through which
the FIPs affect privacy concerns and trust. This research argues that FIPs load
individuals with control, which has been found to influence privacy concerns
and trust level. We will use an experimental design methodology to conduct the
study. The results are expected to have both theoretical and managerial
implications.",
company law package,http://arxiv.org/abs/1811.12621v1,A Core Ontology for Privacy Requirements Engineering,"Nowadays, most companies need to collect, store, and manage personal
information in order to deliver their services. Accordingly, privacy has
emerged as a key concern for these companies since they need to comply with
privacy laws and regulations. To deal with them properly, such privacy concerns
should be considered since the early phases of system design. Ontologies have
proven to be a key factor for elaborating high-quality requirements models.
However, most existing work deals with privacy as a special case of security
requirements, thereby missing essential traits of this family of requirements.
In this paper, we introduce COPri, a Core Ontology for Privacy requirements
engineering that adopts and extends our previous work on privacy requirements
engineering ontology that has been mined through a systematic literature
review. Additionally, we implement, validate and then evaluate our ontology.",
company law package,http://arxiv.org/abs/1910.01303v2,"From Senseless Swarms to Smart Mobs: Tuning Networks for Prosocial
  Behaviour","Social media have been seen to accelerate the spread of negative content such
as disinformation and hate speech, often unleashing reckless herd mentality
within networks, further aggravated by malicious entities using bots for
amplification. So far, the response to this emerging global crisis has centred
around social media platform companies making reactive moves that appear to
have greater symbolic value than practical utility. These include taking down
patently objectionable content or manually deactivating the accounts of bad
actors, while leaving vast troves of negative content to circulate and
perpetuate within social networks. Governments worldwide have thus sought to
intervene using regulatory tools, with countries such as France, Germany and
Singapore introducing laws to compel technology companies to take down or
correct erroneous and harmful content. However, the relentless pace of
technological progress enfeebles regulatory measures that seem fated for
obsolescence.",
company law package,http://arxiv.org/abs/physics/0510058v3,"Scaling theory of temporal correlations and size dependent fluctuations
  in the traded value of stocks","Records of the traded value f_i(t) of stocks display fluctuation scaling, a
proportionality between the standard deviation sigma(i) and the average <f(i)>:
sigma(i) ~ f(i)^alpha, with a strong time scale dependence alpha(dt). The
non-trivial (i.e., neither 0.5 nor 1) value of alpha may have different origins
and provides information about the microscopic dynamics. We present a set of
recently discovered stylized facts, and then show their connection to such
behavior. The functional form alpha(dt) originates from two aspects of the
dynamics: Stocks of larger companies both tend to be traded in larger packages,
and also display stronger correlations of traded value.","Phys. Rev. E 73, 046109 (2006)"
company law package,http://arxiv.org/abs/1312.5481v1,"Students' Perceptions and Attitude towards the effectiveness of Prezi
  Uses in learning Islamic Subject","Prezi is a Hungarian software company, producing a cloud-based presentation
software and storytelling tool for presenting ideas on a virtual canvas
(Prezi.com). Prezi is one of the teaching materials that help students in
learning process. This study aims to explore the effectiveness of using Prezi
in Islamic education subject among secondary schools under the topic of
marriage in Islam: polygamy. Specifically its aims to identify students
interest and second examine their attitude towards the uses of Prezi in
learning Islamic educations. A total of 22 students participated in the survey,
employing a 22-item questionnaire. The data was analyzed quantitatively using
Statistical Package for the Social Sciences (SPSS). Result from this study
revealed that student shows their interest in learning Islamic Educations when
teachers uses Prezi. In additions students also show positive attitude towards
uses of Prezi in Classroom. As conclusions, the uses of Prezi presentation is
easy and its technique for developing a more creative and innovative approach
in teaching strategies among Islamic educators.",International Journal of Computer Science (IJASCSE)2013
company law package,http://arxiv.org/abs/1811.01688v1,"Collaboration and integration through information technologies in supply
  chains","Supply chain management encompasses various processes including various
conventional logistics activities, and various other processes These processes
are supported -- to a certain limit -- by coordination and integration
mechanisms which are long-term strategies that give competitive advantage
through overall supply chain efficiency. Information Technology, by the way of
collecting, sharing and gathering data, exchanging information, optimising
process through package software, is becoming one of the key developments and
success of these collaboration strategies. This paper proposes a study to
identify the methods used for collaborative works in the supply chain and
focuses on some of its areas, as between a company and its suppliers (i.e.,
inventory sharing) and its customers (i.e., customer demand, forecasting), and
also the integration of product information in the value chain.","IJTM, 2004, 28 (2), pp.259-273"
company law package,http://arxiv.org/abs/1901.01810v1,An ERP Implementation Method : Studying a Pharmaceutical Company,"Analysing the development process for an ERP solution, in our case SAP, is
one of the most critical processes in implementing standard software packages.
Modelling of the proposed system can facilitate the development of enterprise
systems not from scratch but through use of predefined parts who represents the
best knowledge captured from numerous case studies. This aim at abstracting the
specification of the required information system as well as modelling the
process towards this goal. Modelling plays a central role in the organisation
of the information systems development process and the information systems
community has developed a large number of conceptual models, systems of
concepts, for representing conceptual schemata. In the area of ERP systems,
because of the characteristics that distinguishes them, conceptual modelling
can help in all aspects of the development process, from goal elicitation to
reuse of the captured knowledge, through the use of the appropriate modelling
schemata. SAP offers a standardised software solution, thus making easier the
alignment of SAP requirements to enterprise requirements in a goal form, and
the correspondent business processes.",
company law package,http://arxiv.org/abs/0808.0668v1,"Dynamics of non-harmonic internal gravity wave packets in stratified
  media","In the paper taking the assumption of the slowness of the change of the
parameters of the vertically stratified medium in the horizontal direction and
in time, the evolution of the non-harmonic wave packages of the internal
gravity waves has been analyzed. The concrete form of the wave packages can be
expressed through some model functions and is defined by the local behavior of
the dispersive curves of the separate modes near to the corresponding special
points. The solution of this problem is possible with the help of the modified
variant of the special-time ray method offered by the authors (the method of
geometrical optics), the basic difference of which consists that the asymptotic
representation of the solution may be found in the form the series of the
non-integer degrees of some small parameter. At that the exponent depends on
the concrete form of representation of this package. The obvious kind of the
representation is determined from the principle of the localness and the
asymptotic behavior of the solution in the stationary and the
horizontally-homogeneous case. The phases of the wave packages are determined
from the corresponding equations of the eikonal, which can be solved
numerically on the characteristics (rays). Amplitudes of the wave packages are
determined from the laws of conservation of the some invariants along the
characteristics (rays).",
company law package,http://arxiv.org/abs/1211.1242v1,Information and Communication Technology in Combating Counterfeit Drugs,"Pharma frauds are on the rise, counterfeit drugs are giving sleepless nights
to patients, pharmaceutical companies and governments. The laws prohibiting the
sales of counterfeit drugs cannot succeed without technological interventions.
Several analytical techniques and tools including spectroscopy, holograms,
barcoding, differentiated packing, radio frequency identification,
fingerprints, hyperspectral imaging etc. have been employed over the years in
combating this menace; however this challenge is becoming more sophisticated
with the evolution of the World Wide Web and online pharmacies. This paper
presents a review on the contribution of Information and Communication
Technology (ICT) as a drug counterfeit countermeasure.","International Journal of Engineering and Technology Volume 2 No.
  9, September, 2012"
company law package,http://arxiv.org/abs/physics/0508203v1,Impact of Stock Market Structure on Intertrade Time and Price Dynamics,"The NYSE and NASDAQ stock markets have very different structures and there is
continuing controversy over whether differences in stock price behaviour are
due to market structure or company characteristics. As the influence of market
structure on stock prices may be obscured by exogenous factors such as demand
and supply, we hypothesize that modulation of the flow of transactions due to
market operations may carry a stronger imprint of the internal market
mechanism. We analyse times between consecutive transactions (ITT) for NYSE and
NASDAQ stocks, and we relate the dynamical properties of the ITT with those of
the corresponding price fluctuations. We find a robust scale-invariant temporal
organisation in the ITT of stocks which is independent of individual company
characteristics and industry sector, but which depends on market structure. We
find that stocks registered on the NASDAQ exhibit stronger correlations in
their transaction timing within a trading day, compared with NYSE stocks.
Further, we find that companies that transfer from the NASDAQ to the NYSE show
a reduction in the correlation strength of transaction timing within a trading
day, after the move, suggesting influences of market structure. Surprisingly,
we also observe that stronger power-law correlations in the ITT are coupled
with stronger power-law correlations in absolute price returns and higher price
volatility, suggesting a strong link between the dynamical properties of ITT
and the corresponding price fluctuations over a broad range of time scales.
Comparing the NYSE and NASDAQ, we demonstrate that the higher correlations we
find in ITT for NASDAQ stocks are matched by higher correlations in absolute
price returns and by higher volatility, suggesting that market structure may
affect price behaviour through information contained in transaction timing.",
company law package,http://arxiv.org/abs/1010.3815v2,Individual and Group Dynamics in Purchasing Activity,"As a major part of the daily operation in an enterprise, purchasing frequency
is of constant change. Recent approaches on the human dynamics can provide some
new insights into the economic behaviors of companies in the supply chain. This
paper captures the attributes of creation times of purchase orders to an
individual vendor, as well as to all vendors, and further investigates whether
they have some kind of dynamics by applying logarithmic binning to the
construction of distribution plot. It's found that the former displays a
power-law distribution with approximate exponent 2.0, while the latter is
fitted by a mixture distribution with both power-law and exponential
characteristics. Obviously, two distinctive characteristics are presented for
the interval time distribution from the perspective of individual dynamics and
group dynamics. Actually, this mixing feature can be attributed to the fitting
deviations as they are negligible for individual dynamics, but those of
different vendors are cumulated and then lead to an exponential factor for
group dynamics. To better describe the mechanism generating the heterogeneity
of purchase order assignment process from the objective company to all its
vendors, a model driven by product life cycle is introduced, and then the
analytical distribution and the simulation result are obtained, which are in
good line with the empirical data.",Physica A 392 (2013) 343-349
company law package,http://arxiv.org/abs/1005.1899v1,The ABC of Digital Business Ecosystems,"The European Commission has the power to inspire, initiate and sponsor huge
transnational projects to an extent impossible for most other entities. These
projects can address universal themes and develop well-being models that are
valuable across a diversity of societies and economies. It is a universal fact
that SMEs in all countries provide a substantial proportion of total
employment, and conduct much of a nation's innovative activity. Yet these
smaller companies struggle in global markets on a far from level playing field,
where large companies have distinct advantages. To redress this imbalance the
Commission saw it as a priority to improve the trading capability of the Small
and Medium-sized Enterprises (SMEs), and perceived digital platforms as the
modern means to this end. They considered that the best operational model for a
vibrant Web2.0-based Internet services industry would be by analogy to
well-performing biological ecosystems. Open Source Software is adopted in the
DBE/OPAALS projects as the best support for sustainability of such complex
electronic webs, since it minimises interoperability problems, enables code
access for cheaper in-house modification or development of systems, and reduces
both capital and operating expenditure.","The ABC of Digital Business Ecosystems. J Stanley and G Briscoe.
  Communications Law - Journal of Computer, Media and Telecommunications Law,
  15(1), 2010."
company law package,http://arxiv.org/abs/1903.09518v1,Trial of an AI: Empowering people to explore law and science challenges,"Artificial Intelligence represents many things: a new market to conquer or a
quality label for tech companies, a threat for traditional industries, a menace
for democracy, or a blessing for our busy everyday life. The press abounds in
examples illustrating these aspects, but one should draw not hasty and
premature conclusions. The first successes in AI have been a surprise for
society at large-including researchers in the field. Today, after the initial
stupefaction, we have examples of the system reactions: traditional companies
are heavily investing in AI, social platforms are monitored during elections,
data collection is more and more regulated, etc. The resilience of an
organization (i.e. its capacity to resist to a shock) relies deeply on the
perception of its environment. Future problems have to be anticipated, while
unforeseen events occurring have to be quickly identified in order to be
mitigated as fast as possible. The author states that this clear perception
starts with a common definition of AI in terms of capacities and limits. AI
practitioners should make notions and concepts accessible to the general public
and the impacted fields (e.g. industries, law, education). It is a truism that
only law experts would have the potential to estimate IA impacts on judicial
system. However, questions remain on how to connect different kind of expertise
and what is the appropriate level of detail required for the knowledge
exchanges. And the same consideration is true for dissemination towards
society. Ultimately, society will live with decisions made by the ""experts"". It
sounds wise to involve society in the decision process rather than risking to
pay consequences later. Therefore, society also needs the key concepts to
understand AI impact on their life. This was the purpose of the trial of an IA
that took place in October 2018 at the Court of Appeal of Paris: gathering
experts from various fields to expose challenges in law and science towards a
general public.","IFIM's International Journal on Law & Regulation of Artificial
  Intelligence & Robotics, 2019, 1 (1)"
company law package,http://arxiv.org/abs/1204.6549v1,Power Law Distributions of Patents as Indicators of Innovation,"The total number of patents produced by a country (or the number of patents
produced per capita) is often used as an indicator for innovation. Here we
present evidence that the distribution of patents amongst applicants within
many OECD countries is well-described by power laws with exponents that vary
between 1.66 (Japan) and 2.37 (Poland). Using simulations based on simple
preferential attachment-type rules that generate power laws, we find we can
explain some of the variation in exponents between countries, with countries
that have larger numbers of patents per applicant generally exhibiting smaller
exponents in both the simulated and actual data. Similarly we find that the
exponents for most countries are inversely correlated with other indicators of
innovation, such as R&D intensity or the ubiquity of export baskets. This
suggests that in more advanced economies, which tend to have smaller values of
the exponent, a greater proportion of the total number of patents are filed by
large companies than in less advanced countries.",
company law package,http://arxiv.org/abs/1609.07818v1,"Programming the Universe: The First Commandment of Software Engineering
  for all Varieties of Information Systems","Since the early days of computers and programs, the process and outcomes of
software development has been a minefield plagued with problems and failures,
as much as the complexity and complication of software and its development has
increased by a thousandfold in half a century. Over the years, a number of
theories, laws, best practices, manifestos and methodologies have emerged, with
varied degrees of (un)success. Our experience as software engineers of complex
and large-scale systems shows that those guidelines are bound to previously
defined and often narrow scopes. Enough is enough. Nowadays, nearly every
company is in the software and services business and everything is - or is
managed by - software. It is about time, then, that the laws that govern our
universe ought to be redefined. In this context, we discuss and present a set
of universal laws that leads us to propose the first commandment of software
engineering for all varieties of information systems.","Proceedings of the 30th Brazilian Symposium on Software
  Engineering (SBES '16), Eduardo Santana de Almeida (Ed.). ACM, New York, NY,
  USA, 153-156, (2016)"
company law package,http://arxiv.org/abs/1802.06285v1,Greening Geographical Power Allocation for Cellular Networks,"Harvesting energy from nature (solar, wind etc.) is envisioned as a key
enabler for realizing green wireless networks. However, green energy sources
are geographically distributed and the power amount is random which may not
enough to power a base station by a single energy site. Burning brown energy
sources such as coal and crude oil, though companied with carbon dioxide
emission, provides stable power. In this paper, without sacrificing
communication quality, we investigate how to perform green energy allocation to
abate the dependence on brown energy with hybrid brown and green energy
injected in power networks. We present a comprehensive framework to
characterize the performance of hybrid green and brown energy empowered
cellular network. Novel performance metric ""bits/ton\ce{CO2}/Hz"" is proposed to
evaluate the greenness of the communication network. As green energy is usually
generated from distributed geographical locations and is time varying, online
geographical power allocation algorithm is proposed to maximize the greenness
of communication network considering electricity transmission's physical laws
i.e., Ohm's law and Kirchhoff's circuit laws. Simulations show that
geographically distributed green energy sources complement each other by
improving the communication capacity while saving brown energy consumption.
Besides, the penetration of green energy can also help reduce power loss on the
transmission breaches.",
company law package,http://arxiv.org/abs/cs/0507055v2,ReacProc: A Tool to Process Reactions Describing Particle Interactions,"ReacProc is a program written in C/C++ programming language which can be used
(1) to check out of reactions describing particles interactions against
conservation laws and (2) to reduce input reaction into some canonical form. A
table with particles properties is available within ReacProc package.",
company law package,http://arxiv.org/abs/1803.04761v1,"Asymmetric Influence of Employees and Trading Partners on Company's
  Sales and its Dynamical Origin","Growth of business firms or companies has been a subject of intensive
research over a century. However, there still remains controversy about the
basic mechanisms of their growth. Inspired by previous work on scaling laws in
other systems, here we extend the notion of size of firms from a scalar to a
vector in order to characterize in more detail the mechanisms of growth and
decay of firms. Based on a large scale dataset of Japanese firms covering over
two million firms for two decades (1994-2015), we compile the dataset of
vectors of three components, namely, annual sales, number of employee and
number of trading partners. We find that the number of employees is more
influential in determining firm sales compared to the number of trading
partners. This asymmetry is validated by regressions of sales against these
parameters and the analysis of growth rate correlations. We then explore
multi-variate dynamics of firms by elaborating an evolutionary flow diagram of
the averaged motion in the three-dimensional vector space. The flow diagram
indicates that firms which deviate from the balanced scaling relation tend to
return to this relation. We also find that firms with a chance of large sales
growth suffer the risk of high disappearance rate. These results could serve
for prediction and modeling of firms, and are relevant for theoretical
understanding of the general principles governing complex systems.",
company law package,http://arxiv.org/abs/1809.05762v1,"Using Artificial Intelligence to Support Compliance with the General
  Data Protection Regulation","The General Data Protection Regulation (GDPR) is a European Union regulation
that will replace the existing Data Protection Directive on 25 May 2018. The
most significant change is a huge increase in the maximum fine that can be
levied for breaches of the regulation. Yet fewer than half of UK companies are
fully aware of GDPR - and a number of those who were preparing for it stopped
doing so when the Brexit vote was announced. A last-minute rush to become
compliant is therefore expected, and numerous companies are starting to offer
advice, checklists and consultancy on how to comply with GDPR. In such an
environment, artificial intelligence technologies ought to be able to assist by
providing best advice; asking all and only the relevant questions; monitoring
activities; and carrying out assessments. The paper considers four areas of
GDPR compliance where rule based technologies and/or machine learning
techniques may be relevant: * Following compliance checklists and codes of
conduct; * Supporting risk assessments; * Complying with the new regulations
regarding technologies that perform automatic profiling; * Complying with the
new regulations concerning recognising and reporting breaches of security. It
concludes that AI technology can support each of these four areas. The
requirements that GDPR (or organisations that need to comply with GDPR) state
for explanation and justification of reasoning imply that rule-based approaches
are likely to be more helpful than machine learning approaches. However, there
may be good business reasons to take a different approach in some
circumstances.","Artificial Intelligence and Law (2017) 25, 429 - 443"
company law package,http://arxiv.org/abs/1712.02987v1,Collaborative Company Profiling: Insights from an Employee's Perspective,"Company profiling is an analytical process to build an indepth understanding
of company's fundamental characteristics. It serves as an effective way to gain
vital information of the target company and acquire business intelligence.
Traditional approaches for company profiling rely heavily on the availability
of rich finance information about the company, such as finance reports and SEC
filings, which may not be readily available for many private companies.
However, the rapid prevalence of online employment services enables a new
paradigm - to obtain the variety of company's information from their employees'
online ratings and comments. This, in turn, raises the challenge to develop
company profiles from an employee's perspective. To this end, in this paper, we
propose a method named Company Profiling based Collaborative Topic Regression
(CPCTR), for learning the latent structural patterns of companies. By
formulating a joint optimization framework, CPCTR has the ability in
collaboratively modeling both textual (e.g., reviews) and numerical information
(e.g., salaries and ratings). Indeed, with the identified patterns, including
the positive/negative opinions and the latent variable that influences salary,
we can effectively carry out opinion analysis and salary prediction. Extensive
experiments were conducted on a real-world data set to validate the
effectiveness of CPCTR. The results show that our method provides a
comprehensive understanding of company characteristics and delivers a more
effective prediction of salaries than other baselines.",
company law package,http://arxiv.org/abs/cs/0002018v2,Efficient generation of rotating workforce schedules,"Generating high-quality schedules for a rotating workforce is a critical task
in all settings where a certain staffing level must be guaranteed beyond the
capacity of single employees, such as for instance in industrial plants,
hospitals, or airline companies. Results from ergonomics \cite{BEST91} indicate
that rotating workforce schedules have a profound impact on the health and
social life of employees as well as on their performance at work. Moreover,
rotating workforce schedules must satisfy legal requirements and should also
meet the objectives of the employing organization. We describe our solution to
this problem. A basic design decision was to aim at quickly obtaining
high-quality schedules for realistically sized problems while maintaining human
control. The interaction between the decision maker and the algorithm therefore
consists in four steps: (1) choosing a set of lengths of work blocks (a work
block is a sequence of consecutive days of work shifts), (2) choosing a
particular sequence of work and days-off blocks among those that have optimal
weekend characteristics, (3) enumerating possible shift sequences for the
chosen work blocks subject to shift change constraints and bounds on sequences
of shifts, and (4) assignment of shift sequences to work blocks while
fulfilling the staffing requirements. The combination of constraint
satisfaction and problem-oriented intelligent backtracking algorithms in each
of the four steps allows to find good solutions for real-world problems in
acceptable time. Computational results from real-world problems and from
benchmark examples found in the literature confirm the viability of our
approach. The algorithms are now part of a commercial shift scheduling software
package.",
company law package,http://arxiv.org/abs/1503.02064v1,"A Unified Platform Enabling Power System Circuit Model Data Transfer
  Among Different Software","Diversity of software packages to simulate the power system circuits is
considerable. It is challenging to transfer power system circuit model data
(PSCMD) among different software tools and rebuild the same circuit in the
second software environment. This paper proposes a unified platform (UP) where
PSCMD are stored in a spreadsheet file with a defined format. Script-based
PSCMD transfer applications, written in MATLAB, have been developed for a set
of software to read the circuit model data from the UP spreadsheet and
reconstruct the circuit in the destination software. This significantly eases
the process of transferring circuit model data between each pair of software
tools. In this paper ETAP, OpenDSS, Grid LabD, and DEW are considered. In order
to test the developed PSCMD transfer applications, circuit model data of a test
circuit and an actual sample circuit from a Californian utility company, both
built in CYME, were exported into the spreadsheet file according to the UP
format. Thereafter, circuit model data were imported successfully from the
spreadsheet files into all above mentioned software using the PSCMD transfer
applications developed for each software individually. Finally, load flow
analysis is performed in all software and the obtained results match with each
other.",
company law package,http://arxiv.org/abs/1509.02256v3,Matrix Computations and Optimization in Apache Spark,"We describe matrix computations available in the cluster programming
framework, Apache Spark. Out of the box, Spark provides abstractions and
implementations for distributed matrices and optimization routines using these
matrices. When translating single-node algorithms to run on a distributed
cluster, we observe that often a simple idea is enough: separating matrix
operations from vector operations and shipping the matrix operations to be ran
on the cluster, while keeping vector operations local to the driver. In the
case of the Singular Value Decomposition, by taking this idea to an extreme, we
are able to exploit the computational power of a cluster, while running code
written decades ago for a single core. Another example is our Spark port of the
popular TFOCS optimization package, originally built for MATLAB, which allows
for solving Linear programs as well as a variety of other convex programs. We
conclude with a comprehensive set of benchmarks for hardware accelerated matrix
computations from the JVM, which is interesting in its own right, as many
cluster programming frameworks use the JVM. The contributions described in this
paper are already merged into Apache Spark and available on Spark installations
by default, and commercially supported by a slew of companies which provide
further services.",
company law package,http://arxiv.org/abs/1608.06759v1,"Systematic Evaluation of Sandboxed Software Deployment for Real-time
  Software on the Example of a Self-Driving Heavy Vehicle","Companies developing and maintaining software-only products like web shops
aim for establishing persistent links to their software running in the field.
Monitoring data from real usage scenarios allows for a number of improvements
in the software life-cycle, such as quick identification and solution of
issues, and elicitation of requirements from previously unexpected usage. While
the processes of continuous integration, continuous deployment, and continuous
experimentation using sandboxing technologies are becoming well established in
said software-only products, adopting similar practices for the automotive
domain is more complex mainly due to real-time and safety constraints. In this
paper, we systematically evaluate sandboxed software deployment in the context
of a self-driving heavy vehicle that participated in the 2016 Grand Cooperative
Driving Challenge (GCDC) in The Netherlands. We measured the system's
scheduling precision after deploying applications in four different execution
environments. Our results indicate that there is no significant difference in
performance and overhead when sandboxed environments are used compared to
natively deployed software. Thus, recent trends in software architecting,
packaging, and maintenance using microservices encapsulated in sandboxes will
help to realize similar software and system engineering for cyber-physical
systems.",
company law package,http://arxiv.org/abs/1706.09172v1,Sketches and Diagrams in Practice,"Sketches and diagrams play an important role in the daily work of software
developers. In this paper, we investigate the use of sketches and diagrams in
software engineering practice. To this end, we used both quantitative and
qualitative methods. We present the results of an exploratory study in three
companies and an online survey with 394 participants. Our participants included
software developers, software architects, project managers, consultants, as
well as researchers. They worked in different countries and on projects from a
wide range of application areas. Most questions in the survey were related to
the last sketch or diagram that the participants had created. Contrary to our
expectations and previous work, the majority of sketches and diagrams contained
at least some UML elements. However, most of them were informal. The most
common purposes for creating sketches and diagrams were designing, explaining,
and understanding, but analyzing requirements was also named often. More than
half of the sketches and diagrams were created on analog media like paper or
whiteboards and have been revised after creation. Most of them were used for
more than a week and were archived. We found that the majority of participants
related their sketches to methods, classes, or packages, but not to source code
artifacts with a lower level of abstraction.",
company law package,http://arxiv.org/abs/1808.06900v2,"Defending against Intrusion of Malicious UAVs with Networked UAV Defense
  Swarms","Nowadays, companies such as Amazon, Alibaba, and even pizza chains are
pushing forward to use drones, also called UAVs (Unmanned Aerial Vehicles), for
service provision, such as package and food delivery. As governments intend to
use these immense economic benefits that UAVs have to offer, urban planners are
moving forward to incorporate so-called UAV flight zones and UAV highways in
their smart city designs. However, the high-speed mobility and behavior
dynamics of UAVs need to be monitored to detect and, subsequently, to deal with
intruders, rogue drones, and UAVs with a malicious intent. This paper proposes
a UAV defense system for the purpose of intercepting and escorting a malicious
UAV outside the flight zone. The proposed UAV defense system consists of a
defense UAV swarm, which is capable to self-organize its defense formation in
the event of intruder detection, and chase the malicious UAV as a networked
swarm. Modular design principles have been used for our fully localized
approach. We developed an innovative auto-balanced clustering process to
realize the intercept- and capture-formation. As it turned out, the resulting
networked defense UAV swarm is resilient against communication losses. Finally,
a prototype UAV simulator has been implemented. Through extensive simulations,
we show the feasibility and performance of our approach.",
company law package,http://arxiv.org/abs/1902.09155v2,CityJSON: A compact and easy-to-use encoding of the CityGML data model,"The international standard CityGML is both a data model and an exchange
format to store digital 3D models of cities. While the data model is used by
several cities, companies, and governments, in this paper we argue that its
XML-based exchange format has several drawbacks. These drawbacks mean that it
is difficult for developers to implement parsers for CityGML, and that
practitioners have, as a consequence, to convert their data to other formats if
they want to exchange them with others. We present CityJSON, a new JSON-based
exchange format for the CityGML data model (version 2.0.0). CityJSON was
designed with programmers in mind, so that software and APIs supporting it can
be quickly built. It was also designed to be compact (a compression factor of
around six with real-world datasets), and to be friendly for web and mobile
development. We argue that it is considerably easier to use than the CityGML
format, both for reading and for creating datasets. We discuss in this paper
the main features of CityJSON, briefly present the different software packages
to parse/view/edit/create files (including one to automatically convert between
the JSON and GML encodings), analyse how real-world datasets compare to those
of CityGML, and we also introduce \emph{Extensions}, which allow us to extend
the core data model in a documented manner.",
company law package,http://arxiv.org/abs/1303.5061v1,"Which research in design creativity and innovation? Let us not forget
  the reality of companies","Studying design creativity and innovation from practical perspectives for
companies requires both a good understanding of the company ecosystem and its
inner processes contributing to delivered innovations and a rigorous design
research methodology to provide effective design models, methods, platforms
that are truly effective in the context of company. Working in an Industrial
Engineering laboratory, we advocate a more systemic vision of design creativity
and innovation in company ecosystems. We present in this paper an attempt to
develop and make professional an innovation engineering. Our research works are
illustrated along the different research topics of an innovation process. We
start by a recent survey on innovation practice and organizational models led
in 28 large companies. The lessons learned about this survey reinforce our
belief that there is a need for a new method in agile management of radical
innovation projects in company contexts. We currently develop, test and apply
such a methodology named: Radical Innovation Design(RID). Its effectiveness has
been evaluated through a large scale evaluation of the project outcomes for the
company. Two extensions of RID have been proposed and deployed in company
contexts: a selection procedure for innovation clusters and a value-driven
process for airplane development projects.","International Journal of Design Creativity and Innovation 2 (2013)
  1-21"
SE Directive,http://arxiv.org/abs/1709.02759v1,Semantic Preserving Embeddings for Generalized Graphs,"A new approach to the study of Generalized Graphs as semantic data structures
using machine learning techniques is presented. We show how vector
representations maintaining semantic characteristics of the original data can
be obtained from a given graph using neural encoding architectures and
considering the topological properties of the graph. Semantic features of these
new representations are tested by using some machine learning tasks and new
directions on efficient link discovery, entitity retrieval and long distance
query methodologies on large relational datasets are investigated using real
datasets.
  ----
  En este trabajo se presenta un nuevo enfoque en el contexto del aprendizaje
autom\'atico multi-relacional para el estudio de Grafos Generalizados. Se
muestra c\'omo se pueden obtener representaciones vectoriales que mantienen
caracter\'isticas sem\'anticas del grafo original utilizando codificadores
neuronales y considerando las propiedades topol\'ogicas del grafo. Adem\'as, se
eval\'uan las caracter\'isticas sem\'anticas capturadas por estas nuevas
representaciones y se investigan nuevas metodolog\'ias eficientes relacionadas
con Link Discovery, Entity Retrieval y consultas a larga distancia en grandes
conjuntos de datos relacionales haciendo uso de bases de datos reales.",
SE Directive,http://arxiv.org/abs/1708.05548v1,Moving object tracking employing rigid body motion on matrix Lie groups,"In this paper we propose a novel method for estimating rigid body motion by
modeling the object state directly in the space of the rigid body motion group
SE(2). It has been recently observed that a noisy manoeuvring object in SE(2)
exhibits banana-shaped probability density contours in its pose. For this
reason, we propose and investigate two state space models for moving object
tracking: (i) a direct product SE(2)xR3 and (ii) a direct product of the two
rigid body motion groups SE(2)xSE(2). The first term within these two state
space constructions describes the current pose of the rigid body, while the
second one employs its second order dynamics, i.e., the velocities. By this, we
gain the flexibility of tracking omnidirectional motion in the vein of a
constant velocity model, but also accounting for the dynamics in the rotation
component. Since the SE(2) group is a matrix Lie group, we solve this problem
by using the extended Kalman filter on matrix Lie groups and provide a detailed
derivation of the proposed filters. We analyze the performance of the filters
on a large number of synthetic trajectories and compare them with (i) the
extended Kalman filter based constant velocity and turn rate model and (ii) the
linear Kalman filter based constant velocity model. The results show that the
proposed filters outperform the other two filters on a wide spectrum of types
of motion.",
SE Directive,http://arxiv.org/abs/1812.01395v2,"Cut to the chase: Revisiting the relevance of software engineering
  research","Software engineering (SE) research should be relevant to industrial practice.
There has been a debate on this issue in the community since 1980's by pioneers
such as Robert Glass and Colin Potts. As we pass the milestone of ""50 Years of
Software Engineering"", some recent positive efforts have been made in this
direction, e.g., establishing ""industrial"" tracks in several SE conferences.
However, we, as a community, are still struggling with research relevance and
utility. The goal of this paper is to act as another ""wake-up call"" for the
community to reflect and act on the relevance of SE research. The contributions
of this paper are as follows: (1) a review of the debate on research relevance
in other fields; (2) a Multi-vocal Literature Review (MLR) of the debate in SE
(46 sources) and the suggestions discussed in the community for improving the
situation; (3) a summary of the experience of the authors in conducting SE
research with varying degrees of relevance; and (4) a review of recent
activities being done in the SE community to improve relevance. There has been
no systematic literature review on the topic of research relevance in SE yet.
Some of our MLR findings are that: the top-3 root causes of low relevance, as
discussed in the community are: (1) Simplistic view (wrong assumptions) about
SE in practice; (2) Wrong identification of research problems (needs); and (3)
Issues with research mindset. The top-3 suggestions for improving research
relevance are: (1) Using appropriate research approaches such as
action-research, that would increase chances of research relevance; (2)
Choosing relevant (practical) problems; and (3) Collaborating with industry. By
synthesizing all the discussions on this debate so far, this paper hopes to
encourage further discussions and actions in the community to increase our
collective efforts to improve the research relevance in our discipline.",
SE Directive,http://arxiv.org/abs/1704.02829v1,"Using highly uniform and smooth Selenium colloids as low-loss
  magnetodielectric building blocks of optical metafluids","We systematically analyzed magnetodielectric resonances of Se colloids for
the first time to exploit the possibility for use as building blocks of
all-dielectric optical metafluids. By taking synergistic advantages of Se
colloids, including (i) high-refractive-index at optical frequencies, (ii)
unprecedented structural uniformity, and (iii) versatile access to copious
quantities, the Kerker-type directional light scattering resulting from
efficient coupling between strong electric and magnetic resonances were
observed directly from Se colloidal suspension. Thus, the use of Se colloid as
a generic magnetodielectric building block highlights an opportunity for the
fluidic low-loss optical antenna, which can be processed via spin-coating and
painting.",
SE Directive,http://arxiv.org/abs/1712.00307v2,"Energy- and Spectral- Efficiency Tradeoff for D2D-Multicasts in Underlay
  Cellular Networks","Underlay in-band device-to-device (D2D) multicast communication, where the
same content is disseminated via direct links in a group, has the potential to
improve the spectral and energy efficiencies of cellular networks. However,
most of the existing approaches for this problem only address either spectral
efficiency (SE) or energy efficiency (EE). We study the tradeoff between SE and
EE in a single cell D2D integrated cellular network, where multiple D2D
multicast groups (MGs) may share the uplink channel with multiple cellular
users (CUs). We formulate the EE maximization problem with constraint on SE and
maximum available transmission power. A power allocation algorithm is proposed
to solve this problem and its efficacy is demonstrated via extensive numerical
simulations. The tradeoff between SE and EE as a function of density of D2D
MGs, and maximum transmission power of a MG is characterized.",
SE Directive,http://arxiv.org/abs/1812.08687v1,"Investigation of the quality of an As35S65 grating by spectroscopic
  ellipsometry","The quality of an As35S65 chalcogenide glass (ChG) grating fabricated by
electron beam lithography (EBL) was characterized by optical scatterometry
based on spectroscopic ellipsometry (SE) in the visible and near infrared
spectral range and complementary techniques providing direct images, especially
atomic force microscopy (AFM). The geometric dimensions and the shape of
patterned grating lines were determined by fitting modeled values (calculated
by the Fourier modal method) to SE experimental data. A simple power-dependent
function with only one variable parameter was successfully used to describe the
shape of the patterned lines. The result yielded by SE is shown to correspond
to AFM measurement with high accuracy, provided that optical constants of ChG
modified by EBL were used in the fitting procedure. The line edge roughness
(LER) of the grating was also investigated by further fitting the SE data to
find out that no LER is optically detectable in the spectral range used, which
is essential for the functionality of optical tools fabricated by EBL.",
SE Directive,http://arxiv.org/abs/1907.02441v1,"Semantic-Effectiveness Filtering and Control for Post-5G Wireless
  Connectivity","The traditional role of a communication engineer is to address the technical
problem of transporting bits reliably over a noisy channel. With the emergence
of 5G, and the availability of a variety of competing and coexisting wireless
systems, wireless connectivity is becoming a commodity. This article argues
that communication engineers in the post-5G era should extend the scope of
their activity in terms of design objectives and constraints beyond
connectivity to encompass the semantics of the transferred bits within the
given applications and use cases. To provide a platform for semantic-aware
connectivity solutions, this paper introduces the concept of a
semantic-effectiveness (SE) plane as a core part of future communication
architectures. The SE plane augments the protocol stack by providing
standardized interfaces that enable information filtering and direct control of
functionalities at all layers of the protocol stack. The advantages of the SE
plane are described in the perspective of recent developments in 5G, and
illustrated through a number of example applications. The introduction of a SE
plane may help replacing the current ""next-G paradigm"" in wireless evolution
with a framework based on continuous improvements and extensions of the systems
and standards.",
SE Directive,http://arxiv.org/abs/1808.08127v1,"Recalibrating Fully Convolutional Networks with Spatial and Channel
  'Squeeze & Excitation' Blocks","In a wide range of semantic segmentation tasks, fully convolutional neural
networks (F-CNNs) have been successfully leveraged to achieve state-of-the-art
performance. Architectural innovations of F-CNNs have mainly been on improving
spatial encoding or network connectivity to aid gradient flow. In this article,
we aim towards an alternate direction of recalibrating the learned feature maps
adaptively; boosting meaningful features while suppressing weak ones. The
recalibration is achieved by simple computational blocks that can be easily
integrated in F-CNNs architectures. We draw our inspiration from the recently
proposed 'squeeze & excitation' (SE) modules for channel recalibration for
image classification. Towards this end, we introduce three variants of SE
modules for segmentation, (i) squeezing spatially and exciting channel-wise,
(ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and
channel 'squeeze & excitation'. We effectively incorporate the proposed SE
blocks in three state-of-the-art F-CNNs and demonstrate a consistent
improvement of segmentation accuracy on three challenging benchmark datasets.
Importantly, SE blocks only lead to a minimal increase in model complexity of
about 1.5%, while the Dice score increases by 4-9% in the case of U-Net. Hence,
we believe that SE blocks can be an integral part of future F-CNN
architectures.",
SE Directive,http://arxiv.org/abs/1809.00057v2,"A Survey on State Estimation Techniques and Challenges in Smart
  Distribution Systems","This paper presents a review of the literature on State Estimation (SE) in
power systems. While covering some works related to SE in transmission systems,
the main focus of this paper is Distribution System State Estimation (DSSE).
The paper discusses a few critical topics of DSSE, including mathematical
problem formulation, application of pseudo-measurements, metering instrument
placement, network topology issues, impacts of renewable penetration, and
cyber-security. Both conventional and modern data-driven and probabilistic
techniques have been reviewed. This paper can provide researchers and utility
engineers with insights into the technical achievements, barriers, and future
research directions of DSSE.",
SE Directive,http://arxiv.org/abs/1101.1685v1,"Longitudinal Dependance Of Solsticial Hadley Cell Detected At The Edge
  Of The Massive Martian Erg","Using public HIRISE images of MARS, I derive the wind directions at high
Northern lattitudes, where many interesting eolian features are observed.
BArchan dunes show prominent wind direction from the North indicating that they
formed during the southern summer. But a few record consistent SE winds near
the UTOPIA PLANITIA basin. The wind reversal is consistent with a local
perturbation of the solsticial Hadley cell caused by geological depression.",
SE Directive,http://arxiv.org/abs/1411.7753v1,On Low Discrepancy Samplings in Product Spaces of Motion Groups,"Deterministically generating near-uniform point samplings of the motion
groups like SO(3), SE(3) and their n-wise products SO(3)^n, SE(3)^n is
fundamental to numerous applications in computational and data sciences. The
natural measure of sampling quality is discrepancy. In this work, our main goal
is construct low discrepancy deterministic samplings in product spaces of the
motion groups. To this end, we develop a novel strategy (using a two-step
discrepancy construction) that leads to an almost exponential improvement in
size (from the trivial direct product). To the best of our knowledge, this is
the first nontrivial construction for SO(3)^n, SE(3)^n and the hypertorus T^n.
  We also construct new low discrepancy samplings of S^2 and SO(3). The central
component in our construction for SO(3) is an explicit construction of N points
in S^2 with discrepancy \tilde{\O}(1/\sqrt{N}) with respect to convex sets,
matching the bound achieved for the special case of spherical caps in
\cite{ABD_12}. We also generalize the discrepancy of Cartesian product sets
\cite{Chazelle04thediscrepancy} to the discrepancy of local Cartesian product
sets.
  The tools we develop should be useful in generating low discrepancy samplings
of other complicated geometric spaces.",
SE Directive,http://arxiv.org/abs/1703.07172v1,"Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural
  Network Based Speech Enhancement","We propose a multi-objective framework to learn both secondary targets not
directly related to the intended task of speech enhancement (SE) and the
primary target of the clean log-power spectra (LPS) features to be used
directly for constructing the enhanced speech signals. In deep neural network
(DNN) based SE we introduce an auxiliary structure to learn secondary
continuous features, such as mel-frequency cepstral coefficients (MFCCs), and
categorical information, such as the ideal binary mask (IBM), and integrate it
into the original DNN architecture for joint optimization of all the
parameters. This joint estimation scheme imposes additional constraints not
available in the direct prediction of LPS, and potentially improves the
learning of the primary target. Furthermore, the learned secondary information
as a byproduct can be used for other purposes, e.g., the IBM-based
post-processing in this work. A series of experiments show that joint LPS and
MFCC learning improves the SE performance, and IBM-based post-processing
further enhances listening quality of the reconstructed speech.",
SE Directive,http://arxiv.org/abs/1803.02579v2,"Concurrent Spatial and Channel Squeeze & Excitation in Fully
  Convolutional Networks","Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in
image segmentation for a plethora of applications. Architectural innovations
within F-CNNs have mainly focused on improving spatial encoding or network
connectivity to aid gradient flow. In this paper, we explore an alternate
direction of recalibrating the feature maps adaptively, to boost meaningful
features, while suppressing weak ones. We draw inspiration from the recently
proposed squeeze & excitation (SE) module for channel recalibration of feature
maps for image classification. Towards this end, we introduce three variants of
SE modules for image segmentation, (i) squeezing spatially and exciting
channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)
and (iii) concurrent spatial and channel squeeze & excitation (scSE). We
effectively incorporate these SE modules within three different
state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent
improvement of performance across all architectures, while minimally effecting
model complexity. Evaluations are performed on two challenging applications:
whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)
and organ segmentation on whole body contrast enhanced CT scans (Visceral
Dataset).",
SE Directive,http://arxiv.org/abs/1710.04814v1,"Optical constants of DC sputtering derived ITO, TiO2 and TiO2:Nb thin
  films characterized by spectrophotometry and spectroscopic ellipsometry for
  optoelectronic devices","Thin films of inorganic materials as Tin-doped indium oxide, titanium oxide,
Niobium doped titanium oxide, were deposited for comparison on glass and
Polyethylene terephthalate (PET) substrates with a DC sputtering method. These
thin films have been characterized by different techniques: Dektak Surface
Profilometer, X-ray diffraction (XRD), SEM, (UV/Vis/NIR) spectrophotometer and
spectroscopic ellipsometry (SE). The optical parameters of these films such as
transmittance, reflectance, refractive index, extinction coefficient, energy
gap obtained with different electronic transitions, real and imaginary
({\epsilon}_r,{\epsilon}_i) dielectric constants, were determined in the
wavelengths range of (200 - 2200) nm. The results were compared with SE
measurements in the ranges of (0.56- 6.19) eV by a new amorphous model with
steps of 1 nm. SE measurements of optical constant have been examined and
confirm the accuracy of the (UV/Vis/NIR) results. The optical properties
indicate an excellent transmittance in the visible range of (400 - 800) nm. The
average transmittance of films on glass is about (86%, 91%, 85%) for (ITO,
TiO2, TiO2:Nb (NTO)) respectively and decreases to about (85%, 81%, 82%) for
PET substrates. For all these materials the optical band gap for direct
transition was (3.53, 3.3, 3.6) eV on glass substrates and on PET substrates
using two methods (UV and SE). A comparison between optical constants and
thickness of these ultrathin films observed gives an excellent agreement with
the UV results. The deposited films were also analyzed by XRD and showed an
amorphous structure. The structural morphology of these thin films has been
investigated and compared.","Journal of Non-Crystalline Solids Volume 476, 15 November 2017,
  Pages 1-14"
SE Directive,http://arxiv.org/abs/1805.01026v3,"Computing CNN Loss and Gradients for Pose Estimation with Riemannian
  Geometry","Pose estimation, i.e. predicting a 3D rigid transformation with respect to a
fixed co-ordinate frame in, SE(3), is an omnipresent problem in medical image
analysis with applications such as: image rigid registration, anatomical
standard plane detection, tracking and device/camera pose estimation. Deep
learning methods often parameterise a pose with a representation that separates
rotation and translation. As commonly available frameworks do not provide means
to calculate loss on a manifold, regression is usually performed using the
L2-norm independently on the rotation's and the translation's
parameterisations, which is a metric for linear spaces that does not take into
account the Lie group structure of SE(3). In this paper, we propose a general
Riemannian formulation of the pose estimation problem. We propose to train the
CNN directly on SE(3) equipped with a left-invariant Riemannian metric,
coupling the prediction of the translation and rotation defining the pose. At
each training step, the ground truth and predicted pose are elements of the
manifold, where the loss is calculated as the Riemannian geodesic distance. We
then compute the optimisation direction by back-propagating the gradient with
respect to the predicted pose on the tangent space of the manifold SE(3) and
update the network weights. We thoroughly evaluate the effectiveness of our
loss function by comparing its performance with popular and most commonly used
existing methods, on tasks such as image-based localisation and intensity-based
2D/3D registration. We also show that hyper-parameters, used in our loss
function to weight the contribution between rotations and translations, can be
intrinsically calculated from the dataset to achieve greater performance
margins.",
SE Directive,http://arxiv.org/abs/0912.5511v1,A general approach to belief change in answer set programming,"We address the problem of belief change in (nonmonotonic) logic programming
under answer set semantics. Unlike previous approaches to belief change in
logic programming, our formal techniques are analogous to those of
distance-based belief revision in propositional logic. In developing our
results, we build upon the model theory of logic programs furnished by SE
models. Since SE models provide a formal, monotonic characterisation of logic
programs, we can adapt techniques from the area of belief revision to belief
change in logic programs. We introduce methods for revising and merging logic
programs, respectively. For the former, we study both subset-based revision as
well as cardinality-based revision, and we show that they satisfy the majority
of the AGM postulates for revision. For merging, we consider operators
following arbitration merging and IC merging, respectively. We also present
encodings for computing the revision as well as the merging of logic programs
within the same logic programming framework, giving rise to a direct
implementation of our approach in terms of off-the-shelf answer set solvers.
These encodings reflect in turn the fact that our change operators do not
increase the complexity of the base formalism.",
SE Directive,http://arxiv.org/abs/1307.8051v1,Secondary Emission Calorimetry: Fast and Radiation-Hard,"A novel calorimeter sensor for electron, photon and hadron energy measurement
based on Secondary Emission(SE) to measure ionization is described, using
sheet-dynodes directly as the active detection medium; the shower particles in
an SE calorimeter cause direct secondary emission from dynode arrays comprising
the sampling or absorbing medium. Data is presented on prototype tests and
Monte Carlo simulations. This sensor can be made radiation hard at GigaRad
levels, is easily transversely segmentable at the mm scale, and in a
calorimeter has energy signal rise-times and integration comparable to or
better than plastic scintillation/PMT calorimeters. Applications are mainly in
the energy and intensity frontiers.",
cross-border mobility,http://arxiv.org/abs/1309.5725v1,"Comparing the impact of mobile nodes arrival patterns in mobile ad hoc
  networks using poisson and pareto models","Mobile Ad hoc Networks (MANETs) are dynamic networks populated by mobile
stations, or mobile nodes (MNs). Mobility model is a hot topic in many areas,
for example, protocol evaluation, network performance analysis and so on.How to
simulate MNs mobility is the problem we should consider if we want to build an
accurate mobility model. When new nodes can join and other nodes can leave the
network and therefore the topology is dynamic.Specifically, Mobile Ad hoc
Networks consist of a collection of nodes randomly placed in a line (not
necessarily straight). Mobile Ad hoc Networks do appear in many real-world
network applications such as a vehicular Mobile Ad hoc Networks built along a
highway in a city environment or people in a particular location. Mobile Nodes
in Mobile Ad hoc Networks are usually laptops, Personal Digital Assistants or
mobile phones. This paper presents comparative results that have been carried
out via Matrix lab software simulation. The study investigates the impact of
mobility predictive models on mobile nodes parameters such as, the arrival rate
and the size of mobile nodes in a given area using Pareto and Poisson
distributions. The results have indicated that mobile nodes arrival rates may
have influence on Mobile Nodes population (as a larger number) in a location.
The Pareto distribution is more reflective of the modeling mobility for Mobile
Ad hoc Networks than the Poisson distribution.","International Journal of Wireless & Mobile Networks (IJWMN) Vol.
  5, No. 4, August 2013 International Journal of Wireless & Mobile Networks
  (IJWMN) Vol. 5, No. 4, August 2013"
cross-border mobility,http://arxiv.org/abs/1003.4078v1,"A Group Vehicular Mobility Model for Routing Protocol Analysis in Mobile
  Ad Hoc Network","Performance of routing protocols in mobile ad-hoc networks is greatly
affected by the dynamic nature of nodes, route failures, wireless channels with
variable bandwidth and scalability issues. A mobility model imitates the real
world movement of mobile nodes and is central component to simulation based
studies. In this paper we consider mobility nodes which mimic the vehicular
motion of nodes like Manhattan mobility model and City Section mobility model.
We also propose a new Group Vehicular mobility model that takes the best
features of group mobility models like Reference Point Group mobility model and
applies it to vehicular models. We analyze the performance of our model known
as Group Vehicular mobility model (GVMM) and other vehicular mobility models
with various metrics. This analysis provides us with an insight about the
impact of mobility models on the performance of routing protocols for ad-hoc
networks. The routing protocols are simulated and measured for performance and
finally we arrive at the correlation about the impact of mobility models on
routing protocols, which are central to the design of mobile adhoc networks.","Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/"
cross-border mobility,http://arxiv.org/abs/1910.01290v1,"Mobile Phone Use as Sequential Processes: From Discrete Behaviors to
  Sessions of Behaviors and Trajectories of Sessions","Mobile phone use is an unfolding process by nature. In this study, it is
explicated as two sequential processes: mobile sessions composed of an
uninterrupted set of behaviors and mobile trajectories composed of mobile
sessions and mobile-off time. A dataset of a five-month behavioral logfile of
mobile application use by approximately 2,500 users in Hong Kong is used.
Mobile sessions are constructed and mined to uncover sequential characteristics
and patterns in mobile phone use. Mobile trajectories are analyzed to examine
intraindividual change and interindividual differences on mobile re-engagement
as indicators of behavioral dynamics in mobile phone use. The study provides
empirical support for and expands the boundaries of existing theories about
combinatorial use of information and communication technologies (ICTs).
Finally, the understanding on mobile temporality is enhanced, that is, mobile
temporality is homogeneous across social sectors. Furthermore, mobile phones
redefine, rather than blur, the boundary between private and public time.",
cross-border mobility,http://arxiv.org/abs/1004.1747v1,Mobile Database System: Role of Mobility on the Query Processing,"The rapidly expanding technology of mobile communication will give mobile
users capability of accessing information from anywhere and any time. The
wireless technology has made it possible to achieve continuous connectivity in
mobile environment. When the query is specified as continuous, the requesting
mobile user can obtain continuously changing result. In order to provide
accurate and timely outcome to requesting mobile user, the locations of moving
object has to be closely monitored. The objective of paper is to discuss the
problem related to the role of personal and terminal mobility and query
processing in the mobile environment.","IJCSIS, Vol. 7 No. 3, March 2010, 211-216"
cross-border mobility,http://arxiv.org/abs/1212.2567v1,The effect of the number of mobile nodes on varying speeds of manets,"Mobile Ad hoc Networks are dynamic networks populated by mobile devices, or
mobile nodes.The Mobile Nodes are free to move anywhere and at any time. The
population of the nodes may have some influence on the mobility rate of the
mobile nodes. This paper presents simulation results using Matrix Laboratory
software. The study investigates the influence of mobile nodes parameters such
as number of nodes on the nodes speeds and nodes distribution in a given area.
The results have indicated that the number of mobile nodes have impact on the
speeds of the nodes in a location.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.6, November 2012"
cross-border mobility,http://arxiv.org/abs/cs/0611144v6,"Coding Improves the Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc
  Networks: Two-Dimensional I.I.D. Mobility Models","In this paper, we investigate the delay-throughput trade-offs in mobile
ad-hoc networks under two-dimensional i.i.d. mobility models. We consider two
mobility time-scales: (i) Fast mobility where node mobility is at the same
time-scale as data transmissions; (ii) Slow mobility where node mobility is
assumed to occur at a much slower time-scale than data transmissions. Given a
delay constraint $D,$ the main results are as follows: (1) For the
two-dimensional i.i.d. mobility model with fast mobiles, the maximum throughput
per source-destination (S-D) pair is shown to be $O(\sqrt{D/n}),$ where $n$ is
the number of mobiles. (2) For the two-dimensional i.i.d. mobility model with
slow mobiles, the maximum throughput per S-D pair is shown to be
$O(\sqrt[3]{D/n}).$ (3) For each case, we propose a joint coding-scheduling
algorithm to achieve the optimal delay-throughput trade-offs.",
cross-border mobility,http://arxiv.org/abs/1303.3805v2,Measuring and Predicting Speed of Social Mobilization,"Large-scale mobilization of individuals across social networks is becoming
increasingly influential in society. However, little is known about what traits
of recruiters and recruits and affect the speed at which one mobilizes the
other. Here we identify and measure traits of individuals and their
relationships that predict mobilization speed. We ran a global social
mobilization contest and recorded personal traits of the participants and those
they recruited. We identified how those traits corresponded with the speed of
mobilization. Recruits mobilized faster when they first heard about the contest
directly from the contest organization, and decreased in speed when hearing
from less personal source types (e.g. family vs. media). Mobilization was
faster when the recruiter and the recruit heard about the contest through the
same source type, and slower when both individuals were in different countries.
Females mobilized other females faster than males mobilized other males.
Younger recruiters mobilized others faster, and older recruits mobilized
slower. These findings suggest relevant factors for engineering social
mobilization tasks for increased speed.",
cross-border mobility,http://arxiv.org/abs/1701.06767v2,Towards the quality improvement of cross-platform mobile applications,"During last ten years, the number of smartphones and mobile applications has
been constantly growing. Android, iOS and Windows Mobile are three mobile
platforms that cover almost all smartphones in the world in 2017. Developing a
mobile app involves first to choose the platforms the app will run, and then to
develop specific solutions (i.e., native apps) for each chosen platform using
platform-related toolkits such as AndroidSDK. Across-platform mobile
application is an app that runs on two or more mobile platforms. Several
frameworks have been proposed to simplify the development of cross-platform
mobile applications and to reduce development and maintenance costs.They are
called cross-platform mobile app development frameworks.However, to our
knowledge, the life-cycle and the quality of cross-platforms mobile
applications built using those frameworks have not been studied in depth. Our
main goal is to first study the processes of development and maintenance of
mobile applications built using cross-platform mobile app development
frameworks, focusing particularly on the bug-fixing activity. Then, we aim at
defining tools for automated repairing bugs from cross-platform mobile
applications.",
cross-border mobility,http://arxiv.org/abs/1402.5477v1,"Mobile Conductance and Gossip-based Information Spreading in Mobile
  Networks","In this paper, we propose a general analytical framework for information
spreading in mobile networks based on a new performance metric, mobile
conductance, which allows us to separate the details of mobility models from
the study of mobile spreading time. We derive a general result for the
information spreading time in mobile networks in terms of this new metric, and
instantiate it through several popular mobility models. Large scale network
simulation is conducted to verify our analysis.",
cross-border mobility,http://arxiv.org/abs/0908.0667v1,"Interworking Scheme Using Optimized SIP Mobility for MultiHomed Mobile
  Nodes in Wireless Heterogeneous Networks","Nowadays, mobile users wish to use their multi-interface mobile devices to
access the Internet through network points of attachment (PoA) based on
heterogeneous wireless technologies. They also wish to seamlessly change the
PoAs during their ongoing sessions to improve service quality and/or reduce
monetary cost. If appropriately handled, multihomed mobile nodes offer a
potential solution to this issue. In this sense, the management of multihomed
mobile nodes in heterogeneous environment is a key research topic. In this
paper, we present an improvement of SIP mobility (pre-call plus mid-call
mobility) to support seamless mobility of multihomed mobile nodes in
heterogeneous wireless networks. Pre-call mobility is extended to associate
user identifier (i.e. SIP URI) and interface identifiers (i.e. IP addresses).
The multiple addresses of a mobile device are weighted by the user to create a
priority list in the SIP server so as to guarantee resilient reachability of
mobile nodes and to avoid unnecessary signaling through wireless links, thus
saving radio resources. Then, three variations of mid-call mobility, called
hard, hybrid and soft procedures, are also proposed. Their main aim is to
minimize, or even avoid, packet losses during interface switching at the mobile
node. The proposed solutions have been implemented in a wireless heterogeneous
testbed composed of 802.11 WLAN plus 3.5 cellular network, which are fully
controlled and configurable. The testbed has been used to study the performance
and the robustness of the three proposed mid-call mobility procedures.",
cross-border mobility,http://arxiv.org/abs/1309.5944v1,"The impact of mobile nodes arrival patterns in mobile ad hoc networks
  using poisson models","Mobile Ad hoc Networks (MANETs) are dynamic networks populated by mobile
stations, or mobile nodes (MNs). Specifically, MANETs consist of a collection
of nodes randomly placed in a line (not necessarily straight). Mobile Ad hoc
Networks do appear in many real-world network applications such as a vehicular
Mobile Ad hoc Networks built along a highway in a city environment or people in
a particular location. Mobile Nodes in Mobile Ad hoc Networks are usually
laptops, Personal Digital Assistants or mobile phones. These devices may use
Blue-tooth and/or IEEE 802.11 network interfaces and communicate in a
decentralized manner. Mobility is a key feature of Mobile Ad hoc Networks. Each
node may work as a router and the network can dynamically change with time;
when new nodes can join, and other nodes can leave the network. This paper
presents comparative results that have been carried out via Matrix lab software
simulation. The study investigates the impact of mobile nodes parameters such
as the speed, the arrival rate and the size of mobile nodes in a given area
using Poisson distribution. The results have indicated that mobile nodes
arrival rates may have influence on Mobile Nodes population (as a larger
number) in a location.","International Journal of Managing Information Technology (IJMIT)
  Vol.4, No.3, August 2012"
cross-border mobility,http://arxiv.org/abs/1408.5420v1,"Measures of Human Mobility Using Mobile Phone Records Enhanced with GIS
  Data","In the past decade, large scale mobile phone data have become available for
the study of human movement patterns. These data hold an immense promise for
understanding human behavior on a vast scale, and with a precision and accuracy
never before possible with censuses, surveys or other existing data collection
techniques. There is already a significant body of literature that has made key
inroads into understanding human mobility using this exciting new data source,
and there have been several different measures of mobility used. However,
existing mobile phone based mobility measures are inconsistent, inaccurate, and
confounded with social characteristics of local context. New measures would
best be developed immediately as they will influence future studies of mobility
using mobile phone data. In this article, we do exactly this. We discuss
problems with existing mobile phone based measures of mobility and describe new
methods for measuring mobility that address these concerns. Our measures of
mobility, which incorporate both mobile phone records and detailed GIS data,
are designed to address the spatial nature of human mobility, to remain
independent of social characteristics of context, and to be comparable across
geographic regions and time. We also contribute a discussion of the variety of
uses for these new measures in developing a better understanding of how human
mobility influences micro-level human behaviors and well-being, and macro-level
social organization and change.",
cross-border mobility,http://arxiv.org/abs/1605.06884v3,"Mobile Cloud Computing with a UAV-Mounted Cloudlet: Optimal Bit
  Allocation for Communication and Computation","Mobile cloud computing relieves the tension between compute-intensive mobile
applications and battery-constrained mobile devices by enabling the offloading
of computing tasks from mobiles to a remote processors. This paper considers a
mobile cloud computing scenario in which the ""cloudlet"" processor that provides
offloading opportunities to mobile devices is mounted on unmanned aerial
vehicles (UAVs) to enhance coverage. Focusing on a slotted communication system
with frequency division multiplexing between mobile and UAV, the joint
optimization of the number of input bits transmitted in the uplink by the
mobile to the UAV, the number of input bits processed by the cloudlet at the
UAV, and the number of output bits returned by the cloudlet to the mobile in
the downlink in each slot is carried out by means of dual decomposition under
maximum latency constraints with the aim of minimizing the mobile energy
consumption. Numerical results reveal the critical importance of an optimized
bit allocation in order to enable significant energy savings as compared to
local mobile execution for stringent latency constraints.",
cross-border mobility,http://arxiv.org/abs/1801.02837v1,Malware detection techniques for mobile devices,"Mobile devices have become very popular nowadays, due to its portability and
high performance, a mobile device became a must device for persons using
information and communication technologies. In addition to hardware rapid
evolution, mobile applications are also increasing in their complexity and
performance to cover most needs of their users. Both software and hardware
design focused on increasing performance and the working hours of a mobile
device. Different mobile operating systems are being used today with different
platforms and different market shares. Like all information systems, mobile
systems are prone to malware attacks. Due to the personality feature of mobile
devices, malware detection is very important and is a must tool in each device
to protect private data and mitigate attacks. In this paper, analysis of
different malware detection techniques used for mobile operating systems is
provides. The focus of the analysis will be on the to two competing mobile
operating systems - Android and iOS. Finally, an assessment of each technique
and a summary of its advantages and disadvantages is provided. The aim of the
work is to establish a basis for developing a mobile malware detection tool
based on user profiling.","International Journal of Mobile Network Communications &
  Telematics ( IJMNCT), vol 7, 2015"
cross-border mobility,http://arxiv.org/abs/1406.0256v1,"HYBRIST Mobility Model- A Novel Hybrid Mobility Model for VANET
  Simulations","Simulations play a vital role in implementing, testing and validating
proposed algorithms and protocols in VANET. Mobility model, defined as the
movement pattern of vehicles, is one of the main factors that contribute
towards the efficient implementation of VANET algorithms and protocols. Using
near reality mobility models ensure that accurate results are obtained from
simulations. Mobility models that have been proposed and used to implement and
test VANET protocols and algorithms are either the urban mobility model or
highway mobility model. Algorithms and protocols implemented using urban or
highway mobility models may not produce accurate results in hybrid mobility
models without enhancement due to the vast differences in mobility patterns. It
is on this score the Hybrist, a novel hybrid mobility model is proposed. The
realistic mobility pattern trace file of the proposed Hybrist hybrid mobility
model can be imported to VANET simulators such as Veins and network simulators
such as ns2 and Qualnet to simulate VANET algorithms and protocols.","IJCA 86(14):15-21, January 2014"
cross-border mobility,http://arxiv.org/abs/1404.6935v1,"Homophily and the Speed of Social Mobilization: The Effect of Acquired
  and Ascribed Traits","Large-scale mobilization of individuals across social networks is becoming
increasingly prevalent in society. However, little is known about what affects
the speed of social mobilization. Here we use a framed field experiment to
identify and measure properties of individuals and their relationships that
predict mobilization speed. We ran a global social mobilization contest and
recorded personal traits of the participants and those they recruited. We
studied the effects of ascribed traits (gender, age) and acquired traits
(geography, and information source) on the speed of mobilization. We found that
homophily, a preference for interacting with other individuals with similar
traits, had a mixed role in social mobilization. Homophily was present for
acquired traits, in which mobilization speed was faster when the recuiter and
recruit had the same trait compared to different traits. In contrast, we did
not find support for homophily for the ascribed traits. Instead, those traits
had other, non-homophily effects: Females mobilized other females faster than
males mobilized other males. Younger recruiters mobilized others faster, and
older recruits mobilized slower. Recruits also mobilized faster when they first
heard about the contest directly from the contest organization, and decreased
in speed when hearing from less personal source types (e.g. family vs. media).
These findings show that social mobilization includes dynamics that are unlike
other, more passive forms of social activity propagation. These findings
suggest relevant factors for engineering social mobilization tasks for
increased speed.",PLoS ONE 9(4): e95140. 2014
cross-border mobility,http://arxiv.org/abs/1811.02491v1,"Mobile Data Science: Towards Understanding Data-Driven Intelligent
  Mobile Applications","Due to the popularity of smart mobile phones and context-aware technology,
various contextual data relevant to users' diverse activities with mobile
phones is available around us. This enables the study on mobile phone data and
context-awareness in computing, for the purpose of building data-driven
intelligent mobile applications, not only on a single device but also in a
distributed environment for the benefit of end users. Based on the availability
of mobile phone data, and the usefulness of data-driven applications, in this
paper, we discuss about mobile data science that involves in collecting the
mobile phone data from various sources and building data-driven models using
machine learning techniques, in order to make dynamic decisions intelligently
in various day-to-day situations of the users. For this, we first discuss the
fundamental concepts and the potentiality of mobile data science to build
intelligent applications. We also highlight the key elements and explain
various key modules involving in the process of mobile data science. This
article is the first in the field to draw a big picture, and thinking about
mobile data science, and it's potentiality in developing various data-driven
intelligent mobile applications. We believe this study will help both the
researchers and application developers for building smart data-driven mobile
applications, to assist the end mobile phone users in their daily activities.","EAI Endorsed Transactions on Scalable Information Systems, 2018"
cross-border mobility,http://arxiv.org/abs/1112.4018v1,Mobile IP and protocol authentication extension,"Mobile IP is an open standard, defined by the Internet Engineering Task Force
(IETF) RFC 3220. By using Mobile IP, you can keep the same IP address, stay
connected, and maintain ongoing applications while roaming between IP networks.
Mobile IP is scalable for the Internet because it is based on IP - any media
that can support IP can support Mobile IP.",
cross-border mobility,http://arxiv.org/abs/1002.1874v1,Mobility Impact on Performance of Mobile Grids,"Wireless mobile grids are one of the emerging grid types, which help to pool
the resources of several willing and cooperative mobile devices to resolve a
computationally intensive task. The mobile grids exhibit stronger challenges
like mobility management of devices, providing transparent access to grid
resources, task management and handling of limited resources so that resources
are shared efficiently. Task execution on these devices should not be affected
by their mobility. The proposed work presents performance evaluation of
wireless mobile grid using normal walk mobility model. The normal walk model
represents daily motion of users and the direction of motion is mostly
symmetric in a real life environment, thus it is effective in location updating
of a mobile station and in turn helps task distribution among these available
mobile stations. Some of the performance parameters such as Task Execution
Time, task failure rate, communication overhead on Brokering Server and
Monitoring Cost are discussed.","International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 1, pp. 106-111, January 2010, USA"
cross-border mobility,http://arxiv.org/abs/1006.4326v1,"Stationary and Mobile Target Detection using Mobile Wireless Sensor
  Networks","In this work, we study the target detection and tracking problem in mobile
sensor networks, where the performance metrics of interest are probability of
detection and tracking coverage, when the target can be stationary or mobile
and its duration is finite. We propose a physical coverage-based mobility
model, where the mobile sensor nodes move such that the overlap between the
covered areas by different mobile nodes is small. It is shown that for
stationary target scenario the proposed mobility model can achieve a desired
detection probability with a significantly lower number of mobile nodes
especially when the detection requirements are highly stringent. Similarly,
when the target is mobile the coverage-based mobility model produces a
consistently higher detection probability compared to other models under
investigation.","Proc. INFOCOM IEEE Conference on Computer Communications 2010, p.
  1"
cross-border mobility,http://arxiv.org/abs/1009.1708v1,"On the Performance Evaluation and Analysis of the Hybridised Bittorrent
  Protocol with Partial Mobility Characteristics","Engaging mobility with file sharing is considered very promising in today's
run Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing
protocol can be rarely combined with the mobility scenario framework since
resources are not available due to the dynamically changing topology network.
As a result, mobility in P2P-oriented file sharing platforms, degrades the
end-to-end efficiency and the system's performance. This work proposes a new
hybridized model, which takes into account the mobility characteristics of the
combined Bittorrent protocol in a centralized manner enabling partial mobility
characteristics, where the clients of the network use a distinct technique to
differentiate between mobile and static nodes. Many parameters were taken into
consideration like the round trip delays, the diffusion process, and the
seeding techniques, targeting the maximization of the average throughput in the
clustered swarms containing mobile peers. Partial mobility characteristics are
set in a peer-tracker and peer-peer communication enhancement schema with
partial mobility, allowing an optimistic approach to attain high availability
and throughput response as simulation results show.",
cross-border mobility,http://arxiv.org/abs/1007.2980v1,Publishing and Discovery of Mobile Web Services in Peer to Peer Networks,"It is now feasible to host Web Services on a mobile device due to the
advances in cellular devices and mobile communication technologies. However,
the reliability, usability and responsiveness of the Mobile Hosts depend on
various factors including the characteristics of available network,
computational resources, and better means of searching the services provided by
them. P2P enhances the adoption of Mobile Host in commercial environments.
Mobile Hosts in P2P can collaboratively share the resources of individual
peers. P2P also enhances the service discovery of huge number of Web Services
possible with Mobile Hosts. Advanced features like post filtering with weight
of keywords and context-awareness can also be exploited to select the best
possible mobile Web Service. This paper proposes the concept of Mobile Hosts in
P2P networks and identifies the means of publishing and discovery of Web
Services in mobile P2P networks.",
cross-border mobility,http://arxiv.org/abs/1312.6565v1,Mobile Multimedia Recommendation in Smart Communities: A Survey,"Due to the rapid growth of internet broadband access and proliferation of
modern mobile devices, various types of multimedia (e.g. text, images, audios
and videos) have become ubiquitously available anytime. Mobile device users
usually store and use multimedia contents based on their personal interests and
preferences. Mobile device challenges such as storage limitation have however
introduced the problem of mobile multimedia overload to users. In order to
tackle this problem, researchers have developed various techniques that
recommend multimedia for mobile users. In this survey paper, we examine the
importance of mobile multimedia recommendation systems from the perspective of
three smart communities, namely, mobile social learning, mobile event guide and
context-aware services. A cautious analysis of existing research reveals that
the implementation of proactive, sensor-based and hybrid recommender systems
can improve mobile multimedia recommendations. Nevertheless, there are still
challenges and open issues such as the incorporation of context and social
properties, which need to be tackled in order to generate accurate and
trustworthy mobile multimedia recommendations.","IEEE Access, vol.1, pp.606-624, 2013"
cross-border mobility,http://arxiv.org/abs/1706.08048v1,"Mobile Phone Forensics: An Investigative Framework based on User
  Impulsivity and Secure Collaboration Errors","This paper uses a scenario-based role-play experiment based on the usage of
QR codes to detect how mobile users respond to social engineering attacks
conducted via mobile devices. The results of this experiment outline a guided
mobile phone forensics investigation method which could facilitate the work of
digital forensics investigators while analysing the data from mobile devices.
The behavioural response of users could be impacted by several aspects, such as
impulsivity, smartphone usage and security or simply awareness that QR codes
could contain malware. The findings indicate that the impulsivity of users is
one of the key areas that determine the common mistakes of mobile device users.
As a result, an investigative framework for mobile phone forensics is proposed
based on the impulsivity and common mistakes of mobile device users. As a
result, an investigative framework for mobile phone forensics is proposed based
on the impulsivity and common mistakes of mobile device users. It could help
the forensics investigators by potentially shortening the time spent on
investigation of possible breach scenarios.","Contemporary Digital Forensic Investigations of Cloud and Mobile
  Applications, Pages 79-89,Chapter 6, 2017"
cross-border mobility,http://arxiv.org/abs/1808.01989v1,Technological conditions of mobile learning in high school,"This paper reviews the history of mobile learning, provides a definition of
mobile learning. The properties, advantages and disadvantages of mobile
learning, areas of its implementation at the Technical University and mobile
learning tools were specified. The aim of the article is the analysis of the
modern state of mobile learning and the determination of the conditions of its
implementation at the high technical educational institutions. The process of
the mobile learning in the national education system is in its formation stage.
Nowadays the following stages of its development are formed, which are based on
the availability of the technical means for the mobile learning and the mobile
access implementation to educational resources. The mobile learning is the
logical and innovation process in the education system, which is defined as a
learning technology which uses the mobile devices, communication technology and
intelligent user interfaces.","Metallurgical and Mining Industry, 3(2015) 161-164"
cross-border mobility,http://arxiv.org/abs/1908.09047v1,Parameter Modeling for Small-Scale Mobility in Indoor THz Communication,"Despite such challenges as high path loss and equipment cost, THz
communication is becoming one of the potentially viable means through which
ultra-high data rate can be achieved. To compensate for the high path loss, we
present parameter modeling for indoor THz communication. To maximize efficient
and opportunistic use of resources, we analyze the potential workarounds for a
single access point to satisfy most of the mobile terminals by varying such
parameters as humidity, distance, frequency windows, beamwidths, antenna
placement, and user mobility type. One promising parameter is antenna
beamwidth, where narrower beams results in higher antenna gain. However, this
can lead to ""\textit{beamwidth dilemma}"" scenario, where narrower beamwidth can
result in significant outages due to device mobility and orientation. In this
paper, we address this challenge by presenting a mobility model that performs
an extensive analysis of different human mobility scenarios, where each
scenario has different data rate demands and movement patterns. We observe that
for mobile users, there are optimal beamwidths that are affected by the
mobility type (high mobility, constrained mobility, and low mobility) and AP
placement.",
cross-border mobility,http://arxiv.org/abs/0705.0326v1,"Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc Networks: Hybrid
  Random Walk and One-Dimensional Mobility Models","Optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models
have been established in [23], where we showed that the optimal trade-offs can
be achieved using rate-less codes when the required delay guarantees are
sufficient large. In this paper, we extend the results to other mobility models
including two-dimensional hybrid random walk model, one-dimensional i.i.d.
mobility model and one-dimensional hybrid random walk model. We consider both
fast mobiles and slow mobiles, and establish the optimal delay-throughput
trade-offs under some conditions. Joint coding-scheduling algorithms are also
proposed to achieve the optimal trade-offs.",
cross-border mobility,http://arxiv.org/abs/1310.4595v1,"Proceedings Ninth International Workshop on Foundations of Mobile
  Computing","Mobile communication has become a vigorous field of research in computer
science, due to the wide spreading of mobile technologies, applications and
services. The intertwining of communication, computation and mobility
constantly poses new challenges to algorithmic design in this area. The
Foundations of Mobile Computing (FOMC) workshop is dedicated to all aspects
that cover contributions both in the design and analysis of
discrete/distributed algorithms, and in the system modeling of mobile, wireless
and similarly dynamic networks. It aims to bring together the practitioners and
theoreticians of the field to foster cooperation between research in mobile
computing and algorithms.","EPTCS 132, 2013"
cross-border mobility,http://arxiv.org/abs/1605.02886v1,On Mobile Cloud for Smart City Applications,"This paper is devoted to mobile cloud services in Smart City projects. As per
mobile cloud computing paradigm, the data processing and storage are moved from
the mobile device to a cloud. In the same time, Smart City services typically
contain a set of applications with data sharing options. Most of the services
in Smart Cities are actually mashups combined data from several sources. This
means that access to all available data is vital to the services. And the
mobile cloud is vital because the mobile terminals are one of the main sources
for data gathering. In our work, we discuss criteria for selecting mobile cloud
services.",
cross-border mobility,http://arxiv.org/abs/1812.01077v1,Brief survey of Mobility Analyses based on Mobile Phone Datasets,"This is a brief survey of the research performed by Grandata Labs in
collaboration with numerous academic groups around the world on the topic of
human mobility. A driving theme in these projects is to use and improve Data
Science techniques to understand mobility, as it can be observed through the
lens of mobile phone datasets. We describe applications of mobility analyses
for urban planning, prediction of data traffic usage, building delay tolerant
networks, generating epidemiologic risk maps and measuring the predictability
of human mobility.",
cross-border mobility,http://arxiv.org/abs/cs/0105031v1,State Analysis and Aggregation Study for Multicast-based Micro Mobility,"IP mobility addresses the problem of changing the network point-of-attachment
transparently during movement. Mobile IP is the proposed standard by IETF.
Several studies, however, have shown that Mobile IP has several drawbacks, such
as triangle routing and poor handoff performance. Multicast-based mobility has
been proposed as a promising solution to the above problems, incurring less
end-to-end delays and fast smooth handoff. Nonetheless, such architecture
suffers from multicast state scalability problems with the growth in number of
mobile nodes. This architecture also requires ubiquitous multicast deployment
and more complex security measures. To alleviate these problems, we propose an
intra-domain multicast-based mobility solution. A mobility proxy allocates a
multicast address for each mobile that moves to its domain. The mobile uses
this multicast address within a domain for micro mobility. Also, aggregation is
considered to reduce the multicast state. We conduct multicast state analysis
to study the efficiency of several aggregation techniques. We use extensive
simulation to evaluate our protocol's performance over a variety of real and
generated topologies. We take aggregation gain as metric for our evaluation.
  Our simulation results show that in general leaky aggregation obtains better
gains than perfect aggregation. Also, we notice that aggregation gain increases
with the increase in number of visiting mobile nodes and with the decrease in
number of mobility proxies within a domain.",
cross-border mobility,http://arxiv.org/abs/1307.0687v1,Security for Smart Mobile Networks: The NEMESYS Approach,"The growing popularity of smart mobile devices such as smartphones and
tablets has made them an attractive target for cyber-criminals, resulting in a
rapidly growing and evolving mobile threat as attackers experiment with new
business models by targeting mobile users. With the emergence of the first
large-scale mobile botnets, the core network has also become vulnerable to
distributed denial-of-service attacks such as the signaling attack.
Furthermore, complementary access methods such as Wi-Fi and femtocells
introduce additional vulnerabilities for the mobile users as well as the core
network. In this paper, we present the NEMESYS approach to smart mobile network
security. The goal of the NEMESYS project is to develop novel security
technologies for seamless service provisioning in the smart mobile ecosystem,
and to improve mobile network security through a better understanding of the
threat landscape. To this purpose, NEMESYS will collect and analyze information
about the nature of cyber-attacks targeting smart mobile devices and the core
network so that appropriate counter-measures can be taken. We are developing a
data collection infrastructure that incorporates virtualized mobile honeypots
and honeyclients in order to gather, detect and provide early warning of mobile
attacks and understand the modus operandi of cyber-criminals that target mobile
devices. By correlating the extracted information with known attack patterns
from wireline networks, we plan to reveal and identify the possible shift in
the way that cyber-criminals launch attacks against smart mobile devices.",
cross-border mobility,http://arxiv.org/abs/1305.5483v1,"NEMESYS: Enhanced Network Security for Seamless Service Provisioning in
  the Smart Mobile Ecosystem","As a consequence of the growing popularity of smart mobile devices, mobile
malware is clearly on the rise, with attackers targeting valuable user
information and exploiting vulnerabilities of the mobile ecosystems. With the
emergence of large-scale mobile botnets, smartphones can also be used to launch
attacks on mobile networks. The NEMESYS project will develop novel security
technologies for seamless service provisioning in the smart mobile ecosystem,
and improve mobile network security through better understanding of the threat
landscape. NEMESYS will gather and analyze information about the nature of
cyber-attacks targeting mobile users and the mobile network so that appropriate
counter-measures can be taken. We will develop a data collection infrastructure
that incorporates virtualized mobile honeypots and a honeyclient, to gather,
detect and provide early warning of mobile attacks and better understand the
modus operandi of cyber-criminals that target mobile devices. By correlating
the extracted information with the known patterns of attacks from wireline
networks, we will reveal and identify trends in the way that cyber-criminals
launch attacks against mobile devices.",
cross-border mobility,http://arxiv.org/abs/1704.03065v2,"MoMo: a group mobility model for future generation mobile wireless
  networks","Existing group mobility models were not designed to meet the requirements for
accurate simulation of current and future short distance wireless networks
scenarios, that need, in particular, accurate, up-to-date informa- tion on the
position of each node in the network, combined with a simple and flexible
approach to group mobility modeling. A new model for group mobility in wireless
networks, named MoMo, is proposed in this paper, based on the combination of a
memory-based individual mobility model with a flexible group behavior model.
MoMo is capable of accurately describing all mobility scenarios, from
individual mobility, in which nodes move inde- pendently one from the other, to
tight group mobility, where mobility patterns of different nodes are strictly
correlated. A new set of intrinsic properties for a mobility model is proposed
and adopted in the analysis and comparison of MoMo with existing models. Next,
MoMo is compared with existing group mobility models in a typical 5G network
scenario, in which a set of mobile nodes cooperate in the realization of a
distributed MIMO link. Results show that MoMo leads to accurate, robust and
flexible modeling of mobility of groups of nodes in discrete event simulators,
making it suitable for the performance evaluation of networking protocols and
resource allocation algorithms in the wide range of network scenarios expected
to characterize 5G networks.",
cross-border mobility,http://arxiv.org/abs/1812.03289v1,"Mobile Money: Understanding and Predicting its Adoption and Use in a
  Developing Economy","Access to financial institutions is difficult in developing economies and
especially for the poor. However, the widespread adoption of mobile phones has
enabled the development of mobile money systems that deliver financial services
through the mobile phone network. Despite the success of mobile money, there is
a lack of quantitative studies that unveil which factors contribute to the
adoption and sustained usage of such services. In this paper, we describe the
results of a quantitative study that analyzes data from the world's leading
mobile money service, M-Pesa. We analyzed millions of anonymized mobile phone
communications and M-Pesa transactions in an African country. Our contributions
are threefold: (1) we analyze the customers' usage of M-Pesa and report
large-scale patterns of behavior; (2) we present the results of applying
machine learning models to predict mobile money adoption (AUC=0.691), and
mobile money spending (AUC=0.619) using multiple data sources: mobile phone
data, M-Pesa agent information, the number of M-Pesa friends in the user's
social network, and the characterization of the user's geographic location; (3)
we discuss the most predictive features in both models and draw key
implications for the design of mobile money services in a developing country.
We find that the most predictive features are related to mobile phone activity,
to the presence of M-Pesa users in a customer's ego-network and to mobility. We
believe that our work will contribute to the understanding of the factors
playing a role in the adoption and sustained usage of mobile money services in
developing economies.",
cross-border mobility,http://arxiv.org/abs/1108.6254v1,"Integrated Solution Scheme for Handover Latency Diminution in Proxy
  Mobile IPv6","Recent trends show that there are swift developments and fast convergence of
wireless and mobile communication networks with internet services to provide
the quality of ubiquitous access to network users. Most of the wireless
networks and mobile cellular networks are moving to be all IP based. These
networks are connected through the private IP core networks using the TCP/IP
protocol or through the Internet. As such, there is room to improve the
mobility support through the Internet and support ubiquitous network access by
providing seamless handover. This is especially true with the invention of
portable mobile and laptop devices that can be connected almost everywhere at
any time. However, the recent explosion on the usage of mobile and laptop
devices has also generated several issues in terms of performance and quality
of service. Nowadays, mobile users demand high quality performance, best
quality of services and seamless connections that support real-time application
such as audio and video streaming. The goal of this paper is to study the
impact and evaluate the mobility management protocols under micro mobility
domain on link layer and network layer handover performance. Therefore, this
paper proposes an integration solution of network-based mobility management
framework, based on Proxy Mobile IPv6, to alleviate handover latency, packet
loss and increase throughput and the performance of video transmission when
mobile host moves to new network during handover on high speed mobility.
Simulations are conducted to analyze the relationship between the network
performances with the moving speed of mobile host over mobility protocols.
Based on simulation results, we presented and analyzed the results of mobility
protocols under intra-domain traffics in micro mobility domain.",
cross-border mobility,http://arxiv.org/abs/0810.0394v1,Mobility Management Framework,"This paper investigates mobility management strategies from the point of view
of their need of signalling and processing resources on the backbone network
and load on the air interface. A method is proposed to model the serving
network and mobile node mobility in order to be able to compare the different
types of mobility management algorithms. To obtain a good description of the
network we calculate descriptive parameters from given topologies. Most
mobility approaches derived from existing protocols are analyzed and their
performances are numerically compared in various network and mobility
scenarios. We developed a mobility management framework that is able to give
general designing guidelines for the next generation mobility managements on
given network, technology and mobility properties. With our model an operator
can design the network and tune the parameters to obtain the optimal
implementation of course revising existing systems is also possible. We present
a vertical handover decision method as a special application of our model
framework.",
cross-border mobility,http://arxiv.org/abs/1107.4940v1,Mobile Cloud Computing: A Comparison of Application Models,"Cloud computing is an emerging concept combining many fields of computing.
The foundation of cloud computing is the delivery of services, software and
processing capacity over the Internet, reducing cost, increasing storage,
automating systems, decoupling of service delivery from underlying technology,
and providing flexibility and mobility of information. However, the actual
realization of these benefits is far from being achieved for mobile
applications and open many new research questions. In order to better
understand how to facilitate the building of mobile cloud-based applications,
we have surveyed existing work in mobile computing through the prism of cloud
computing principles. We give a definition of mobile cloud coputing and provide
an overview of the results from this review, in particular, models of mobile
cloud applications. We also highlight research challenges in the area of mobile
cloud computing. We conclude with recommendations for how this better
understanding of mobile cloud computing can help building more powerful mobile
applications.",
cross-border mobility,http://arxiv.org/abs/1202.2586v8,Gossip-based Information Spreading in Mobile Networks,"Mobile networks receive increasing research interest recently due to their
increasingly wide applications in various areas; mobile ad hoc networks (MANET)
and Vehicular ad hoc networks (VANET) are two prominent examples. Mobility
introduces challenges as well as opportunities: it is known to improve the
network throughput as shown in [1]. In this paper, we analyze the effect of
mobility on the information spreading based on gossip algorithms. Our
contributions are twofold. Firstly, we propose a new performance metric, mobile
conductance, which allows us to separate the details of mobility models from
the study of mobile spreading time. Secondly, we explore the mobile
conductances of several popular mobility models, and offer insights on the
corresponding results. Large scale network simulation is conducted to verify
our analysis.",
cross-border mobility,http://arxiv.org/abs/1401.1907v1,Analyzing an Analytical Solution Model for Simultaneous Mobility,"Current mobility models for simultaneous mobility have their convolution in
designing simultaneous movement where mobile nodes (MNs) travel randomly from
the two adjacent cells at the same time and also have their complexity in the
measurement of the occurrences of simultaneous handover. Simultaneous mobility
problem incurs when two of the MNs start handover approximately at the same
time. As Simultaneous mobility is different for the other mobility pattern,
generally occurs less number of times in real time; we analyze that a
simplified simultaneous mobility model can be considered by taking only
symmetric positions of MNs with random steps. In addition to that, we simulated
the model using mSCTP and compare the simulation results in different scenarios
with customized cell ranges. The analytical results shows that with the bigger
the cell sizes, simultaneous handover with random steps occurrences become lees
and for the sequential mobility (where initial positions of MNs is
predetermined) with random steps, simultaneous handover is more frequent.","International Journal of Wireless & Mobile Networks (IJWMN) Vol.
  5, December 2013, pp 111-124"
cross-border mobility,http://arxiv.org/abs/1210.4644v1,Dimensions and issues of mobile agent technology,"Mobile Agent is a type of software system which acts ""intelligently"" on one's
behalf with the feature of autonomy, learning ability and most importantly
mobility. Now mobile agents are gaining interest in the research community. In
this article mobile agents will be addressed as tools for mobile computing.
Mobile agents have been used in applications ranging from network management to
information management. We present mobile agent concept, characteristics,
classification, need, applications and technical constraints in the mobile
technology. We also provide a brief case study about how mobile agent is used
for information retrieval.","International Journal of Artificial Intelligence & Applications
  (IJAIA), Vol.3, No.5, 2012, 51-61"
cross-border mobility,http://arxiv.org/abs/1307.7502v2,Universal Predictability of Mobility Patterns in Cities,"Despite the long history of modelling human mobility, we continue to lack a
highly accurate approach with low data requirements for predicting mobility
patterns in cities. Here, we present a population-weighted opportunities model
without any adjustable parameters to capture the underlying driving force
accounting for human mobility patterns at the city scale. We use various
mobility data collected from a number of cities with different characteristics
to demonstrate the predictive power of our model. We find that insofar as the
spatial distribution of population is available, our model offers universal
prediction of mobility patterns in good agreement with real observations,
including distance distribution, destination travel constraints and flux. In
contrast, the models that succeed in modelling mobility patterns in countries
are not applicable in cities, which suggests that there is a diversity of human
mobility at different spatial scales. Our model has potential applications in
many fields relevant to mobility behaviour in cities, without relying on
previous mobility measurements.","J R Soc. Interface 11, 0834 (2014)"
cross-border mobility,http://arxiv.org/abs/1307.7563v1,Cooperative Caching Framework for Mobile Cloud Computing,"Due to the advancement in mobile devices and wireless networks mobile cloud
computing, which combines mobile computing and cloud computing has gained
momentum since 2009. The characteristics of mobile devices and wireless network
makes the implementation of mobile cloud computing more complicated than for
fixed clouds. This section lists some of the major issues in Mobile Cloud
Computing. One of the key issues in mobile cloud computing is the end to end
delay in servicing a request. Data caching is o ne of the techniques widely
used in wired and wireless networks to improve data access efficiency. In this
paper we explore the possibility of a cooperative caching approach to enhance
data access efficiency in mobile cloud computing. The proposed approach is
based on cloudlets, one of the architecture designed for mobile cloud
computing.","Global Journal of Computer Science and Technology, Volume 13 Issue
  8 Version 1.0 Year 2013 Network, Web & Security Volume 13 Issue 8 Version 1.0
  Year 2013"
cross-border mobility,http://arxiv.org/abs/1603.03551v1,The three primary colors of mobile systems,"In this paper, we present the notion of ""mobile 3C systems in which the
""Communications"", ""Computing"", and ""Caching"" (i.e., 3C) make up the three
primary resources/funcationalties, akin to the three primary colors, for a
mobile system. We argue that in future mobile networks, the roles of computing
and caching are as intrinsic and essential as communications, and only the
collective usage of these three primary resources can support the sustainable
growth of mobile systems. By defining the 3C resources in their canonical
forms, we reveal the important fact that ""caching"" affects the mobile system
performance by introducing non-causality into the system, whereas ""computing""
achieves capacity gains by performing logical operations across mobile system
entities. Many existing capacity-enhancing techniques such as coded multicast,
collaborative transmissions, and proactive content pushing can be cast into the
native 3C framework for analytical tractability. We further illustrate the
mobile 3C concepts with practical examples, including a system on
broadcast-unicast convergence for massive media content delivery. The mobile 3C
design paradigm opens up new possibilities as well as key research problems
bearing academic and practice significance.",
cross-border mobility,http://arxiv.org/abs/1005.4017v1,"A Route Optimization technique for registered and unregistered CN's in
  NEMO","As the demand of, requesting the Internet without any disturbance by the
mobile users of any network is increasing the IETF started working on Network
Mobility (NEMO). Maintaining the session of all the nodes in mobile network
with its home network and external nodes can be provided by the basic Network
Mobility support protocol. It provides mobility at IP level to complete
networks, allowing a Mobile Network to change its point of attachment to the
Internet, while maintaining the ongoing sessions of the nodes of the network.
The Mobile Router (MR) manages the mobility even though the nodes don't know
the status of mobility. This article discusses few basic concepts and
limitations of NEMO protocol and proposes two ways to optimize the NEMO routing
technique for registered and unregistered Correspondent Nodes (CN) of the
Mobile Network Node (MNN).","Journal of Computing, Volume 2, Issue 5, May 2010"
cross-border mobility,http://arxiv.org/abs/1007.2981v1,A Mediation Framework for Mobile Web Service Provisioning,"Web Services and mobile data services are the newest trends in information
systems engineering in wired and wireless domains, respectively. Web Services
have a broad range of service distributions while mobile phones have large and
expanding user base. To address the confluence of Web Services and pervasive
mobile devices and communication environments, a basic mobile Web Service
provider was developed for smart phones. The performance of this Mobile Host
was also analyzed in detail. Further analysis of the Mobile Host to provide
proper QoS and to check Mobile Host's feasibility in the P2P networks,
identified the necessity of a mediation framework. The paper describes the
research conducted with the Mobile Host, identifies the tasks of the mediation
framework and then discusses the feasible realization details of such a mobile
Web Services mediation framework.",
cross-border mobility,http://arxiv.org/abs/1312.3847v1,Cloud Service-Aware Location Update in Mobile Cloud Computing,"Mobile devices are becoming the primary platforms for many users who always
roam around when accessing the cloud computing services. From this, the cloud
computing is integrated into the mobile environment by introducing a new
paradigm, mobile cloud computing. In the context of mobile computing, the
battery life of mobile device is limited, and it is important to balance the
mobility performance and energy consumption. Fortunately, cloud services
provide both opportunities and challenges for mobility management. Taking the
activities of cloud services accessing into consideration, we propose a
service-aware location update mechanism, which can detect the presence and
location of the mobile device without traditional periodic registration update.
Analytic model and simulation are developed to investigate the new mechanism.
The results demonstrate that the service-aware location update management can
reduce the location update times and handoff signaling, which can efficiently
save power consumption for mobile devices.",
cross-border mobility,http://arxiv.org/abs/1204.1597v1,"An Intelligent Software Workflow Process Design for Location Management
  on Mobile Devices","Advances in the technologies of networking, wireless communication and
trimness of computers lead to the rapid development in mobile communication
infrastructure, and have drastically changed information processing on mobile
devices. Users carrying portable devices can freely move around, while still
connected to the network. This provides flexibility in accessing information
anywhere at any time. For improving more flexibility on mobile device, the new
challenges in designing software systems for mobile networks include location
and mobility management, channel allocation, power saving and security. In this
paper, we are proposing intelligent software tool for software design on mobile
devices to fulfill the new challenges on mobile location and mobility
management. In this study, the proposed Business Process Redesign (BPR) concept
is aims at an extension of the capabilities of an existing, widely used process
modeling tool in industry with 'Intelligent' capabilities to suggest favorable
alternatives to an existing software workflow design for improving
flexibilities on mobile devices.","International Journal of Advanced Computer Science and
  Applications(IJACSA)-2010"
cross-border mobility,http://arxiv.org/abs/1408.2632v1,"A Study of Network Based Mobility Management Schemes, 6LoWPAN Mobility,
  Open Issues and Proposed Solutions","Wireless Sensor Nodes (SNs), the key elements for building Internet of Things
(IOT), have been deployed widely in order to get and transmit information over
the internet. With the introduction of IPv6 over Low Power Wireless Personal
Area Network (6LoWPAN), it is possible to connect these constrained devices to
IPv6 Networks and transmit IPv6 packets. The sensor nodes are being
deployed/installed on many objects and some of them are mobile (moving)
including mobile gadgets, physical objects (living or non-living) etc. These
mobile objects require sufficient Mobility Management Schemes to take care of
data transmission. Host based mobility protocols; MIPv6 and its extensions are
not suitable for these resource constrained devices. In this paper our focus is
to study PMIPv6 based mobility management and different Scenarios based on it
along with sensor devices. Existing research has made many improvements in
terms of HO latency but less attention has paid towards signaling cost and
packet loss particularly in time critical areas. The study provides the
complete survey of network based mobility management schemes, 6LoWPAN mobility,
challenges associated with them and solutions to meet these challenges.",
cross-border mobility,http://arxiv.org/abs/1705.06926v1,"Experimental Study on Low Power Wide Area Networks (LPWAN) for Mobile
  Internet of Things","In the past decade, we have witnessed explosive growth in the number of
low-power embedded and Internet-connected devices, reinforcing the new
paradigm, Internet of Things (IoT). The low power wide area network (LPWAN),
due to its long-range, low-power and low-cost communication capability, is
actively considered by academia and industry as the future wireless
communication standard for IoT. However, despite the increasing popularity of
`mobile IoT', little is known about the suitability of LPWAN for those mobile
IoT applications in which nodes have varying degrees of mobility. To fill this
knowledge gap, in this paper, we conduct an experimental study to evaluate,
analyze, and characterize LPWAN in both indoor and outdoor mobile environments.
Our experimental results indicate that the performance of LPWAN is surprisingly
susceptible to mobility, even to minor human mobility, and the effect of
mobility significantly escalates as the distance to the gateway increases.
These results call for development of new mobility-aware LPWAN protocols to
support mobile IoT.",
cross-border mobility,http://arxiv.org/abs/1906.01418v1,An End-User Development approach for Mobile Web Augmentation,"The trend towards mobile devices usage has put more than ever the Web as a
ubiquitous platform where users perform all kind of tasks. In some cases, users
access the Web with 'native' mobile applications developed for well-known
sites, such as LinkedIn, Facebook, Twitter, etc. These native applications
might offer further (e.g. location-based) functionalities to their users in
comparison with their corresponding Web sites, because they were developed with
mobile features in mind. However, most Web applications have not this native
mobile counterpart and users access them using browsers in the mobile device.
Users might eventually want to add mobile features on these Web sites even
though those features were not supported originally. In this paper we present a
novel approach to allow end users to augment their preferred Web sites with
mobile features. This end-user approach is supported by a framework for mobile
Web augmentation that we describe in the paper. We also present a set of
supporting tools and a validation experiment with end users.","Mobile Information Systems, Hindawi/IOS Press, 2017"
cross-border mobility,http://arxiv.org/abs/1907.05102v1,Block Prefix Mechanism for Flow Mobility in PMIPv6 Based Networks,"The next generation Internet is deemed to be heterogeneous in nature and
mobile devices connected to the Internet are expected to be equipped with
different wireless network interfaces. As seamless mobility is important in
such networks, handover between different network types, called vertical
handover, is an important issue in such networks. While proposing standards
like Mobile IPv6 (MIPv6) and Proxy Mobile IPv6 (PMIPv6) for mobility management
protocols, one important challenge being addressed by IETF work groups and the
research community is flow mobility in multi-homed heterogeneous wireless
networks. In this paper we propose and analyze a block prefix mechanism for
flow mobility in PMIPv6 and conducted extensive analytical and simulation
studies to compare the proposed mechanism with existing prefix based mechanisms
for flow mobility in PMIPv6 reported in terms of important performance metrics
such as handover latency, average hop delay, packet density, signaling cost and
packet loss. Both analytical and simulation results demonstrate that the
proposed mechanism outperforms the existing flow mobility management procedures
using either shared or unique prefixes.",
cross-border mobility,http://arxiv.org/abs/1501.05212v1,QoE-Centric Localized Mobility Management for Future Mobile Networks,"Mobility support in future networks will be predominately based on micro
mobility protocols. Current proposed schemes such as Hierarchical Mobile IPv6
(HMIPv6) and more importantly Proxy Mobile IPv6 (PMIPv6) provide localized
mobility support by electing a node within the network (topologically close to
the Access Routers (AR)) to act as mobility anchor. Such schemes can
significantly improve handover latency, as well as the end-to-end signalling
overhead, but might entail scalability issues, which in some instances, do not
fit adequately with the current explosion of mobile Internet traffic, and the
evolutionary trend towards flat network architectures. The notion of using
Distributed Mobility Management (DMM) allows for decentralization by anchoring
nodes at their AR. The idea is that for sessions with duration less than the
cell residence time efficient mobility can be supported. However, DMM might be
highly suboptimal in instances where nodes perform multiple handovers during
the session lifetime. Hence, one approach cannot be effective for all types of
applications. In this paper a hybrid mobility management solution, integrated
with a new routing scheme, is proposed. The scheme selects the most suitable
mobility approach, centralized or distributed, by taking into account the
flows' requirement, in terms of Quality of Experience (QoE), and new routing
constraints. The results of optimization problem show that the proposed
approach can achieve efficient resource utilisation, ease network congestion,
and lead to a significant improvement in the QoE.",
cross-border mobility,http://arxiv.org/abs/1105.4431v1,"Service Level Agreement for the QoS Guaranteed Mobile IPTV Services over
  Mobile WiMAX Networks","While mobile IPTV services are supported through the mobile WiMAX networks,
there must need some guaranteed bandwidth for the IPTV services especially if
IPTV and non-IPTV services are simultaneously supported by the mobile WiMAX
networks. The quality of an IPTV service definitely depends on the allocated
bandwidth for that channel. However, due to the high quality IPTV services and
to support of huge non-IPTV traffic over mobile WiMAX networks, it is not
possible to guarantee the sufficient amount of the limited mobile WiMAX
bandwidth for the mobile IPTV services every time. A Service Level Agreement
(SLA) between the mobile IPTV service provider and mobile WiMAX network
operator to reserve sufficient bandwidth for the IPTV calls can increase the
satisfaction level of the mobile IPTV users. In this paper, we propose a SLA
negotiation procedure for mobile IPTV users over mobile WiMAX networks. The
Bandwidth Broker controls the allocated bandwidth for IPTV and non-IPTV users.
The proposed dynamically reserved bandwidth for the IPTV services increases the
IPTV user's satisfaction level. The simulation results state that, our proposed
scheme is able to provide better user satisfaction level for the IPTV users.","The Journal of Korea Information and Communications Society
  (KICS), vol.36, no.4, April 2011"
cross-border mobility,http://arxiv.org/abs/1809.03559v1,Deep Learning Towards Mobile Applications,"Recent years have witnessed an explosive growth of mobile devices. Mobile
devices are permeating every aspect of our daily lives. With the increasing
usage of mobile devices and intelligent applications, there is a soaring demand
for mobile applications with machine learning services. Inspired by the
tremendous success achieved by deep learning in many machine learning tasks, it
becomes a natural trend to push deep learning towards mobile applications.
However, there exist many challenges to realize deep learning in mobile
applications, including the contradiction between the miniature nature of
mobile devices and the resource requirement of deep neural networks, the
privacy and security concerns about individuals' data, and so on. To resolve
these challenges, during the past few years, great leaps have been made in this
area. In this paper, we provide an overview of the current challenges and
representative achievements about pushing deep learning on mobile devices from
three aspects: training with mobile data, efficient inference on mobile
devices, and applications of mobile deep learning. The former two aspects cover
the primary tasks of deep learning. Then, we go through our two recent
applications that apply the data collected by mobile devices to inferring mood
disturbance and user identification. Finally, we conclude this paper with the
discussion of the future of this area.",
cross-border mobility,http://arxiv.org/abs/physics/0507158v2,The Economic Mobility in Money Transfer,"In this paper, we investigate the economic mobility in some money transfer
models which have been applied into the research on wealth distribution. We
demonstrate the mobility by recording the time series of agents' ranks and
observing their volatility. We also compare the mobility quantitatively by
employing an index, ""the per capita aggregate change in log-income"", raised by
economists. Like the shape of distribution, the character of mobility is also
decided by the trading rule in these transfer models. It is worth noting that
even though different models have the same type of distribution, their mobility
characters may be quite different.",
cross-border mobility,http://arxiv.org/abs/1107.3671v1,Impact of Mobility On QoS of Mobile WiMax Network With CBR Application,"The issue of mobility is important in wireless network because internet
connectivity can only be effective if it's available during the movement of
node. To enhance mobility, wireless access systems are designed such as IEEE
802.16e to operate on the move without any disruption of services. In this
paper we are analyzing the impact of mobility on the QoS parameters
(Throughput, Average Jitter and Average end to end Delay) of a mobile WiMAX
network (IEEE 802.16e) with CBR application.","International Journal of Advancements in Technology , Vol. 2, No.
  3, July 2011"
cross-border mobility,http://arxiv.org/abs/1203.3920v1,"Stochastic Characteristics and Simulation of the Random Waypoint
  Mobility Model","Simulation results for Mobile Ad-Hoc Networks (MANETs) are fundamentally
governed by the underlying Mobility Model. Thus it is imperative to find
whether events functionally dependent on the mobility model 'converge' to well
defined functions or constants. This shall ensure the long-run consistency
among simulation performed by disparate parties. This paper reviews a work on
the discrete Random Waypoint Mobility Model (RWMM), addressing its long run
stochastic stability. It is proved that each model in the targeted discrete
class of the RWMM satisfies Birkhoff's pointwise ergodic theorem [13], and
hence time averaged functions on the mobility model surely converge. We also
simulate the most common and general version of the RWMM to give insight into
its working.",
cross-border mobility,http://arxiv.org/abs/1310.1720v1,Solvation Effects on Hole Mobility in the Poly G/Poly C Duplex,"Theoretical calculations of solvation contribution to hole energy in a
polynucleotide chain give very low hole mobility values at zero temperature,
\mu < 10^{-3} cm^2/(V s). We calculated hole mobility at physiological
temperature for the Poly G/Poly C DNA duplex, which gave substantially larger
mobility values. Mobility over the temperature range 20-400 K was calculated.
Taking stacking interaction into account substantially increased hole mobility.","Russian Journal of Physical Chemistry A, 86, 832-836 (2012)"
cross-border mobility,http://arxiv.org/abs/1503.05992v1,"Application Security framework for Mobile App Development in Enterprise
  setup","Enterprise Mobility has been increasing the reach over the years. Initially
Mobile devices were adopted as consumer devices. However, the enterprises world
over have rightly taken the leap and started using the ubiquitous technology
for managing its employees as well as to reach out to the customers. While the
Mobile ecosystem has been evolving over the years, the increased exposure of
mobility in Enterprise framework have caused major focus on the security
aspects of it. While a significant focus have been put on network security,
this paper discusses on the approach that can be taken at Mobile application
layer, which would reduce the risk to the enterprises.",
cross-border mobility,http://arxiv.org/abs/1307.4127v1,"Impact of mobility models on clustering based routing protocols in
  mobile WSNs","This paper presents comparison of different hierarchical (position and
non-position based) protocols with respect to different mobility models.
Previous work mainly focuses on static networks or at most a single mobility
model. Using only one mobility model may not predict the behavior of routing
protocol accurately. Simulation results show that mobility has large impact on
the behavior of WSN routing protocols. Also, position based routing protocols
performs better in terms of packet delivery compared to non position based
routing protocols.",
cross-border mobility,http://arxiv.org/abs/1602.04868v1,Deep Feature-based Face Detection on Mobile Devices,"We propose a deep feature-based face detector for mobile devices to detect
user's face acquired by the front facing camera. The proposed method is able to
detect faces in images containing extreme pose and illumination variations as
well as partial faces. The main challenge in developing deep feature-based
algorithms for mobile devices is the constrained nature of the mobile platform
and the non-availability of CUDA enabled GPUs on such devices. Our
implementation takes into account the special nature of the images captured by
the front-facing camera of mobile devices and exploits the GPUs present in
mobile devices without CUDA-based frameorks, to meet these challenges.",
cross-border mobility,http://arxiv.org/abs/1304.7124v1,Security threats in Prepaid Mobile,"Recent communications environment significantly expand the mobile
environment. Prepaid mobile services for 3G networks enables telecommunication
to sign up new users by utilizing the latest in converged billing technologies.
The worldwide mobile communication market is exploding, and 50 percent of
subscribers are expected to use prepaid billing . Prepaid services are driving
mobile communication into emerging markets such as South America, Eastern
Europe, Asia, Africa and Gulf Countries. Prepaid phone service requires a user
to make payment before calling. It is quite common to get prepaid SIM cards on
every major Network. This paper discuss about various prepaid techniques,
challenges and countermeasures in prepaid mobile communication system .",
cross-border mobility,http://arxiv.org/abs/1305.4163v1,Local Messages for Smartphones,"This paper describes a new model for local messaging based on the network
proximity. We present a novelty mobile mashup which combines Wi-Fi proximity
measurements with Cloud Messaging. Our mobile mashup combines passive
monitoring for smart phones and cloud based messaging for mobile operational
systems. Passive monitoring can determine the location of mobile subscribers
(mobile phones, actually) without the active participation of mobile users.
This paper describes how to combine the passive monitoring and notifications.",
cross-border mobility,http://arxiv.org/abs/1403.7691v1,Mobile Conductance in Sparse Networks and Mobility-Connectivity Tradeoff,"In this paper, our recently proposed mobile-conductance based analytical
framework is extended to the sparse settings, thus offering a unified tool for
analyzing information spreading in mobile networks. A penalty factor is
identified for information spreading in sparse networks as compared to the
connected scenario, which is then intuitively interpreted and verified by
simulations. With the analytical results obtained, the mobility-connectivity
tradeoff is quantitatively analyzed to determine how much mobility may be
exploited to make up for network connectivity deficiency.",
cross-border mobility,http://arxiv.org/abs/1606.00462v1,"Analysis of applications suitable for mobile learning of preschool
  children","This article considers the use of mobile learning in Bulgarian education by
young children. The most used mobile operating systems are analyzed. Also some
of the most used existing applications suitable for mobile learning of
preschool children are presented and classified. Keywords: Mobile applications
for preschool children, mobile learning.",
cross-border mobility,http://arxiv.org/abs/1606.05477v1,4G Mobile Communication Systems: Key Technology and Evolution,"With the worldwide third-generation mobile communication system gradually
implemented, the future development of mobile communications has become a hot
topic and evolution of the problem. This paper introduces the fourth generation
mobile communication system and its performance and network structure and OFDM,
software defined radio, smart antennas, IPv6 and other key technologies, and
analyzes the relationship between 4G mobile communication system for mobile
communications and 3G, and the evolution of communication systems do Prospect.",
cross-border mobility,http://arxiv.org/abs/1606.07164v1,"Mobile Converged Networks: Framework, Optimization and Challenges","In this paper, a new framework of mobile converged networks is proposed for
flexible resource optimization over multi-tier wireless heterogeneous networks.
Design principles and advantages of this new framework of mobile converged
networks are discussed. Moreover, mobile converged network models based on
interference coordination and energy efficiency are presented and the
corresponding optimization algorithms are developed. Furthermore, future
challenges of mobile converged networks are identified to promote the study in
modeling and performance analysis of mobile converged networks.","IEEE Wireless Communications, vol. 21, no. 6, pp. 34-40, 2014"
cross-border mobility,http://arxiv.org/abs/1704.08408v1,"Anisotropic carrier mobility of distorted Dirac cones: theory and
  application","We have theoretically investigated the intrinsic carrier mobility in
semimetals with distorted Dirac cones under both longitudinal and transverse
acoustic phonon scattering. An analytic formula for the carrier mobility was
obtained. It shows that tilting significantly reduces the mobility. The theory
was then applied to 8B-Pmmn borophene and borophane (fully hydrogenated
borophene), both of which have tilted Dirac cones. The predicted carrier
mobilities in 8B-Pmmn borophene at room temperature are both higher than that
in graphene. For borophane, despite its superhigh Fermi velocity, the carrier
mobility is lower than that in 8B-Pmmn owing to its smaller elastic constant
under shear strain.",
cross-border mobility,http://arxiv.org/abs/1807.11745v1,Deep Visual Odometry Methods for Mobile Robots,"Technology has made navigation in 3D real time possible and this has made
possible what seemed impossible. This paper explores the aspect of deep visual
odometry methods for mobile robots. Visual odometry has been instrumental in
making this navigation successful. Noticeable challenges in mobile robots
including the inability to attain Simultaneous Localization and Mapping have
been solved by visual odometry through its cameras which are suitable for human
environments. More intuitive, precise and accurate detection have been made
possible by visual odometry in mobile robots. Another challenge in the mobile
robot world is the 3D map reconstruction for exploration. A dense map in mobile
robots can facilitate for localization and more accurate findings.",
cross-border mobility,http://arxiv.org/abs/1902.01942v1,"A Distributed Self-Organization Approach to Minimize the Signaling and
  Delay Caused by Mobility Management Function in Cellular Networks","To manage mobility, RAN nodes in both 4G and 5G are grouped into a hierarchy
of geographical areas. We demonstrate a 4G/5G compliant Network Level Mobility
Management Optimization solution based on User Equipment (UE) Mobility to
minimize signaling (i.e., handover signaling, paging and tracking area updates)
and handover latency by dynamically reconfiguring the association between nodes
in the Radio Access Network (RAN) and nodes (e.g. Mobility Management Entity),
functions (e.g. Access and Mobility Management Function) and Location Regions
(e.g. Tracking Area, Registration Area, Tracking Area List) in the core
network.",
cross-border mobility,http://arxiv.org/abs/1308.4391v1,On Optimal and Fair Service Allocation in Mobile Cloud Computing,"This paper studies the optimal and fair service allocation for a variety of
mobile applications (single or group and collaborative mobile applications) in
mobile cloud computing. We exploit the observation that using tiered clouds,
i.e. clouds at multiple levels (local and public) can increase the performance
and scalability of mobile applications. We proposed a novel framework to model
mobile applications as a location-time workflows (LTW) of tasks; here users
mobility patterns are translated to mobile service usage patterns. We show that
an optimal mapping of LTWs to tiered cloud resources considering multiple QoS
goals such application delay, device power consumption and user cost/price is
an NP-hard problem for both single and group-based applications. We propose an
efficient heuristic algorithm called MuSIC that is able to perform well (73% of
optimal, 30% better than simple strategies), and scale well to a large number
of users while ensuring high mobile application QoS. We evaluate MuSIC and the
2-tier mobile cloud approach via implementation (on real world clouds) and
extensive simulations using rich mobile applications like intensive signal
processing, video streaming and multimedia file sharing applications. Our
experimental and simulation results indicate that MuSIC supports scalable
operation (100+ concurrent users executing complex workflows) while improving
QoS. We observe about 25% lower delays and power (under fixed price
constraints) and about 35% decrease in price (considering fixed delay) in
comparison to only using the public cloud. Our studies also show that MuSIC
performs quite well under different mobility patterns, e.g. random waypoint and
Manhattan models.",
cross-border mobility,http://arxiv.org/abs/1801.02705v2,"Analyzing Mobility-Traffic Correlations in Large WLAN Traces: Flutes vs.
  Cellos","Two major factors affecting mobile network performance are mobility and
traffic patterns. Simulations and analytical-based performance evaluations rely
on models to approximate factors affecting the network. Hence, the
understanding of mobility and traffic is imperative to the effective evaluation
and efficient design of future mobile networks. Current models target either
mobility or traffic, but do not capture their interplay. Many trace-based
mobility models have largely used pre-smartphone datasets (e.g., AP-logs), or
much coarser granularity (e.g., cell-towers) traces. This raises questions
regarding the relevance of existing models, and motivates our study to revisit
this area. In this study, we conduct a multidimensional analysis, to
quantitatively characterize mobility and traffic spatio-temporal patterns, for
laptops and smartphones, leading to a detailed integrated mobility-traffic
analysis. Our study is data-driven, as we collect and mine capacious datasets
(with 30TB, 300k devices) that capture all of these dimensions. The
investigation is performed using our systematic (FLAMeS) framework. Overall,
dozens of mobility and traffic features have been analyzed. The insights and
lessons learnt serve as guidelines and a first step towards future integrated
mobility-traffic models. In addition, our work acts as a stepping-stone towards
a richer, more-realistic suite of mobile test scenarios and benchmarks.",
cross-border mobility,http://arxiv.org/abs/1903.09916v1,Characterizing Location-based Mobile Tracking in Mobile Ad Networks,"Mobile apps nowadays are often packaged with third-party ad libraries to
monetize user data.",
cross-border mobility,http://arxiv.org/abs/1904.09274v1,Deep Learning on Mobile Devices - A Review,"Recent breakthroughs in deep learning and artificial intelligence
technologies have enabled numerous mobile applications. While traditional
computation paradigms rely on mobile sensing and cloud computing, deep learning
implemented on mobile devices provides several advantages. These advantages
include low communication bandwidth, small cloud computing resource cost, quick
response time, and improved data privacy. Research and development of deep
learning on mobile and embedded devices has recently attracted much attention.
This paper provides a timely review of this fast-paced field to give the
researcher, engineer, practitioner, and graduate student a quick grasp on the
recent advancements of deep learning on mobile devices. In this paper, we
discuss hardware architectures for mobile deep learning, including Field
Programmable Gate Arrays, Application Specific Integrated Circuit, and recent
mobile Graphic Processing Units. We present Size, Weight, Area and Power
considerations and their relation to algorithm optimizations, such as
quantization, pruning, compression, and approximations that simplify
computation while retaining performance accuracy. We cover existing systems and
give a state-of-the-industry review of TensorFlow, MXNet, Mobile AI Compute
Engine, and Paddle-mobile deep learning platform. We discuss resources for
mobile deep learning practitioners, including tools, libraries, models, and
performance benchmarks. We present applications of various mobile sensing
modalities to industries, ranging from robotics, healthcare and multi-media,
biometrics to autonomous drive and defense. We address the key deep learning
challenges to overcome, including low quality data, and small
training/adaptation data sets. In addition, the review provides numerous
citations and links to existing code bases implementing various technologies.","SPIE Defense + Commercial Sensing, Invited Paper. April 2019,
  Baltimore, MD"
cross-border mobility,http://arxiv.org/abs/1907.07062v2,"scikit-mobility: a Python library for the analysis, generation and risk
  assessment of mobility data","The last decade has witnessed the emergence of massive mobility data sets,
such as tracks generated by GPS devices, call detail records, and geo-tagged
posts from social media platforms. These data sets have fostered a vast
scientific production on various applications of human mobility analysis,
ranging from computational epidemiology to urban planning and transportation
engineering. A strand of literature addresses data cleaning issues related to
raw spatiotemporal trajectories, while the second line of research focuses on
discovering the statistical ""laws"" that govern human movements. A significant
effort has also been put on designing algorithms to generate synthetic
trajectories able to reproduce, realistically, the laws of human mobility. Last
but not least, a line of research addresses the crucial problem of privacy,
proposing techniques to perform the re-identification of individuals in a
database. Despite the increasing importance of human mobility analysis for many
scientific and industrial domains, a view on state of the art cannot avoid
noticing that there is no statistical software that can support scientists and
practitioners with all the aspects mentioned above of mobility data analysis.
To fill this gap, we propose scikit-mobility, a Python library that has the
ambition of providing an environment to reproduce existing research, analyze
mobility data, and simulate human mobility habits. scikit-mobility is efficient
and easy to use as it extends the well-known standard pandas, a popular Python
library for data analysis. Moreover, scikit-mobility provides the user with
many functionalities, from visualizing trajectories to generating synthetic
data, from analyzing the statistical patterns of trajectories to assessing the
privacy risk related to the analysis of mobility data sets.",
cross-border mobility,http://arxiv.org/abs/cs/0006022v1,"Multicast-based Architecture for IP Mobility: Simulation Analysis and
  Comparison with Basic Mobile IP","With the introduction of a newer generation of wireless devices and
technologies, the need for an efficient architecture for IP mobility is
becoming more apparent. Several architectures have been proposed to support IP
mobility. Most studies, however, show that current architectures, in general,
fall short from satisfying the performance requirements for wireless
applications, mainly audio. Other studies have shown performance improvement by
using multicast to reduce latency and packet loss during handoff. In this
study, we propose a multicast-based architecture to support IP mobility. We
evaluate our approach through simulation, and we compare it to mainstream
approaches for IP mobility, mainly, the Mobile IP protocol. Comparison is
performed according to the required performance criteria, such as smooth
handoff and efficient routing.
  Our simulation results show significant improvement for the proposed
architecture. On average, basic Mobile IP consumes almost twice as much network
bandwidth, and experiences more than twice as much end-to-end and handoff
delays, as does our proposed architecture. Furthermore, we propose an extension
to Mobile IP to support our architecture with minimal modification.",
cross-border mobility,http://arxiv.org/abs/0802.0188v1,Partitioning the Threads of a Mobile System,"In this paper, we show how thread partitioning helps in proving properties of
mobile systems. Thread partitioning consists in gathering the threads of a
mobile system into several classes. The partitioning criterion is left as a
parameter of both the mobility model and the properties we are interested in.
Then, we design a polynomial time abstract interpretation-based static analysis
that counts the number of threads inside each partition class.",
cross-border mobility,http://arxiv.org/abs/1004.4554v2,Highway Mobility and Vehicular Ad-Hoc Networks in NS-3,"The study of vehicular ad-hoc networks (VANETs) requires efficient and
accurate simulation tools. As the mobility of vehicles and driver behavior can
be affected by network messages, these tools must include a vehicle mobility
model integrated with a quality network simulator. We present the first
implementation of a well-known vehicle mobility model to ns-3, the next
generation of the popular ns-2 networking simulator. Vehicle mobility and
network communication are integrated through events. User-created event
handlers can send network messages or alter vehicle mobility each time a
network message is received and each time vehicle mobility is updated by the
model. To aid in creating simulations, we have implemented a straight highway
model that manages vehicle mobility, while allowing for various user
customizations. We show that the results of our implementation of the mobility
model matches that of the model's author and provide an example of using our
implementation in ns-3.",
cross-border mobility,http://arxiv.org/abs/1211.5418v1,A survey on data and transaction management in mobile databases,"The popularity of the Mobile Database is increasing day by day as people need
information even on the move in the fast changing world. This database
technology permits employees using mobile devices to connect to their corporate
networks, hoard the needed data, work in the disconnected mode and reconnect to
the network to synchronize with the corporate database. In this scenario, the
data is being moved closer to the applications in order to improve the
performance and autonomy. This leads to many interesting problems in mobile
database research and Mobile Database has become a fertile land for many
researchers. In this paper a survey is presented on data and Transaction
management in Mobile Databases from the year 2000 onwards. The survey focuses
on the complete study on the various types of Architectures used in Mobile
databases and Mobile Transaction Models. It also addresses the data management
issues namely Replication and Caching strategies and the transaction management
functionalities such as Concurrency Control and Commit protocols,
Synchronization, Query Processing, Recovery and Security. It also provides
Research Directions in Mobile databases.",
cross-border mobility,http://arxiv.org/abs/1401.2542v1,"Performance Study of Mobile TV over Mobile WiMAX Considering Different
  Modulation and Coding Techniques","With the advent of the wide-spread use of smart phones, video streaming over
mobile wireless networks has suddenly taken a huge surge in recent years.
Considering its enormous potential, mobile WiMAX is emerging as a viable
technology for mobile TV which is expected to become of key importance in the
future of mobile indus- try. In this paper, a simulation performance study of
Mobile TV over mobile WiMAX is conducted with different types of adaptive
modulation and coding taking into account key system and environment parameters
which include the variation in the speed of the mobile, path-loss, scheduling
service classes with the fixed type of mod- ulations. Our simulation has been
conducted using OPNET simulation. Simulation results show that dynamic
adaptation of modulation and coding schemes based onchannel conditions can
offer considerably more en- hanced QoS and at the same time reduce the overall
bandwidthof the system.","Int. J. Communications,Network and System Sciences, 2014, 7, 10-21"
cross-border mobility,http://arxiv.org/abs/1401.4844v1,Managing Congestion Control in Mobile AD-HOC Network Using Mobile Agents,"In mobile adhoc networks, congestion occurs with limited resources. The
standard TCP congestion control mechanism is not able to handle the special
properties of a shared wireless channel. TCP congestion control works very well
on the Internet. But mobile adhoc networks exhibit some unique properties that
greatly affect the design of appropriate protocols and protocol stacks in
general, and of congestion control mechanism in particular. As it turned out,
the vastly differing environment in a mobile adhoc network is highly
problematic for standard TCP. Many approaches have been proposed to overcome
these difficulties. Mobile agent based congestion control Technique is proposed
to avoid congestion in adhoc network. When mobile agent travels through the
network, it can select a less-loaded neighbor node as its next hop and update
the routing table according to the node congestion status. With the aid of
mobile agents, the nodes can get the dynamic network topology in time. In this
paper, a mobile agent based congestion control mechanism is presented.",
cross-border mobility,http://arxiv.org/abs/1410.4375v1,The mobile devices and its mobile learning usage analysis,"The usage of mobile devices for mobile learning is becoming increasingly
popular. There is a new brand of students in the universities now-a-days who
are easily connected to technology and innovative mobile devices. We attempt to
do an analysis on a survey done with university students on mobile device usage
for mobile learning purposes. This is to find the learning trends within the
student community so that some of these popular practices could be encouraged
to enhance learning among the student community. Both the quantitative and
qualitative approaches are adopted in the analysis. The results are discussed
and conclusions drawn in the end.",
cross-border mobility,http://arxiv.org/abs/1410.4537v1,"Factors Influencing Quality of Mobile Apps:Role of Mobile App
  Development Life Cycle","In this paper, The mobile application field has been receiving astronomical
attention from the past few years due to the growing number of mobile app
downloads and withal due to the revenues being engendered .With the surge in
the number of apps, the number of lamentable apps/failing apps has withal been
growing.Interesting mobile app statistics are included in this paper which
might avail the developers understand the concerns and merits of mobile
apps.The authors have made an effort to integrate all the crucial factors that
cause apps to fail which include negligence by the developers, technical
issues, inadequate marketing efforts, and high prospects of the
users/consumers.The paper provides suggestions to eschew failure of apps. As
per the various surveys, the number of lamentable/failing apps is growing
enormously, primarily because mobile app developers are not adopting a standard
development life cycle for the development of apps. In this paper, we have
developed a mobile application with the aid of traditional software development
life cycle phases (Requirements, Design, Develop, Test, and, Maintenance) and
we have used UML, M-UML, and mobile application development technologies.",
cross-border mobility,http://arxiv.org/abs/1009.5347v1,"ARMrayan Multimedia Mobile CMS: a Simplified Approach towards
  Content-Oriented Mobile Application Designing","The ARMrayan Multimedia Mobile CMS (Content Management System) is the first
mobile CMS that gives the opportunity to users for creating multimedia J2ME
mobile applications with their desired content, design and logo; simply,
without any need for writing even a line of code. The low-level programming and
compatibility problems of the J2ME, along with UI designing difficulties, makes
it hard for most people -even programmers- to broadcast their content to the
widespread mobile phones used by nearly all people. This system provides
user-friendly, PC-based tools for creating a tree index of pages and inserting
multiple multimedia contents (e.g. sound, video and picture) in each page for
creating a J2ME mobile application. The output is a stand-alone Java mobile
application that has a user interface, shows texts and pictures and plays music
and videos regardless of the type of devices used as long as the devices
support the J2ME platform. Bitmap fonts have also been used thus Middle Eastern
languages can be easily supported on all mobile phone devices. We omitted
programming concepts for users in order to simplify multimedia content-oriented
mobile application designing for use in educational, cultural or marketing
centers. Ordinary operators can now create a variety of multimedia mobile
applications such as tutorials, catalogues, books, and guides in minutes rather
than months. Simplicity and power has been the goal of this CMS. In this paper,
we present the software engineered-designed concepts of the ARMrayan MCMS along
with the implementation challenges faces and solutions adapted.","International Conference on Wireless Communication and Mobile
  Computing (ICWCMC 2010), Proceedings of WASET, vol. 62, pp. 62-67, February
  2010"
cross-border mobility,http://arxiv.org/abs/1303.4347v1,Maximizing the Lifetime of Multi-chain PEGASIS using Sink Mobility,"In this paper, we propose the mobility of a sink in improved energy efficient
PEGASIS-based protocol (IEEPB) to advance the network lifetime of Wireless
Sensor Networks (WSNs). The multi-head chain, multi-chain concept and the sink
mobility affects largely in enhancing the network lifetime of wireless sensors.
Thus, we recommend Mobile sink improved energy-efficient PEGASIS-based routing
protocol (MIEEPB); a multi-chain model having a sink mobility, to achieve
proficient energy utilization of wireless sensors. As the motorized movement of
mobile sink is steered by petrol or current, there is a need to confine this
movement within boundaries and the trajectory of mobile sink should be fixed.
In our technique, the mobile sink moves along its trajectory and stays for a
sojourn time at sojourn location to guarantee complete data collection. We
develop an algorithm for trajectory of mobile sink. We ultimately perform
wide-ranging experiments to assess the performance of the proposed method. The
results reveal that the proposed way out is nearly optimal and also better than
IEEPB in terms of network lifetime.","World Applied Sciences Journal 21 (9): 1283-1289, 2013"
cross-border mobility,http://arxiv.org/abs/1504.07563v1,A New Secure Mobile Cloud Architecture,"The demand and use of mobile phones, PDAs and smart phones are constantly on
the rise as such, manufacturers of these devices are improving the technology
and usability of these devices constantly. Due to the handy shape and size
these devices come in, their processing capabilities and functionalities, they
are preferred by many over the conventional desktop or laptop computers. Mobile
devices are being used today to perform most tasks that a desktop or laptop
computer could be used for. On this premise, mobile devices are also used to
connect to the resources of cloud computing hence, mobile cloud computing
(MCC). The seemingly ubiquitous and pervasive nature of most mobile devices has
made it acceptable and adequate to match the ubiquitous and pervasive nature of
cloud computing. Mobile cloud computing is said to have increased the
challenges known to cloud computing due to the security loop holes that most
mobile devices have.",
cross-border mobility,http://arxiv.org/abs/1508.00299v1,When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective,"Mobile sensing is an emerging technology that utilizes agent-participatory
data for decision making or state estimation, including multimedia
applications. This article investigates the structure of mobile sensing schemes
and introduces crowdsourcing methods for mobile sensing. Inspired by social
network, one can establish trust among participatory agents to leverage the
wisdom of crowds for mobile sensing. A prototype of social network inspired
mobile multimedia and sensing application is presented for illustrative
purpose. Numerical experiments on real-world datasets show improved performance
of mobile sensing via crowdsourcing. Challenges for mobile sensing with respect
to Internet layers are discussed.",
cross-border mobility,http://arxiv.org/abs/1105.1518v1,"Management of Multiple Mobility Protocols and Tools in Dynamically
  Configurable Networks","Solutions for mobility management in wireless networks have been investigated
and proposed in various research projects and standardization bodies. With the
continuing deployment of different access networks, the wider range of
applications tailored for a mobile environment, and a larger diversity of
wireless end systems, it emerged that a single mobility protocol (such as
Mobile IP) is not sufficient to handle the different requirements adequately.
Thus a solution is needed to manage multiple mobility protocols in end systems
and network nodes, to detect and select the required protocols, versions and
optional features, and enable control on running daemons. For this purpose a
mobility toolbox has been developed as part of the EU funded Ambient Networks
project. This paper describes this modular management approach and illustrates
the additional benefits a mobility protocol can gain by using state transfer as
an example.",
cross-border mobility,http://arxiv.org/abs/1605.01848v1,On the Performance of Mobile Visible Light Communications,"We experimentally characterize the performance of mobile VLC and propose
using OCT precoding to combat mobility-induced performance degradation. Results
show that for approximate 300-Mb/s mobile VLC transmission, OCT precoding
outperforms adaptive-loaded DMT and offers significant packet loss rate
reduction.",
cross-border mobility,http://arxiv.org/abs/1402.1296v1,Mnemonical Body Shortcuts: Gestural Interface for Mobile Devices,"Mobile devices' user interfaces are still quite similar to traditional
interfaces offered by desktop computers, but those can be highly problematic
when used in a mobile context. Human gesture recognition in mobile interaction
appears as an important area to provide suitable on-the-move usability. We
present a body space based approach to improve mobile device interaction and
mobile performance, which we named as Mnemonical Body Shortcuts. The human body
is presented as a rich repository of meaningful relations which are always
available to interact with. These body-based gestures allow the user to
naturally interact with mobile devices with no movement limitations.
Preliminary studies using Radio Frequency Identification (RFID) technology were
performed, validating Mnemonical Body Shortcuts as an appropriate new mobile
interaction mechanism. Following those studies, we developed inertial sensing
prototypes using an accelerometer, ending in the construction and user testing
of a gestural interface for mobile devices capable of properly recognizing
Mnemonical Body Shortcuts and also providing suitable user control mechanisms
and audio, visual and haptic feedback.",
cross-border mobility,http://arxiv.org/abs/1402.3985v1,Challenges and issues in 4G Networks Mobility Management,"Wireless broadband technology is now in motion to provide higher data rate,
wider coverage and improved mobility. Towards this the 4G - network is an
integration of various wireless technologies and expected to provide seamless
mobility. Moreover 4G-networks will be entirely packet switched systems based
on IP protocol. One of the research challenges for 4G-Network is the design of
intelligent mobility management techniques that take advantage of IP-based
technologies to achieve global roaming among various access technologies. Hence
Mobile IPv6 is considered to be one of the key technologies for integration of
heterogeneous networks. However the original Mobile IPv6 does not support fast
handover, which is essential function for mobile networks. Number of research
groups working towards this to develop a common protocol to enable seamless
mobility. In this paper we identify and explore the different issues and
challenges related to mobility management in 4G - networks.","International Journal of Computer Trends and Technology (IJCTT)
  volume 4 Issue 5 May 2013"
economical impact of company mobility,http://arxiv.org/abs/1601.00167v1,"Game-Theoretic Model of Incentivizing Privacy-Aware Users to Consent to
  Location Tracking","Nowadays, mobile users have a vast number of applications and services at
their disposal. Each of these might impose some privacy threats on users'
""Personally Identifiable Information"" (PII). Location privacy is a crucial part
of PII, and as such, privacy-aware users wish to maximize it. This privacy can
be, for instance, threatened by a company, which collects users' traces and
shares them with third parties. To maximize their location privacy, users can
decide to get offline so that the company cannot localize their devices. The
longer a user stays connected to a network, the more services he might receive,
but his location privacy decreases. In this paper, we analyze the trade-off
between location privacy, the level of services that a user experiences, and
the profit of the company. To this end, we formulate a Stackelberg Bayesian
game between the User (follower) and the Company (leader). We present
theoretical results characterizing the equilibria of the game. To the best of
our knowledge, our work is the first to model the economically rational
decision-making of the service provider (i.e., the Company) in conjunction with
the rational decision-making of users who wish to protect their location
privacy. To evaluate the performance of our approach, we have used real-data
from a testbed, and we have also shown that the game-theoretic strategy of the
Company outperforms non-strategic methods. Finally, we have considered
different User privacy types, and have determined the service level that
incentivizes the User to stay connected as long as possible.",
economical impact of company mobility,http://arxiv.org/abs/1910.05596v1,Networks of monetary flow at native resolution,"People and companies move money with every financial transaction they make.
We aim to understand how such activity gives rise to large-scale patterns of
monetary flow. In this work, we trace the movement of e-money through the
accounts of a mobile money system using the provider's own transaction records.
The resulting transaction sequences---balance-respecting trajectories---are
data objects that represent observed monetary flows. Common sequential motifs
correspond to known use-cases of mobile money: digital payments, digital
transfers, and money storage. We find that each activity creates a distinct
network structure within the system, and we uncover coordinated gaming of the
mobile money provider's commission schedule. Moreover, we find that e-money
passes through the system in anywhere from minutes to months. This pronounced
heterogeneity, even within the same use-case, can inform the modeling of
turnover in money supply. Our methodology relates economic activity at the
transaction level to large-scale patterns of monetary flow, broadening the
scope of empirical study about the network and temporal structure of the
economy.",
economical impact of company mobility,http://arxiv.org/abs/1401.5743v1,"The Impact of Social Segregation on Human Mobility in Developing and
  Urbanized Regions","This study leverages mobile phone data to analyze human mobility patterns in
developing countries, especially in comparison to more industrialized
countries. Developing regions, such as the Ivory Coast, are marked by a number
of factors that may influence mobility, such as less infrastructural coverage
and maturity, less economic resources and stability, and in some cases, more
cultural and language-based diversity. By comparing mobile phone data collected
from the Ivory Coast to similar data collected in Portugal, we are able to
highlight both qualitative and quantitative differences in mobility patterns -
such as differences in likelihood to travel, as well as in the time required to
travel - that are relevant to consideration on policy, infrastructure, and
economic development. Our study illustrates how cultural and linguistic
diversity in developing regions (such as Ivory Coast) can present challenges to
mobility models that perform well and were conceptualized in less culturally
diverse regions. Finally, we address these challenges by proposing novel
techniques to assess the strength of borders in a regional partitioning scheme
and to quantify the impact of border strength on mobility model accuracy.",
economical impact of company mobility,http://arxiv.org/abs/1503.00823v1,Influence network in Chinese stock market,"In a stock market, the price fluctuations are interactive, that is, one
listed company can influence others. In this paper, we seek to study the
influence relationships among listed companies by constructing a directed
network on the basis of Chinese stock market. This influence network shows
distinct topological properties, particularly, a few large companies that can
lead the tendency of stock market are recognized. Furthermore, by analyzing the
subnetworks of listed companies distributed in several significant economic
sectors, it is found that the influence relationships are totally different
from one economic sector to another, of which three types of connectivity as
well as hub-like listed companies are identified. In addition, the rankings of
listed companies obtained from the centrality metrics of influence network are
compared with that according to the assets, which gives inspiration to uncover
and understand the importance of listed companies in the stock market. These
empirical results are meaningful in providing these topological properties of
Chinese stock market and economic sectors as well as revealing the
interactively influence relationships among listed companies.",J. Stat. Mech. (2015) P03017
economical impact of company mobility,http://arxiv.org/abs/1907.02480v1,"News and the city: understanding online press consumption patterns
  through mobile data","The always increasing mobile connectivity affects every aspect of our daily
lives, including how and when we keep ourselves informed and consult news
media. By studying mobile web data, provided by one of the major Chilean
telecommunication companies, we investigate how different cohorts of the
population of Santiago De Chile consume news media content through their
smartphones. We address the issue of inequalities in the access to information,
trying to understand to what extent socio-demographic factors impact the
preferences and habits of the users.",
economical impact of company mobility,http://arxiv.org/abs/1802.02507v1,Measuring third party tracker power across web and mobile,"Third-party networks collect vast amounts of data about users via web sites
and mobile applications. Consolidations among tracker companies can
significantly increase their individual tracking capabilities, prompting
scrutiny by competition regulators. Traditional measures of market share, based
on revenue or sales, fail to represent the tracking capability of a tracker,
especially if it spans both web and mobile. This paper proposes a new approach
to measure the concentration of tracking capability, based on the reach of a
tracker on popular websites and apps. Our results reveal that tracker
prominence and parent-subsidiary relationships have significant impact on
accurately measuring concentration.",
economical impact of company mobility,http://arxiv.org/abs/1611.00370v1,"Critical success factors for m-commerce in Saudi Arabia's private sector
  -- a multiple case study analysis","Many developing country firms are investing huge money in the sector of
mobile commerce m commerce Simplifying and understanding the factors which can
impact on m commerce success enables the organisations managers to focus their
efforts on the key areas of their m commerce businesses thereby contributing to
the successful implementation of m commerce This study provides a clear
understanding of m commerce in the private sector in the Kingdom of Saudi
Arabia and identifies the critical success factors of implementing m commerce
within the local business environment A case study approach will be used for
five Saudi companies which use mcommerce represented by Alrajhi Bank Souqcom
Saudi Electricity Company Saudi telecom company STC and Saudi Airlines This
study represents a research in progress and interviews based on the literature
to identify the key success factors for these companies in particular and in
Saudi Arabia s private sector in general",
economical impact of company mobility,http://arxiv.org/abs/1708.02798v1,"Impacts of Culture and Socio-Economic Circumstances on Users' Behavior
  and Mobile Broadband Technology Diffusion Trends","The use of Internet and Internet-based services on PCs, Laptops, Net Pads,
Mobile Phones, PDAs etc have not only changed the global economy but also the
way people communicate and their life styles. It also has evolved people from
different origins, cultures, beliefs across the national boundaries. As a
result it has become an absolute necessity to address the cross-cultural issues
of information systems (IS) reflecting the user behaviours and influencing the
way the mobile broadband technology is being accepted as well as the way it is
changing the life styles of different groups of people. This paper reports on
an on-going research effort which studies the impacts of culture and
socio-economic circumstances on users' behavior and mobile broadband technology
diffusion trends.",
economical impact of company mobility,http://arxiv.org/abs/1707.06247v3,On the Economics of Ransomware,"While recognized as a theoretical and practical concept for over 20 years,
only now ransomware has taken centerstage as one of the most prevalent
cybercrimes. Various reports demonstrate the enormous burden placed on
companies, which have to grapple with the ongoing attack waves. At the same
time, our strategic understanding of the threat and the adversarial interaction
between organizations and cybercriminals perpetrating ransomware attacks is
lacking.
  In this paper, we develop, to the best of our knowledge, the first
game-theoretic model of the ransomware ecosystem. Our model captures a
multi-stage scenario involving organizations from different industry sectors
facing a sophisticated ransomware attacker. We place particular emphasis on the
decision of companies to invest in backup technologies as part of a contingency
plan, and the economic incentives to pay a ransom if impacted by an attack. We
further study to which degree comprehensive industry-wide backup investments
can serve as a deterrent for ongoing attacks.",
economical impact of company mobility,http://arxiv.org/abs/1806.03086v1,Mobile Phone Metadata for Development,"Mobile phones are now widely adopted by most of the world population. Each
time a call is made (or an SMS sent), a Call Detail Record (CDR) is generated
by the telecom companies for billing purpose. These metadata provide
information on when, how, from where and with whom we communicate.
Conceptually, they can be described as a geospatial, dynamic, weighted and
directed network. Applications of CDRs for development are numerous. They have
been used to model the spread of infectious diseases, study road traffic,
support electrification planning strategies or map socio-economic level of
population. While massive, CDRs are not statistically representative of the
whole population due to several sources of bias (market, usage, spatial and
temporal resolution). Furthermore, mobile phone metadata are held by telecom
companies. Consequently, their access is not necessarily straightforward and
can seriously hamper any operational application. Finally, a trade-off exists
between privacy and utility when using sensitive data like CDRs. New
initiatives such as Open Algorithm might help to deal with these fundamental
questions by allowing researchers to run algorithms on the data that remain
safely stored behind the firewall of the providers.",
economical impact of company mobility,http://arxiv.org/abs/1602.05874v1,Delving into the Security Issues of Mobile Cloud Computing,"Looking at the last decade, progress in technology has made a huge impact on
our lifestyles. Enhanced use of mobile phones has provided a technological
breakthrough, with the latest smartphones capturing the market. The word
smartphone is enough for everyone to understand the tremendous potential it
brought to the market in terms of economics as well as usability. Not only
this, this ever growing mobile mania has a lot more to offer. The familiarity
of applications like dropbox etc is a clear indication of the popularity of
mobile and cloud computing. But where we get all the benefits from this
computing platform, there are some of the challenges too. However, with the
enhanced facilities and luxuries, some challenges are always accompanied.",
economical impact of company mobility,http://arxiv.org/abs/1001.3495v1,Expert System Models in the Companies' Financial and Accounting Domain,"The present paper is based on studying, analyzing and implementing the expert
systems in the financial and accounting domain of the companies, describing the
use method of the informational systems that can be used in the multi-national
companies, public interest institutions, and medium and small dimension
economical entities, in order to optimize the managerial decisions and render
efficient the financial-accounting functionality. The purpose of this paper is
aimed to identifying the economical exigencies of the entities, based on the
already used accounting instruments and the management software that could
consent the control of the economical processes and patrimonial assets.","Journal of Computing, Vol. 2, Issue 1, January 2010"
economical impact of company mobility,http://arxiv.org/abs/1802.05568v1,"CompetitiveBike: Competitive Prediction of Bike-Sharing Apps Using
  Heterogeneous Crowdsourced Data","In recent years, bike-sharing systems have been deployed in many cities,
which provide an economical lifestyle. With the prevalence of bike-sharing
systems, a lot of companies join the market, leading to increasingly fierce
competition. To be competitive, bike-sharing companies and app developers need
to make strategic decisions for mobile apps development. Therefore, it is
significant to predict and compare the popularity of different bike-sharing
apps. However, existing works mostly focus on predicting the popularity of a
single app, the popularity contest among different apps has not been explored
yet. In this paper, we aim to forecast the popularity contest between Mobike
and Ofo, two most popular bike-sharing apps in China. We develop
CompetitiveBike, a system to predict the popularity contest among bike-sharing
apps. Moreover, we conduct experiments on real-world datasets collected from 11
app stores and Sina Weibo, and the experiments demonstrate the effectiveness of
our approach.",
economical impact of company mobility,http://arxiv.org/abs/1401.7435v1,"Mobile Services and ICT4D, To the Network Economy - Bridging the Digital
  Divide, Ethiopia's Case","This paper presents a development paradigm for Ethiopia, based on appropriate
services and innovative use of mobile communications technologies via
applications tailored for sectors like business, finance, healthcare,
governance, education and infotainment. The experience of other developing
countries like India and Kenya is cited so as to adapt those to the Ethiopian
context. Notable application areas in the aforementioned sectors have been
outlined. The ETC 'next generation network' is taken into consideration, with
an emphasis on mobile service offering by the Telco itself and/or third party
service providers. In addition, enabling technologies like mobile internet,
location-based systems, open interfaces to large telecom networks, specifically
service-oriented architecture (SOA), Parlay/JAIN and the like are discussed.
The paper points out possible endeavors by such stakeholders like: telecom
agencies and network operators; businesses, government and NGOs; entrepreneurs
and innovators; technology companies and professionals; as well as researchers
and academic institutions. ICT4D through mobile services and their role in
bridging the digital divide by building a virtual 'network economy' is
presented.",
economical impact of company mobility,http://arxiv.org/abs/1307.2084v1,Mitigating Epidemics through Mobile Micro-measures,"Epidemics of infectious diseases are among the largest threats to the quality
of life and the economic and social well-being of developing countries. The
arsenal of measures against such epidemics is well-established, but costly and
insufficient to mitigate their impact. In this paper, we argue that mobile
technology adds a powerful weapon to this arsenal, because (a) mobile devices
endow us with the unprecedented ability to measure and model the detailed
behavioral patterns of the affected population, and (b) they enable the
delivery of personalized behavioral recommendations to individuals in real
time. We combine these two ideas and propose several strategies to generate
such recommendations from mobility patterns. The goal of each strategy is a
large reduction in infections, with a small impact on the normal course of
daily life. We evaluate these strategies over the Orange D4D dataset and show
the benefit of mobile micro-measures, even if only a fraction of the population
participates. These preliminary results demonstrate the potential of mobile
technology to complement other measures like vaccination and quarantines
against disease epidemics.",
economical impact of company mobility,http://arxiv.org/abs/1808.02547v1,"The economic value of neighborhoods: Predicting real estate prices from
  the urban environment","Housing costs have a significant impact on individuals, families, businesses,
and governments. Recently, online companies such as Zillow have developed
proprietary systems that provide automated estimates of housing prices without
the immediate need of professional appraisers. Yet, our understanding of what
drives the value of houses is very limited. In this paper, we use multiple
sources of data to entangle the economic contribution of the neighborhood's
characteristics such as walkability and security perception. We also develop
and release a framework able to now-cast housing prices from Open data, without
the need for historical transactions. Experiments involving 70,000 houses in 8
Italian cities highlight that the neighborhood's vitality and walkability seem
to drive more than 20% of the housing value. Moreover, the use of this
information improves the nowcast by 60%. Hence, the use of property's
surroundings' characteristics can be an invaluable resource to appraise the
economic and social value of houses after neighborhood changes and,
potentially, anticipate gentrification.",
economical impact of company mobility,http://arxiv.org/abs/1403.7654v1,"Where Businesses Thrive: Predicting the Impact of the Olympic Games on
  Local Retailers through Location-based Services Data","The Olympic Games are an important sporting event with notable consequences
for the general economic landscape of the host city. Traditional economic
assessments focus on the aggregated impact of the event on the national income,
but fail to provide micro-scale insights on why local businesses will benefit
from the increased activity during the Games. In this paper we provide a novel
approach to modeling the impact of the Olympic Games on local retailers by
analyzing a dataset mined from a large location-based social service,
Foursquare. We hypothesize that the spatial positioning of businesses as well
as the mobility trends of visitors are primary indicators of whether retailers
will rise their popularity during the event. To confirm this we formulate a
retail winners prediction task in the context of which we evaluate a set of
geographic and mobility metrics. We find that the proximity to stadiums, the
diversity of activity in the neighborhood, the nearby area sociability, as well
as the probability of customer flows from and to event places such as stadiums
and parks are all vital factors. Through supervised learning techniques we
demonstrate that the success of businesses hinges on a combination of both
geographic and mobility factors. Our results suggest that location-based social
networks, where crowdsourced information about the dynamic interaction of users
with urban spaces becomes publicly available, present an alternative medium to
assess the economic impact of large scale events in a city.",
economical impact of company mobility,http://arxiv.org/abs/1710.10141v1,"Cultural, Economic and Societal Impacts on Users' Behaviour and Mobile
  Broadband Adoption Trends","The diverse range of Internet enabled devices both mobile and fixed has not
only impacted the global economy but the very fabric of human communications
and lifestyles. The ease of access and lowered cost has enabled hitherto
diametrically opposed people to interact and influence each other globally. The
consequence of which is the dire need to address the way culture affects
interaction with information systems across the world. The many facets of which
encompasses human behaviour, socio-economic and cultural factors including
lifestyles and the way of interaction with the information system. The study
group involved participants from Bangladesh and the United Kingdom to ascertain
the users'behavioural patterns and mobile broadband technology diffusion
trends.","Annals of Emerging Technologies in Computing (AETiC), Volume #1,
  Issue #1, pp-34-44, October 2017,
  http://aetic.theiaer.org/archive/v1n1/p5.html"
economical impact of company mobility,http://arxiv.org/abs/1307.3760v1,"The Impacts of Using Business Information Systems on Operational
  Effectiveness in Hungary","Business expectations regarding the introduction of business information
systems were investigated according to company size categories. The results
clearly showed that according to the majority of the respondents the
information supply for decision-makers improved. In contrast, business
information systems as a means of improving competitiveness were only regarded
by corporations, this aspect was only around the average in the other company
size categories. The respondents evaluated to what extent the usage of business
information system provided assistance for their economic analyses. The
obtained results show that business information systems can be utilized well in
controlling and reporting. There are differences in their judgement by size
categories. Particularly corporations can take advantage of the support of
business information systems mainly in the field of planning, plan-actual
analysis and the exploration of cost reducing possibilities.","International Journal of Emerging Research in Management &
  Technology, ISSN: 2278-9359 (Volume-2, Issue-4), 2013"
economical impact of company mobility,http://arxiv.org/abs/1504.05895v1,"Semantic Enrichment of Mobile Phone Data Records Using Background
  Knowledge","Every day, billions of mobile network events (i.e. CDRs) are generated by
cellular phone operator companies. Latent in this data are inspiring insights
about human actions and behaviors, the discovery of which is important because
context-aware applications and services hold the key to user-driven,
intelligent services, which can enhance our everyday lives such as social and
economic development, urban planning, and health prevention. The major
challenge in this area is that interpreting such a big stream of data requires
a deep understanding of mobile network events' context through available
background knowledge. This article addresses the issues in context awareness
given heterogeneous and uncertain data of mobile network events missing
reliable information on the context of this activity. The contribution of this
research is a model from a combination of logical and statistical reasoning
standpoints for enabling human activity inference in qualitative terms from
open geographical data that aimed at improving the quality of human behaviors
recognition tasks from CDRs. We use open geographical data, Openstreetmap
(OSM), as a proxy for predicting the content of human activity in the area. The
user study performed in Trento shows that predicted human activities (top
level) match the survey data with around 93% overall accuracy. The extensive
validation for predicting a more specific economic type of human activity
performed in Barcelona, by employing credit card transaction data. The analysis
identifies that appropriately normalized data on points of interest (POI) is a
good proxy for predicting human economical activities, with 84% accuracy on
average. So the model is proven to be efficient for predicting the context of
human activity, when its total level could be efficiently observed from cell
phone data records, missing contextual information however.","Knowledge-Based Systems, Volume 143, 2018"
economical impact of company mobility,http://arxiv.org/abs/1508.07292v1,"Mining Open Datasets for Transparency in Taxi Transport in Metropolitan
  Environments","Uber has recently been introducing novel practices in urban taxi transport.
Journey prices can change dynamically in almost real time and also vary
geographically from one area to another in a city, a strategy known as surge
pricing. In this paper, we explore the power of the new generation of open
datasets towards understanding the impact of the new disruption technologies
that emerge in the area of public transport. With our primary goal being a more
transparent economic landscape for urban commuters, we provide a direct price
comparison between Uber and the Yellow Cab company in New York. We discover
that Uber, despite its lower standard pricing rates, effectively charges higher
fares on average, especially during short in length, but frequent in
occurrence, taxi journeys. Building on this insight, we develop a smartphone
application, OpenStreetCab, that offers a personalized consultation to mobile
users on which taxi provider is cheaper for their journey. Almost five months
after its launch, the app has attracted more than three thousand users in a
single city. Their journey queries have provided additional insights on the
potential savings similar technologies can have for urban commuters, with a
highlight being that on average, a user in New York saves 6 U.S. Dollars per
taxi journey if they pick the cheapest taxi provider. We run extensive
experiments to show how Uber's surge pricing is the driving factor of higher
journey prices and therefore higher potential savings for our application's
users. Finally, motivated by the observation that Uber's surge pricing is
occurring more frequently that intuitively expected, we formulate a prediction
task where the aim becomes to predict a geographic area's tendency to surge.
Using exogenous to Uber datasets we show how it is possible to estimate
customer demand within an area, and by extension surge pricing, with high
accuracy.",
economical impact of company mobility,http://arxiv.org/abs/1806.03378v1,"Cultural Investment and Urban Socio-Economic Development: A Geo-Social
  Network Approach","Being able to assess the impact of government-led investment onto
socio-economic indicators in cities has long been an important target of urban
planning. However, due to the lack of large-scale data with a fine
spatio-temporal resolution, there have been limitations in terms of how
planners can track the impact and measure the effectiveness of cultural
investment in small urban areas. Taking advantage of nearly 4 million
transition records for three years in London from a popular location-based
social network service, Foursquare, we study how the socio-economic impact of
government cultural expenditure can be detected and predicted. Our analysis
shows that network indicators such as average clustering coefficient or
centrality can be exploited to estimate the likelihood of local growth in
response to cultural investment. We subsequently integrate these features in
supervised learning models to infer socio-economic deprivation changes for
London's neighbourhoods. This research presents how geo-social and mobile
services can be used as a proxy to track and predict socio-economic deprivation
changes as government financial effort is put in developing urban areas and
thus gives evidence and suggestions for further policy-making and investment
optimisation.","Royal Society open science 2017 4 (9), 170413"
economical impact of company mobility,http://arxiv.org/abs/1702.08349v1,"Big Data for Social Sciences: Measuring patterns of human behavior
  through large-scale mobile phone data","Through seven publications this dissertation shows how anonymized mobile
phone data can contribute to the social good and provide insights into human
behaviour on a large scale. The size of the datasets analysed ranges from 500
million to 300 billion phone records, covering millions of people. The key
contributions are two-fold:
  1. Big Data for Social Good: Through prediction algorithms the results show
how mobile phone data can be useful to predict important socio-economic
indicators, such as income, illiteracy and poverty in developing countries.
Such knowledge can be used to identify where vulnerable groups in society are,
reduce economic shocks and is a critical component for monitoring poverty rates
over time. Further, the dissertation demonstrates how mobile phone data can be
used to better understand human behaviour during large shocks in society,
exemplified by an analysis of data from the terror attack in Norway and a
natural disaster on the south-coast in Bangladesh. This work leads to an
increased understanding of how information spreads, and how millions of people
move around. The intention is to identify displaced people faster, cheaper and
more accurately than existing survey-based methods.
  2. Big Data for efficient marketing: Finally, the dissertation offers an
insight into how anonymised mobile phone data can be used to map out large
social networks, covering millions of people, to understand how products spread
inside these networks. Results show that by including social patterns and
machine learning techniques in a large-scale marketing experiment in Asia, the
adoption rate is increased by 13 times compared to the approach used by
experienced marketers. A data-driven and scientific approach to marketing,
through more tailored campaigns, contributes to less irrelevant offers for the
customers, and better cost efficiency for the companies.",
economical impact of company mobility,http://arxiv.org/abs/1401.6102v1,e-commerce business models in the context of web3.0 paradigm,"Web 3.0 promises to have a significant effect in users and businesses. It
will change how people work and play, how companies use information to market
and sell their products, as well as operate their businesses. The basic shift
occurring in Web 3.0 is from information-centric to knowledge-centric patterns
of computing. Web 3.0 will enable people and machines to connect, evolve, share
and use knowledge on an unprecedented scale and in new ways that make our
experience of the Internet better. Additionally, semantic technologies have the
potential to drive significant improvements in capabilities and life cycle
economics through cost reductions, improved efficiencies, enhanced
effectiveness, and new functionalities that were not possible or economically
feasible before. In this paper we look to the semantic web and Web 3.0
technologies as enablers for the creation of value and appearance of new
business models. For that, we analyze the role and impact of Web 3.0 in
business and we identify nine potential business models, based in direct and
undirected revenue sources, which have emerged with the appearance of semantic
web technologies.",
economical impact of company mobility,http://arxiv.org/abs/1806.09063v1,"How LinkedIn Economic Graph Bonds Information and Product: Applications
  in LinkedIn Salary","The LinkedIn Salary product was launched in late 2016 with the goal of
providing insights on compensation distribution to job seekers, so that they
can make more informed decisions when discovering and assessing career
opportunities. The compensation insights are provided based on data collected
from LinkedIn members and aggregated in a privacy-preserving manner. Given the
simultaneous desire for computing robust, reliable insights and for having
insights to satisfy as many job seekers as possible, a key challenge is to
reliably infer the insights at the company level when there is limited or no
data at all. We propose a two-step framework that utilizes a novel, semantic
representation of companies (Company2vec) and a Bayesian statistical model to
address this problem. Our approach makes use of the rich information present in
the LinkedIn Economic Graph, and in particular, uses the intuition that two
companies are likely to be similar if employees are very likely to transition
from one company to the other and vice versa. We compute embeddings for
companies by analyzing the LinkedIn members' company transition data using
machine learning algorithms, then compute pairwise similarities between
companies based on these embeddings, and finally incorporate company
similarities in the form of peer company groups as part of the proposed
Bayesian statistical model to predict insights at the company level. We perform
extensive validation using several different evaluation techniques, and show
that we can significantly increase the coverage of insights while, in fact,
even improving the quality of the obtained insights. For example, we were able
to compute salary insights for 35 times as many title-region-company
combinations in the U.S. as compared to previous work, corresponding to 4.9
times as many monthly active users. Finally, we highlight the lessons learned
from deployment of our system.",
economical impact of company mobility,http://arxiv.org/abs/1702.02722v1,Mobile Data Trading: Behavioral Economics Analysis and Algorithm Design,"Motivated by the recently launched mobile data trading markets (e.g., China
Mobile Hong Kong's 2nd exChange Market), in this paper we study the mobile data
trading problem under the future data demand uncertainty. We introduce a
brokerage-based market, where sellers and buyers propose their selling and
buying quantities, respectively, to the trading platform that matches the
market supply and demand. To understand the users' realistic trading behaviors,
a prospect theory (PT) model from behavioral economics is proposed, which
includes the widely adopted expected utility theory (EUT) as a special case.
Although the PT modeling leads to a challenging non-convex optimization
problem, the optimal solution can be characterized by exploiting the unimodal
structure of the objective function. Building upon our analysis, we design an
algorithm to help estimate the user's risk preference and provide trading
recommendations dynamically, considering the latest market and usage
information. It is shown in our simulation that the risk preferences have a
significant impact on the user's decision and outcome: a risk-averse dominant
user can guarantee a higher minimum profit in the trading, while a risk-seeking
dominant user can achieve a higher maximum profit. By comparing with the EUT
benchmark, it is shown that a PT user with a low reference point is more
willing to buy mobile data. Moreover, when the probability of high future data
demand is low, a PT user is more willing to buy mobile data due to the
probability distortion comparing with an EUT user.",
economical impact of company mobility,http://arxiv.org/abs/1309.4496v1,"Evaluating socio-economic state of a country analyzing airtime credit
  and mobile phone datasets","Reliable statistical information is important to make political decisions on
a sound basis and to help measure the impact of policies. Unfortunately,
statistics offices in developing countries have scarce resources and
statistical censuses are therefore conducted sporadically. Based on mobile
phone communications and history of airtime credit purchases, we estimate the
relative income of individuals, the diversity and inequality of income, and an
indicator for socioeconomic segregation for fine-grained regions of an African
country. Our study shows how to use mobile phone datasets as a starting point
to understand the socio-economic state of a country, which can be especially
useful in countries with few resources to conduct large surveys.",
economical impact of company mobility,http://arxiv.org/abs/1911.02056v1,Response Prediction for Low-Regret Agents,"Companies like Google and Microsoft run billions of auctions every day to
sell advertising opportunities. Any change to the rules of these auctions can
have a tremendous effect on the revenue of the company and the welfare of the
advertisers and the users. Therefore, any change requires careful evaluation of
its potential impacts. Currently, such impacts are often evaluated by running
simulations or small controlled experiments. This, however, misses the
important factor that the advertisers respond to changes. Our goal is to build
a theoretical framework for predicting the actions of an agent (the advertiser)
that is optimizing her actions in an uncertain environment. We model this
problem using a variant of the multi-armed bandit setting where playing an arm
is costly. The cost of each arm changes over time and is publicly observable.
The value of playing an arm is drawn stochastically from a static distribution
and is observed by the agent and not by us. We, however, observe the actions of
the agent. Our main result is that assuming the agent is playing a strategy
with a regret of at most $f(T)$ within the first $T$ rounds, we can learn to
play the multi-armed bandits game (without observing the rewards) in such a way
that the regret of our selected actions is at most $O(k^4(f(T)+1)\log(T))$,
where $k$ is the number of arms.","The 15th Conference on Web and Internet Economics, 2019"
economical impact of company mobility,http://arxiv.org/abs/1810.08861v1,"Empirically Assessing Opportunities for Prefetching and Caching in
  Mobile Apps","Network latency in mobile software has a large impact on user experience,
with potentially severe economic consequences. Prefetching and caching have
been shown effective in reducing the latencies in browser-based systems.
However, those techniques cannot be directly applied to the emerging domain of
mobile apps because of the differences in network interactions. Moreover, there
is a lack of research on prefetching and caching techniques that may be
suitable for the mobile app domain, and it is not clear whether such techniques
can be effective or whether they are even feasible. This paper takes the first
step toward answering these questions by conducting a comprehensive study to
understand the characteristics of HTTP requests in over 1000 popular Android
apps. Our work focuses on the prefetchability of requests using static program
analysis techniques and cacheability of resulting responses. We find that there
is a substantial opportunity to leverage prefetching and caching in mobile
apps, but that suitable techniques must take into account the nature of apps'
network interactions and idiosyncrasies such as untrustworthy HTTP header
information. Our observations provide guidelines for developers to utilize
prefetching and caching schemes in app development, and motivate future
research in this area.",
economical impact of company mobility,http://arxiv.org/abs/1308.0795v1,The Economic and Sustainability Future of Cellular Networks,"Global data traffic is expected to grow exponentially in the next few years
with video and smartphone applications driving data growth. Many mobile network
providers in the UK have either deployed or planning to deploy 4th generation
Long-Term-Evolution (LTE) mobile technology as the solution to meet capacity
demands. This study evaluates the technological improvements in 4G LTE in
comparison to 3G High Speed Packet Access (HSPA) and further conducts a
techno-economic analysis using primary researched tariff data to determine
network operator profitability and mobile tariff strategy to meet user demand.
To ensure holistic analysis, the study also considers the environmental impacts
of LTE by determining the annual carbon emission for a network operator. The
study results shows LTE will prove profitable; however a trade-off has to be
made by network operators between meeting consumer tariff demands or increasing
profitability. Analysis also shows a 63% reduced in carbon emissions is
possible with migration to 4G services with implication of further financial
benefits for network operators as a result.",
economical impact of company mobility,http://arxiv.org/abs/1301.7257v1,"Design, Implementation, and Operation of a Mobile Honeypot","Mobile nodes, in particular smartphones are one of the most relevant devices
in the current Internet in terms of quantity and economic impact. There is the
common believe that those devices are of special interest for attackers due to
their limited resources and the serious data they store. On the other hand, the
mobile regime is a very lively network environment, which misses the (limited)
ground truth we have in commonly connected Internet nodes. In this paper we
argue for a simple long-term measurement infrastructure that allows for (1) the
analysis of unsolicited traffic to and from mobile devices and (2) fair
comparison with wired Internet access. We introduce the design and
implementation of a mobile honeypot, which is deployed on standard hardware for
more than 1.5 years. Two independent groups developed the same concept for the
system. We also present preliminary measurement results.",
economical impact of company mobility,http://arxiv.org/abs/1807.03350v1,"Detecting Socio-Economic Impact of Cultural Investment Through
  Geo-Social Network Analysis","Taking advantage of nearly 4 million transition records for three years in
London from a popular location-based social network service, Foursquare, we
study how to track the impact and measure the effectiveness of cultural
investment in small urban areas. We reveal the underlying relationships between
socio-economic status, local cultural expenditure, and network features
extracted from user mobility trajectories. This research presents how
geo-social and mobile services more generally can be used as a proxy to track
local changes as government financial effort is put in developing urban areas,
and thus gives evidence and suggestions for further policy-making and
investment optimization.",
economical impact of company mobility,http://arxiv.org/abs/1506.03027v1,"Disclosing the network structure of private companies on the web: the
  case of Spanish IBEX 35 share index","It is common for an international company to have different brands, products
or services, information for investors, a corporate blog, affiliates, branches
in different countries, etc. If all these contents appear as independent
additional web domains (AWD), the company should be represented on the web by
all these web domains, since many of these AWDs may acquire remarkable
performance that could mask or distort the real web performance of the company,
affecting therefore on the understanding of web metrics. The main objective of
this study is to determine the amount, type, web impact and topology of the
additional web domains in commercial companies in order to get a better
understanding on their complete web impact and structure. The set of companies
belonging to the Spanish IBEX-35 stock index has been analyzed as testing
bench. We proceeded to identify and categorize all AWDs belonging to these
companies, and to apply both web impact (web presence and visibility) and
network metrics. The results show that AWDs get a high web presence but
relatively low web visibility, due to certain opacity or less dissemination of
some AWDs, favoring its isolation. This is verified by the low network density
values obtained, that occur because AWDs are strongly connected with the
corporate domain (although asymmetrically), but very weakly linked each other.
Although the processes of AWDs creation and categorization are complex (web
policy seems not to be driven by a defined or conscious plan), their influence
on the web performance of IBEX 35companies is meaningful. This research
measures the AWDs influence on companies under webometric terms for the first
time.",
economical impact of company mobility,http://arxiv.org/abs/1312.2383v1,"On the Performance of Filters for Reduction of Speckle Noise in SAR
  Images off the Coast of the Gulf of Guinea","Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methods
that have been proposed for the West African sub-region. With the increase in
the number of oil exploration companies in Ghana (and her neighbors) and the
rise in the coastal activities in the sub-region, there is the need for proper
monitoring of the environmental impact of these socio-economic activities on
the environment. Detection and near real-time information about oil spills are
fundamental in reducing oil spill environmental impact. SAR images are prone to
some noise, which is predominantly speckle noise around the coastal areas. This
paper evaluates the performance of the mean and median filters used in the
preprocessing filtering to reduce speckle noise in SAR images for most image
processing algorithms.",
economical impact of company mobility,http://arxiv.org/abs/0803.0405v1,Multi-dimensional sparse time series: feature extraction,"We show an analysis of multi-dimensional time series via entropy and
statistical linguistic techniques. We define three markers encoding the
behavior of the series, after it has been translated into a multi-dimensional
symbolic sequence. The leading component and the trend of the series with
respect to a mobile window analysis result from the entropy analysis and label
the dynamical evolution of the series. The diversification formalizes the
differentiation in the use of recurrent patterns, from a Zipf law point of
view. These markers are the starting point of further analysis such as
classification or clustering of large database of multi-dimensional time
series, prediction of future behavior and attribution of new data. We also
present an application to economic data. We deal with measurements of money
investments of some business companies in advertising market for different
media sources.",
economical impact of company mobility,http://arxiv.org/abs/0901.4767v1,Quantum Theory of Economics,"In the given work the first attempt to generalize quantum uncertainty
relation on macro objects is made. Business company as one of economical
process participants was chosen by the authors for this purpose. The analogies
between quantum micro objects and the structures which from the first sight do
not have anything in common with physics are given. The proof of generalized
uncertainty relation is produced. With the help of generalized uncertainty
relation the authors wanted to elaborate a new non-traditional approach to the
description of companies' business activity and their developing and try to
formulate some advice for them. Thus, our work makes the base of quantum theory
of econimics",
economical impact of company mobility,http://arxiv.org/abs/1107.0539v1,Corporate competition: A self-organized network,"A substantial number of studies have extended the work on universal
properties in physical systems to complex networks in social, biological, and
technological systems. In this paper, we present a complex networks perspective
on interfirm organizational networks by mapping, analyzing and modeling the
spatial structure of a large interfirm competition network across a variety of
sectors and industries within the United States. We propose two micro-dynamic
models that are able to reproduce empirically observed characteristics of
competition networks as a natural outcome of a minimal set of general
mechanisms governing the formation of competition networks. Both models, which
utilize different approaches yet apply common principles to network formation
give comparable results. There is an asymmetry between companies that are
considered competitors, and companies that consider others as their
competitors. All companies only consider a small number of other companies as
competitors; however, there are a few companies that are considered as
competitors by many others. Geographically, the density of corporate
headquarters strongly correlates with local population density, and the
probability two firms are competitors declines with geographic distance. We
construct these properties by growing a corporate network with competitive
links using random incorporations modulated by population density and
geographic distance. Our new analysis, methodology and empirical results are
relevant to various phenomena of social and market behavior, and have
implications to research fields such as economic geography, economic sociology,
and regional economic development.","Social Networks 33, 3 (July 2011) pp. 219--30"
economical impact of company mobility,http://arxiv.org/abs/1508.01053v1,"A Review of Technical Problems when Conducting an Investigation in Cloud
  Based Environments","Cloud computing is a relatively new technology which is quickly becoming one
of the most important technological advances for computer science. This
technology has had a significant growth in recent years. It is now more
affordable and cloud platforms are becoming more stable. Businesses are
successfully migrating their systems to a cloud infrastructure, obtaining
technological and economic benefits. However, others still remain reluctant to
do it due to both security concerns and the loss of control over their
infrastructures and data that the migration entails. At the same time that new
technologies progress, its benefits appeal to criminals too. They can not only
steal data from clouds, but they can also hide data in clouds, which has
provoked an increased in the number of cybercrimes and their economic impacts.
Their victims range from children and adults to companies and even countries.
On the other hand, digital forensics have negatively suffered the impact of the
boom of cloud computing due to its dynamic nature. The tools and procedures
that were successfully proved and used in digital investigations are now
becoming irrelevant, making it an urging necessity to develop new forensics
capabilities for conducting an investigation in this new environment. As a
consequence of these needs a new area has emerged, Cloud Forensics, which is
the result of the intersection between cloud computing and digital forensics.
  Keywords: Cloud forensics, cloud computing, forensics investigation, forensic
challenges.",
economical impact of company mobility,http://arxiv.org/abs/physics/0512155v1,Effects of Economic Interactions on Credit Risk,"We study a credit risk model which captures effects of economic interactions
on a firm's default probability. Economic interactions are represented as a
functionally defined graph, and the existence of both cooperative, and
competitive, business relations is taken into account. We provide an analytic
solution of the model in a limit where the number of business relations of each
company is large, but the overall fraction of the economy with which a given
company interacts may be small. While the effects of economic interactions are
relatively weak in typical (most probable) scenarios, they are pronounced in
situations of economic stress, and thus lead to a substantial fattening of the
tails of loss distributions in large loan portfolios. This manifests itself in
a pronounced enhancement of the Value at Risk computed for interacting
economies in comparison with their non-interacting counterparts.",
economical impact of company mobility,http://arxiv.org/abs/1810.02815v1,"A General Sensitivity Analysis Approach for Demand Response
  Optimizations","It is well-known that demand response can improve the system efficiency as
well as lower consumers' (prosumers') electricity bills. However, it is not
clear how we can either qualitatively identify the prosumer with the most
impact potential or quantitatively estimate each prosumer's contribution to the
total social welfare improvement when additional resource capacity/flexibility
is introduced to the system with demand response, such as allowing net-selling
behavior. In this work, we build upon existing literature on the electricity
market, which consists of price-taking prosumers each with various appliances,
an electric utility company and a social welfare optimizing distribution system
operator, to design a general sensitivity analysis approach (GSAA) that can
estimate the potential of each consumer's contribution to the social welfare
when given more resource capacity. GSAA is based on existence of an efficient
competitive equilibrium, which we establish in the paper. When prosumers'
utility functions are quadratic, GSAA can give closed forms characterization on
social welfare improvement based on duality analysis. Furthermore, we extend
GSAA to a general convex settings, i.e., utility functions with strong
convexity and Lipschitz continuous gradient. Even without knowing the specific
forms the utility functions, we can derive upper and lower bounds of the social
welfare improvement potential of each prosumer, when extra resource is
introduced. For both settings, several applications and numerical examples are
provided: including extending AC comfort zone, ability of EV to discharge and
net selling. The estimation results show that GSAA can be used to decide how to
allocate potentially limited market resources in the most impactful way.",
economical impact of company mobility,http://arxiv.org/abs/1911.00436v1,Predicting Urban Innovation from the Workforce Mobility Network in US,"While great emphasis has been placed on the role of social interactions as
driver of innovation growth, very few empirical studies have explicitly
investigated the impact of social network structures on the innovation
performance of cities. Past research has mostly explored scaling laws of
socio-economic outputs of cities as determined by, for example, the single
predictor of population. Here, by drawing on a publicly available dataset of
the startup ecosystem, we build the first Workforce Mobility Network among US
metropolitan areas. We found that node centrality computed on this network
accounts for most of the variability observed in cities' innovation performance
and significantly outperforms other predictors such as population size or
density, suggesting that policies and initiatives aiming at sustaining
innovation processes might benefit from fostering professional networks
alongside other economic or systemic incentives. As opposed to previous
approaches powered by census data, our model can be updated in real-time upon
open databases, opening up new opportunities both for researchers in a variety
of disciplines to study urban economies in new ways, and for practitioners to
design tools for monitoring such economies in real-time.",
economical impact of company mobility,http://arxiv.org/abs/physics/0507158v2,The Economic Mobility in Money Transfer,"In this paper, we investigate the economic mobility in some money transfer
models which have been applied into the research on wealth distribution. We
demonstrate the mobility by recording the time series of agents' ranks and
observing their volatility. We also compare the mobility quantitatively by
employing an index, ""the per capita aggregate change in log-income"", raised by
economists. Like the shape of distribution, the character of mobility is also
decided by the trading rule in these transfer models. It is worth noting that
even though different models have the same type of distribution, their mobility
characters may be quite different.",
economical impact of company mobility,http://arxiv.org/abs/1507.00248v1,The Network Picture of Labor Flow,"We construct a data-driven model of flows in graphs that captures the
essential elements of the movement of workers between jobs in the companies
(firms) of entire economic systems such as countries. The model is based on the
observation that certain job transitions between firms are often repeated over
time, showing persistent behavior, and suggesting the construction of static
graphs to act as the scaffolding for job mobility. Individuals in the job
market (the workforce) are modelled by a discrete-time random walk on graphs,
where each individual at a node can possess two states: employed or unemployed,
and the rates of becoming unemployed and of finding a new job are node
dependent parameters. We calculate the steady state solution of the model and
compare it to extensive micro-datasets for Mexico and Finland, comprised of
hundreds of thousands of firms and individuals. We find that our model
possesses the correct behavior for the numbers of employed and unemployed
individuals in these countries down to the level of individual firms. Our
framework opens the door to a new approach to the analysis of labor mobility at
high resolution, with the tantalizing potential for the development of full
forecasting methods in the future.",
economical impact of company mobility,http://arxiv.org/abs/cs/0607144v1,Levels of Product Differentiation in the Global Mobile Phones Market,"The sixth product level called compliant product is a connecting element
between the physical product characteristics and the strategy of the producer
company. The article discusses the differentiation among the product offers of
companies working in the global markets, as well as the strategies which they
use and could use in that respect.",
economical impact of company mobility,http://arxiv.org/abs/1807.06850v2,"Does the performance of TDD hold across software companies and premises?
  A group of industrial experiments on TDD","Test-Driven Development (TDD) has been claimed to increase external software
quality. However, the extent to which TDD increases external quality has been
seldom studied in industrial experiments. We conduct four industrial
experiments in two different companies to evaluate the performance of TDD on
external quality. We study whether the performance of TDD holds across premises
within the same company and across companies. We identify participant-level
characteristics impacting results. Iterative-Test Last (ITL), the reverse
approach of TDD, outperforms TDD in three out of four premises. ITL outperforms
TDD in both companies. The larger the experience with unit testing and testing
tools, the larger the difference in performance between ITL and TDD (in favour
of ITL). Technological environment (i.e., programming language and testing
tool) seems not to impact results. Evaluating participant-level characteristics
impacting results in industrial experiments may ease the understanding of the
performance of TDD in realistic settings.",
economical impact of company mobility,http://arxiv.org/abs/1305.4632v3,"The Process of Mobile Spectrum Allocation and its impact on Electronic
  Commerce and Mobile Commerce","Spectrum being a very scarce natural resource of a country has to be
judicially used for the purpose of nation building and the allocation process
to telecom operators should be very transparent and ethical. There are various
ways of how spectrum can be allocated and there is no best way that can be
adopted universally. The market situation, Government policies, competition etc
determine the price of the spectrum and this is purely a regulatory or a
government decision to sell spectrum to telecom companies. The different
allocation methods, their implications with case studies across the globe is
analysed and presented in this paper. The reason why spectrum allocation should
be fair and transparent and the cost should be reasonable is analysed and
described.",
economical impact of company mobility,http://arxiv.org/abs/1803.09823v1,"The Impact of the Object-Oriented Software Evolution on Software
  Metrics: The Iris Approach","The Object-Oriented (OO) software system evolves over the time to meet the
new requirements. Based on the initial release of software, the continuous
modification of software code leads to software evolution. Software needs to
evolve over the time to meet the new user's requirements. Software companies
often develop variant software of the original one depends on customers' needs.
The main hypothesis of this paper states that the software when it evolves over
the time, its code continues to grow, change and become more complex. This
paper proposes an automatic approach (Iris) to examine the proposed hypothesis.
Originality of this approach is the exploiting of the software variants to
study the impact of software evolution on the software metrics. This paper
presents the results of experiments conducted on three releases of drawing
shapes software, sixteen releases of rhino software, eight releases of mobile
media software and ten releases of ArgoUML software. Based on the extracted
software metrics, It has been found that Iris hypothesis is supported by the
computed metrics.","Indian Journal of Science and Technology, 11(8), 1-8, 2018"
economical impact of company mobility,http://arxiv.org/abs/1606.06279v1,An analytical framework to nowcast well-being using mobile phone data,"An intriguing open question is whether measurements made on Big Data
recording human activities can yield us high-fidelity proxies of socio-economic
development and well-being. Can we monitor and predict the socio-economic
development of a territory just by observing the behavior of its inhabitants
through the lens of Big Data? In this paper, we design a data-driven analytical
framework that uses mobility measures and social measures extracted from mobile
phone data to estimate indicators for socio-economic development and
well-being. We discover that the diversity of mobility, defined in terms of
entropy of the individual users' trajectories, exhibits (i) significant
correlation with two different socio-economic indicators and (ii) the highest
importance in predictive models built to predict the socio-economic indicators.
Our analytical framework opens an interesting perspective to study human
behavior through the lens of Big Data by means of new statistical indicators
that quantify and possibly ""nowcast"" the well-being and the socio-economic
development of a territory.",
economical impact of company mobility,http://arxiv.org/abs/1803.03058v1,"An exploratory study on how Internet of Things developing companies
  handle User Experience Requirements","[Context and motivation] Internet of Things (IoT) is becoming common
throughout everyday lives. However, the interaction is often different from
when using e.g. computers and other smart devices. Furthermore, an IoT device
is often dependent on several other systems, heavily impacting the user
experience (UX). Finally, the domain is changing rapidly and is driven by
technological innovation.
  [Question/problem] In this qualitative study, we explore how companies elicit
UX requirements in the context of IoT. A key part of contemporary IoT
development is also data-driven approaches. Thus, these are also considered in
the study.
  [Principal idea / Results] There is a knowledge gap around data-driven
methodologies, there are examples of companies that collect large amount of
data but do not always know how to utilize it. Furthermore, many of the
companies struggle to handle the larger system context, where their products
and the UX they control are only one part of the complete IoT ecosystem.
  [Contribution] We provide qualitative empirical data from IoT developing
companies. Based on our findings, we identify challenges for the companies and
areas for future work.","24th International Working Conference, Requirements Engineering:
  Foundation for Software Quality 2018, Utrecht, The Netherlands, March 19-22,
  2018"
economical impact of company mobility,http://arxiv.org/abs/1301.4781v1,Ontology-based Recommender System of Economic Articles,"Decision makers need economical information to drive their decisions. The
Company Actualis SARL is specialized in the production and distribution of a
press review about French regional economic actors. This economic review
represents for a client a prospecting tool on partners and competitors. To
reduce the overload of useless information, the company is moving towards a
customized review for each customer. Three issues appear to achieve this goal.
First, how to identify the elements in the text in order to extract objects
that match with the recommendation's criteria presented? Second, How to define
the structure of these objects, relationships and articles in order to provide
a source of knowledge usable by the extraction process to produce new knowledge
from articles? The latter issue is the feedback on customer experience to
identify the quality of distributed information in real-time and to improve the
relevance of the recommendations. This paper presents a new type of
recommendation based on the semantic description of both articles and user
profile.","8th International Conference on Web Information Systems and
  Technologies, Porto : Portugal (2013)"
economical impact of company mobility,http://arxiv.org/abs/1704.04301v1,"A Tree-based Approach for Detecting Redundant Business Rules in very
  Large Financial Datasets","Net Asset Value (NAV) calculation and validation is the principle task of a
fund administrator. If the NAV of a fund is calculated incorrectly then there
is huge impact on the fund administrator; such as monetary compensation,
reputational loss, or loss of business. In general, these companies use the
same methodology to calculate the NAV of a fund, however the type of fund in
question dictates the set of business rules used to validate this. Today, most
Fund Administrators depend heavily on human resources due to the lack of an
automated standardized solutions, however due to economic climate and the need
for efficiency and costs reduction many banks are now looking for an automated
solution with minimal human interaction; i.e., straight through processing
(STP). Within the scope of a collaboration project that focuses on building an
optimal solution for NAV validation, in this paper, we will present a new
approach for detecting correlated business rules. We also show how we evaluate
this approach using real-world financial data.",
economical impact of company mobility,http://arxiv.org/abs/1309.5725v1,"Comparing the impact of mobile nodes arrival patterns in mobile ad hoc
  networks using poisson and pareto models","Mobile Ad hoc Networks (MANETs) are dynamic networks populated by mobile
stations, or mobile nodes (MNs). Mobility model is a hot topic in many areas,
for example, protocol evaluation, network performance analysis and so on.How to
simulate MNs mobility is the problem we should consider if we want to build an
accurate mobility model. When new nodes can join and other nodes can leave the
network and therefore the topology is dynamic.Specifically, Mobile Ad hoc
Networks consist of a collection of nodes randomly placed in a line (not
necessarily straight). Mobile Ad hoc Networks do appear in many real-world
network applications such as a vehicular Mobile Ad hoc Networks built along a
highway in a city environment or people in a particular location. Mobile Nodes
in Mobile Ad hoc Networks are usually laptops, Personal Digital Assistants or
mobile phones. This paper presents comparative results that have been carried
out via Matrix lab software simulation. The study investigates the impact of
mobility predictive models on mobile nodes parameters such as, the arrival rate
and the size of mobile nodes in a given area using Pareto and Poisson
distributions. The results have indicated that mobile nodes arrival rates may
have influence on Mobile Nodes population (as a larger number) in a location.
The Pareto distribution is more reflective of the modeling mobility for Mobile
Ad hoc Networks than the Poisson distribution.","International Journal of Wireless & Mobile Networks (IJWMN) Vol.
  5, No. 4, August 2013 International Journal of Wireless & Mobile Networks
  (IJWMN) Vol. 5, No. 4, August 2013"
economical impact of company mobility,http://arxiv.org/abs/1804.10712v1,"A comprehensive study of Game Theory applications for smart grids,
  demand side management programs, and transportation networks","Game theory is a powerful analytical tool for modeling decision makers
strategies, behaviors and interactions. Act and decisions of a decision maker
can benefit or negatively impact other decision makers interests. Game theory
has been broadly used in economics, politics and engineering field. For
example, game theory can model decision making procedure of different companies
competing with each other to maximize their profit. Here, we present a brief
introduction of game theory formulation and its applications. The focus of the
chapter is non-cooperative Stackelberg game model and its applications in
solving power system related problems. These applications include but not
limited to; expanding transmission network, improving power system reliability,
containing market power in the electricity market, solving power system
dispatch, executing demand response and allocating resource in a wireless
system. Finally, this chapter elaborates on solving a game theory problem
through an example.",
economical impact of company mobility,http://arxiv.org/abs/1102.4853v1,Energy and environmental aspects of mobile communication systems,"The reduction of the energy consumptions of a Telecommunication Power System
represents one of the critical factors of the telecommunication technologies,
both to allow a sizeable saving of economic resources and to realize
""sustainable"" development actions. The consumption of about one hundred base
stations for mobile phones were monitored for a total of over one thousand
days, in order to study the energy consumption in relation to the
environmental, electric and logistics parameters of the stations themselves. It
was possible to survey, then, the role of the mobile communication systems in
the general national energy framework and to plot the best areas of
intervention for saving energy and improving the environmental impact, showing
the role played by air conditioning and transmission equipments. Finally, new
transmission algorithms and the use of renewable energy based techniques have
been tested.","Energy, Volume 36, Issue 2, February 2011, Pages 1109-1114"
economical impact of company mobility,http://arxiv.org/abs/1003.4078v1,"A Group Vehicular Mobility Model for Routing Protocol Analysis in Mobile
  Ad Hoc Network","Performance of routing protocols in mobile ad-hoc networks is greatly
affected by the dynamic nature of nodes, route failures, wireless channels with
variable bandwidth and scalability issues. A mobility model imitates the real
world movement of mobile nodes and is central component to simulation based
studies. In this paper we consider mobility nodes which mimic the vehicular
motion of nodes like Manhattan mobility model and City Section mobility model.
We also propose a new Group Vehicular mobility model that takes the best
features of group mobility models like Reference Point Group mobility model and
applies it to vehicular models. We analyze the performance of our model known
as Group Vehicular mobility model (GVMM) and other vehicular mobility models
with various metrics. This analysis provides us with an insight about the
impact of mobility models on the performance of routing protocols for ad-hoc
networks. The routing protocols are simulated and measured for performance and
finally we arrive at the correlation about the impact of mobility models on
routing protocols, which are central to the design of mobile adhoc networks.","Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/"
economical impact of company mobility,http://arxiv.org/abs/1811.12015v1,"Mobile phone indicators and their relation to the socioeconomic
  organisation of cities","Thanks to the use of geolocated big data in computational social science
research, the spatial and temporal heterogeneity of human activities are
increasingly being revealed. Paired with smaller and more traditional data,
this opens new ways of understanding how people act and move, and how these
movements crystallise into the structural patterns observed by censuses. In
this article we explore the convergence of mobile phone data with more
classical socioeconomic data from census in French cities. We extract mobile
phone indicators from six months worth of Call Detail Records (CDR) data, while
census and administrative data are used to characterize the socioeconomic
organisation of French cities. We address various definitions of cities and
investigate how they impact the relation between mobile phone indicators, such
as the number of calls or the entropy of visited cell towers, and measures of
economic organisation based on census data, such as the level of deprivation,
inequality and segregation. Our findings show that some mobile phone indicators
relate significantly with different socioeconomic organisation of cities.
However, we show that found relations are sensitive to the way cities are
defined and delineated. In several cases, differing city definitions
delineations can change the significance or even the signs of found
correlations. In general, cities delineated in a restricted way (central cores
only) exhibit traces of human activity which are less related to their
socioeconomic organisation than cities delineated as metropolitan areas and
dispersed urban regions.","ISPRS International Journal of Geo-Information 2019, 8(1), 19"
economical impact of company mobility,http://arxiv.org/abs/1907.04122v1,"Uncovering the role of spatial constraints in the differences and
  similarities between physical and virtual mobility","The recent availability of digital traces from Information and Communications
Technologies (ICT) has facilitated the study of both individual- and
population-level movement with unprecedented spatiotemporal resolution,
enabling us to better understand a plethora of socioeconomic processes such as
urbanization, transportation, impact on the environment and epidemic spreading
to name a few. Using empirical spatiotemporal trends, several mobility models
have been proposed to explain the observed regularities in human movement. With
the advent of the World Wide Web, a new type of virtual mobility has emerged
that has begun to supplant many traditional facets of human activity. Here we
conduct a systematic analysis of physical and virtual movement, uncovering both
similarities and differences in their statistical patterns. The differences
manifest themselves primarily in the temporal regime, as a signature of the
spatial and economic constraints inherent in physical movement, features that
are predominantly absent in the virtual space. We demonstrate that once one
moves to the time-independent space of events, i.e the sequences of visited
locations, these differences vanish, and the statistical patterns of physical
and virtual mobility are identical. The observed similarity in navigating these
markedly different domains point towards a common mechanism governing the
movement patterns, a feature we describe through a Metropolis-Hastings type
optimization model, where individuals navigate locations through
decision-making processes resembling a cost-benefit analysis of the utility of
locations. In contrast to existing phenomenological models of mobility, we show
that our model can reproduce the commonalities in the empirically observed
statistics with minimal input.",
economical impact of company mobility,http://arxiv.org/abs/1901.10581v2,"A Review on Energy, Environmental, and Sustainability Implications of
  Connected and Automated Vehicles","Connected and automated vehicles (CAVs) are poised to reshape transportation
and mobility by replacing humans as the driver and service provider. While the
primary stated motivation for vehicle automation is to improve safety and
convenience of road mobility, this transformation also provides a valuable
opportunity to improve vehicle energy efficiency and reduce emissions in the
transportation sector. Progress in vehicle efficiency and functionality,
however, does not necessarily translate to net positive environmental outcomes.
Here we examine the interactions between CAV technology and the environment at
four levels of increasing complexity: vehicle, transportation system, urban
system, and society. We find that environmental impacts come from
CAV-facilitated transformations at all four levels, rather than from CAV
technology directly. We anticipate net positive environmental impacts at the
vehicle, transportation system, and urban system levels, but expect greater
vehicle utilization and shifts in travel patterns at the society level to
offset some of these benefits. Focusing on the vehicle-level improvements
associated with CAV technology is likely to yield excessively optimistic
estimates of environmental benefits. Future research and policy efforts should
strive to clarify the extent and possible synergetic effects from a systems
level in order to envisage and address concerns regarding the short- and
long-term sustainable adoption of CAV technology.","Environmental Science & Technology, 2018, 52(20), 11449-11465"
economical impact of company mobility,http://arxiv.org/abs/1806.11342v1,"The Economics of Video Websites with Membership-Advertising Mode in
  Wireless Networks","In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",
Delaware effect,http://arxiv.org/abs/physics/0202027v1,"Apparent suppression of turbulent magnetic dynamo action by a dc
  magnetic field","Numerical studies of the effect of a dc magnetic field on dynamo action
(development of magnetic fields with large spatial scales), due to
helically-driven magnetohydrodynamic turbulence, are reported. The apparent
effect of the dc magnetic field is to suppress the dynamo action, above a
relatively low threshold. However, the possibility that the suppression results
from an improper combination of rectangular triply spatially-periodic boundary
conditions and a uniform dc magnetic field is addressed: heretofore a common
and convenient computational convention in turbulence investigations. Physical
reasons for the observed suppression are suggested. Other geometries and
boundary conditions are offered for which the dynamo action is expected not to
be suppressed by the presence of a dc magnetic field component.",
Delaware effect,http://arxiv.org/abs/1803.09647v1,Ion Diffusion and Acceleration in Plasma Turbulence,"Particle transport, acceleration and energisation are phenomena of major
importance for both space and laboratory plasmas. Despite years of study, an
accurate theoretical description of these effects is still lacking. Validating
models with self-consistent, kinetic simulations represents today a new
challenge for the description of weakly-collisional, turbulent plasmas. We
perform two-dimensional (2D) hybrid-PIC simulations of steady-state turbulence
to study the processes of diffusion and acceleration. The chosen plasma
parameters allow to span different systems, going from the solar corona to the
solar wind, from the Earth's magnetosheath to confinement devices. To describe
the ion diffusion, we adapted the Nonlinear Guiding Center (NLGC) theory to the
2D case. Finally, we investigated the local influence of coherent structures on
particle energisation and acceleration: current sheets play an important role
if the ions Larmor radii are on the order of the current sheets size. This
resonance-like process leads to the violation of the magnetic moment
conservation, eventually enhancing the velocity-space diffusion.",
Delaware effect,http://arxiv.org/abs/1803.04870v3,"Narcissus: Deriving Correct-By-Construction Decoders and Encoders from
  Binary Formats","It is a neat result from functional programming that libraries of parser
combinators can support rapid construction of decoders for quite a range of
formats. With a little more work, the same combinator program can denote both a
decoder and an encoder. Unfortunately, the real world is full of gnarly
formats, as with the packet formats that make up the standard Internet protocol
stack. Most past parser-combinator approaches cannot handle these formats, and
the few exceptions require redundancy -- one part of the natural grammar needs
to be hand-translated into hints in multiple parts of a parser program. We show
how to recover very natural and nonredundant format specifications, covering
all popular network packet formats and generating both decoders and encoders
automatically. The catch is that we use the Coq proof assistant to derive both
kinds of artifacts using tactics, automatically, in a way that guarantees that
they form inverses of each other. We used our approach to reimplement packet
processing for a full Internet protocol stack, inserting our replacement into
the OCaml-based MirageOS unikernel, resulting in minimal performance
degradation.",
cross-border merger GDP,http://arxiv.org/abs/physics/0607139v2,"Clusters or networks of economies? A macroeconomy study through GDP
  fluctuation correlations","We follow up on the study of correlations between GDP's of rich countries. We
analyze web-downloaded data on GDP that we use as individual wealth signatures
of the country economical state. We calculate the yearly fluctuations of the
GDP. We look for forward and backward correlations between such fluctuations.
The system is represented by an evolving network, nodes being the GDP
fluctuations (or countries) at different times.
  In order to extract structures from the network, we focus on filtering the
time delayed correlations by removing the least correlated links. This
percolation idea-based method reveals the emergence of connections, that are
visualized by a branching representation. Note that the network is made of
weighted and directed links when taking into account a delay time. Such a
measure of collective habits does not fit the usual expectations defined by
politicians or economists.",Physica A 382 (2007) 16-21
cross-border merger GDP,http://arxiv.org/abs/1609.00876v1,Statistical Dynamics of Regional Populations and Economies,"A practical statistical analysis on the regional populations and GDPs of
China is conducted. The result shows that the distribution of the populations
and that of the GDPs obeys the shifted power law, respectively. To understand
these characteristics, a generalized Langevin equation describing variation of
population is proposed based on the correlation between population and GDP as
well as the random fluctuations of the related factors. The equation is
transformed into the Fokker-Plank equation, and the solution demonstrates a
transform of population distribution from the normal Gaussian distribution to a
shifted power law. It also suggests a critical point of time at which the
transform occurs. The shifted power law distribution in the supercritical
situation is qualitatively in accordance with the practical result. The
distribution of the GDPs is derived based on the Cobb-Douglas production
function, and presents a change from a shifted power law to the Gaussian
distribution. This result indicates that the regional GDP distribution of our
society will be the Gaussian distribution in the future. The analysis on the
growth trend of economy suggests it will become a reality. These theoretical
attempts may draw a historical picture of our world in the aspects of
population and economy.",
cross-border merger GDP,http://arxiv.org/abs/physics/0701030v1,Interplay between topology and dynamics in the World Trade Web,"We present an empirical analysis of the network formed by the trade
relationships between all world countries, or World Trade Web (WTW). Each
(directed) link is weighted by the amount of wealth flowing between two
countries, and each country is characterized by the value of its Gross Domestic
Product (GDP). By analysing a set of year-by-year data covering the time
interval 1950-2000, we show that the dynamics of all GDP values and the
evolution of the WTW (trade flow and topology) are tightly coupled. The
probability that two countries are connected depends on their GDP values,
supporting recent theoretical models relating network topology to the presence
of a `hidden' variable (or fitness). On the other hand, the topology is shown
to determine the GDP values due to the exchange between countries. This leads
us to a new framework where the fitness value is a dynamical variable
determining, and at the same time depending on, network topology in a
continuous feedback.","Eur. Phys. J. B 57,159-164 (2007)"
cross-border merger GDP,http://arxiv.org/abs/1308.5572v1,"Insight into the Properties of the UK Power Consumption Using a Linear
  Regression and Wavelet Transform Approach","In this paper, the relationship between the Gross Domestic Product (GDP), air
temperature variations and power consumption is evaluated using the linear
regression and Wavelet Coherence (WTC) approach on a 1971-2011 time series for
the United Kingdom (UK). The results based on the linear regression approach
indicate that some 66% variability of the UK electricity demand can be
explained by the quarterly GDP variations, while only 11% of the quarterly
changes of the UK electricity demand are caused by seasonal air temperature
variations. WTC however, can detect the period of time when GDP and air
temperature significantly correlate with electricity demand and the results of
the wavelet correlation at different time scales indicate that a significant
correlation is to be found on a long-term basis for GDP and on an annual basis
for seasonal air-temperature variations. This approach provides an insight into
the properties of the impact of the main factors on power consumption on the
basis of which the power system development or operation planning and
forecasting the power consumption can be improved.","Elektrotehniski vestnik/Electrotechnical review 79(5), 278-283,
  2012"
cross-border merger GDP,http://arxiv.org/abs/physics/0607098v1,"Cluster structure of EU-15 countries derived from the correlation matrix
  analysis of macroeconomic index fluctuations","The statistical distances between countries, calculated for various moving
average time windows, are mapped into the ultrametric subdominant space as in
classical Minimal Spanning Tree methods. The Moving Average Minimal Length Path
(MAMLP) algorithm allows a decoupling of fluctuations with respect to the mass
center of the system from the movement of the mass center itself. A Hamiltonian
representation given by a factor graph is used and plays the role of cost
function. The present analysis pertains to 11 macroeconomic (ME) indicators,
namely the GDP (x1), Final Consumption Expenditure (x2), Gross Capital
Formation (x3), Net Exports (x4), Consumer Price Index (y1), Rates of Interest
of the Central Banks (y2), Labour Force (z1), Unemployment (z2), GDP/hour
worked (z3), GDP/capita (w1) and Gini coefficient (w2). The target group of
countries is composed of 15 EU countries, data taken between 1995 and 2004. By
two different methods (the Bipartite Factor Graph Analysis and the Correlation
Matrix Eigensystem Analysis) it is found that the strongly correlated countries
with respect to the macroeconomic indicators fluctuations can be partitioned
into stable clusters.",Eur. Phys. J B 57 (2007) 139-146
cross-border merger GDP,http://arxiv.org/abs/physics/0607180v1,"How Do Output Growth Rate Distributions Look Like? Some Time-Series
  Evidence on OECD Countries","This paper investigates the statistical properties of within-country GDP and
industrial production (IP) growth rate distributions. Many empirical
contributions have recently pointed out that cross-section growth rates of
firms, industries and countries all follow Laplace distributions. In this work,
we test whether also within-country, time-series GDP and IP growth rates can be
approximated by tent-shaped distributions. We fit output growth rates with the
exponential-power (Subbotin) family of densities, which includes as particular
cases both the Gaussian and the Laplace distributions. We find that, for a
large number of OECD countries including the U.S., both GDP and IP growth rates
are Laplace distributed. Moreover, we show that fat-tailed distributions
robustly emerge even after controlling for outliers, autocorrelation and
heteroscedasticity.",
cross-border merger GDP,http://arxiv.org/abs/0802.4170v1,"Cluster Expansion Method for Evolving Weighted Networks Having
  Vector-like Nodes","The Cluster Variation Method known in statistical mechanics and condensed
matter is revived for weighted bipartite networks. The decomposition of a
Hamiltonian through a finite number of components, whence serving to define
variable clusters, is recalled. As an illustration the network built from data
representing correlations between (4) macro-economic features, i.e. the so
called $vector$ $components$, of 15 EU countries, as (function) nodes, is
discussed. We show that statistical physics principles, like the maximum
entropy criterion points to clusters, here in a (4) variable phase space: Gross
Domestic Product (GDP), Final Consumption Expenditure (FCE), Gross Capital
Formation (GCF) and Net Exports (NEX). It is observed that the $maximum$
entropy corresponds to a cluster which does $not$ explicitly include the GDP
but only the other (3) ''axes'', i.e. consumption, investment and trade
components. On the other hand, the $minimal$ entropy clustering scheme is
obtained from a coupling necessarily including GDP and FCE. The results confirm
intuitive economic theory and practice expectations at least as regards
geographical connexions. The technique can of course be applied to many other
cases in the physics of socio-economy networks.",Acta Phys. Pol. A 114 (2008) 491-499
cross-border merger GDP,http://arxiv.org/abs/1006.5269v1,Allometric Scaling of Countries,"As huge complex systems consisting of geographic regions, natural resources,
people and economic entities, countries follow the allometric scaling law which
is ubiquitous in ecological, urban systems. We systematically investigated the
allometric scaling relationships between a large number of macroscopic
properties and geographic (area), demographic (population) and economic (GDP,
gross domestic production) sizes of countries respectively. We found that most
of the economic, trade, energy consumption, communication related properties
have significant super-linear (the exponent is larger than 1) or nearly linear
allometric scaling relations with GDP. Meanwhile, the geographic (arable area,
natural resources, etc.), demographic(labor force, military age population,
etc.) and transportation-related properties (road length, airports) have
significant and sub-linear (the exponent is smaller than 1) allometric scaling
relations with area. Several differences of power law relations with respect to
population between countries and cities were pointed out. Firstly, population
increases sub-linearly with area in countries. Secondly, GDP increases linearly
in countries but not super-linearly as in cities. Finally, electricity or oil
consumptions per capita increases with population faster than cities.","Physica A: Statistical Mechanics and its Applications. Volume 389,
  Issue 21, 1 November 2010, Pages 4887-4896"
cross-border merger GDP,http://arxiv.org/abs/1603.02734v1,"Codebook Design for Millimeter-Wave Channel Estimation with Hybrid
  Precoding Structure","In this paper, we study hierarchical codebook design for channel estimation
in millimeter-wave (mmWave) communications with a hybrid precoding structure.
Due to the limited saturation power of mmWave power amplifier (PA), we take the
per-antenna power constraint (PAPC) into consideration. We first propose a
metric, i.e., generalized detection probability (GDP), to evaluate the quality
of \emph{an arbitrary codeword}. This metric not only enables an optimization
approach for mmWave codebook design, but also can be used to compare the
performance of two different codewords/codebooks. To the best of our knowledge,
GDP is the first metric particularly for mmWave codebook design for channel
estimation. We then propose an approach to design a hierarchical codebook
exploiting BeaM Widening with Multi-RF-chain Sub-array technique (BMW-MS). To
obtain crucial parameters of BMW-MS, we provide two solutions, namely a
low-complexity search (LCS) solution to optimize the GDP metric and a
closed-form (CF) solution to pursue a flat beam pattern. Performance
comparisons show that BMW-MS/LCS and BMW-MS/CF achieve very close performances,
and they outperform the existing alternatives under the PAPC.",
cross-border merger GDP,http://arxiv.org/abs/1710.07382v1,Partially Coherent Ptychography by Gradient Decomposition of the Probe,"Coherent ptychographic imaging experiments often discard over 99.9 % of the
flux from a light source to define the coherence of an illumination. Even when
coherent flux is sufficient, the stability required during an exposure is
another important limiting factor. Partial coherence analysis can considerably
reduce these limitations. A partially coherent illumination can often be
written as the superposition of a single coherent illumination convolved with a
separable translational kernel. In this paper we propose the Gradient
Decomposition of the Probe (GDP), a model that exploits translational kernel
separability, coupling the variances of the kernel with the transverse
coherence. We describe an efficient first-order splitting algorithm GDP-ADMM to
solve the proposed nonlinear optimization problem. Numerical experiments
demonstrate the effectiveness of the proposed method with Gaussian and binary
kernel functions in fly-scan measurements. Remarkably, GDP-ADMM produces
satisfactory results even when the ratio between kernel width and beam size is
more than one, or when the distance between successive acquisitions is twice as
large as the beam width.","Acta Crystallographica Section A: Foundations and Advances 74 (3),
  157-169 (2018)"
cross-border merger GDP,http://arxiv.org/abs/0909.4786v1,"Worldwide Use and Impact of the NASA Astrophysics Data System Digital
  Library","By combining data from the text, citation, and reference databases with data
from the ADS readership logs we have been able to create Second Order
Bibliometric Operators, a customizable class of collaborative filters which
permits substantially improved accuracy in literature queries.
  Using the ADS usage logs along with membership statistics from the
International Astronomical Union and data on the population and gross domestic
product (GDP) we develop an accurate model for world-wide basic research where
the number of scientists in a country is proportional to the GDP of that
country, and the amount of basic research done by a country is proportional to
the number of scientists in that country times that country's per capita GDP.
  We introduce the concept of utility time to measure the impact of the
ADS/URANIA and the electronic astronomical library on astronomical research. We
find that in 2002 it amounted to the equivalent of 736 FTE researchers, or $250
Million, or the astronomical research done in France.
  Subject headings: digital libraries; bibliometrics; sociology of science;
information retrieval","The Journal of the American Society for Information Science and
  Technology, Vol. 56, p. 36. (2005)"
cross-border merger GDP,http://arxiv.org/abs/1505.05321v1,Regional Development Classification Model using Decision Tree Approach,"Regional development classification is one way to look at differences in
levels of development outcomes. Some frequently used methods are the shift
share, Gain index, the Iindex Williamson and Klassen typology. The development
of science in the field of data mining, offers a new way for regional
development data classification. This study discusses how the decision tree is
used to classify the level of development based on indicators of regional gross
domestic product (GDP). GDP Data Central Java and Banten used in this study.
Before the data is entered into the decision tree forming algorithm, both the
provincial GDP data are classified using Klassen typology. Three decision tree
algorithms, namely J48, NBTRee and REPTree tested in this study using
cross-validation evaluation, then selected one of the best performing
algorithms. The results show that the J48 has a better accuracy rate which is
equal to 85.18% compared to the algorithm NBTRee and REPTree. Testing the model
is done to the six districts / municipalities in the province of Banten, and
shows that there are two districts / cities are still at the development of the
status quadrant relatively underdeveloped regions, namely Kota Tangerang and
Kabupaten Tangerang. As for the Central Java Province, Kendal, Magelang,
Pemalang, Rembang, Semarang and Wonosobo are an area with a quadrant of
development also on the status of the region is relatively underdeveloped.
Classification model that has been developed is able to classify the level of
development fast and easy to enter data directly into the decision tree is
formed. This study can be used as an alternative decision support for policy
makers in order to determine the future direction of development.","International Journal of Computer Applications Volume 114, No. 8,
  March 2015"
cross-border merger GDP,http://arxiv.org/abs/1608.00275v1,"Metastable Features of Economic Networks and Responses to Exogenous
  Shocks","It has been proved that network structure plays an important role in
addressing a collective behaviour. In this paper we consider a network of firms
and corporations and study its metastable features in an Ising based model. In
our model, we observe that if in a recession the government imposes a demand
shock to stimulate the network, metastable features shape its response.
Actually we find that there is a minimum bound where demand shocks with a size
below it are unable to trigger the market out from recession. We then
investigate the impact of network characteristics on this minimum bound. We
surprisingly observe that in a Watts-Strogatz network though the minimum bound
depends on the average of the degrees, when translated into the economics
language, such a bound is independent of the average degrees. This bound is
about $0.44 \Delta$GDP, where $\Delta$GDP is the gap of GDP between recession
and expansion. We examine our suggestions for the cases of the United States
and the European Union in the recent recession, and compare them with the
imposed stimulations. While stimulation in the US has been above our threshold,
in the EU it has been far below our threshold. Beside providing a minimum bound
for a successful stimulation, our study on the metastable features suggests
that in the time of crisis there is a ""golden time passage"" in which the
minimum bound for successful stimulation can be much lower. So, our study
strongly suggests stimulations to be started within this time passage.","PloS one 11 (10), e0160363 (2016)"
cross-border merger GDP,http://arxiv.org/abs/1902.09872v1,Economic geography and the scaling of urban and regional income in India,"We undertake an exploration of the economic income (Gross Domestic Product,
GDP) of Indian districts and cities based on scaling analyses of the dependence
of these quantities on associated population size. Scaling analysis provides a
straightforward method for the identification of network effects in
socioeconomic organization, which are the tell-tale of cities and urbanization.
For districts, a sub-state regional administrative division in India, we find
almost linear scaling of GDP with population, a result quite different from
urban functional units in other national contexts. Using deviations from
scaling, we explore the behavior of these regional units to find strong
distinct geographic patterns of economic behavior. We characterize these
patterns in detail and connect them to the literature on regional economic
development for a diverse subcontinental nation such as India. Given the
paucity of economic data for Urban Agglomerations in India, we use a set of
assumptions to create a new dataset of GDP based on districts, for large
cities. This reveals superlinear scaling of income with city size, as expected
from theory, while displaying similar underlying patterns of economic geography
observed for district economic performance. This analysis of the economic
performance of Indian cities is severely limited by the absence of
higher-fidelity, direct city level economic data. We discuss the need for
standardized and consistent estimates of the size and change in urban economies
in India, and point to a number of proxies that can be explored to develop such
indicators.",
cross-border merger GDP,http://arxiv.org/abs/1711.10883v1,"An Analytical Framework for Understanding the Intensity of Religious
  Fundamentalism","This paper examines the process of emergence of religious fundamentalism
through development parameters. Therefore this research work reflects an
analytical discussion on how the level of religious fundamentalism can be
explained by the economic, political administrative and legal parameters such
as GDP, Employment to Population ratio, Government Effectiveness, Voice &
Accountability, Rule of Law (World Justice Project Report) and Rule of law
(Governance Indicators).",
cross-border merger GDP,http://arxiv.org/abs/physics/0101078v1,A Unifying Hypothesis for the Conformational Change of Tubulin,"Microtubule dynamic instability arises from the hydrolysis of GTP bound to
the beta-monomer of the tubulin dimer. The conformational change induced by
hydrolysis is unknown, but microtubules disassemble into protofilaments of
GDP-bound tubulin that curve away from the microtubule axis. This paper
presents the unfolding of a portion of the tubulin molecule into the
microtubule interior as a plausible, unifying explanation for diverse
structural and kinetic features of microtubules. This is the first specific
structural hypothesis for the hydrolysis induced conformational change of
tubulin that simultaneously explains weakening of lateral bonds, bending about
longitudinal bonds, changes in protofilament supertwist associated with GTP
hydrolysis, structural features of GDP-tubulin double rings, faster disassembly
at higher temperatures and slower disassembly in the presence of glycerol and
deuterium oxide. The hypothesis suggests further theoretical investigations and
direct experimental tests.",
cross-border merger GDP,http://arxiv.org/abs/0710.5447v1,"Clusters in weighted macroeconomic networks : the EU case. Introducing
  the overlapping index of GDP/capita fluctuation correlations","GDP/capita correlations are investigated in various time windows (TW), for
the time interval 1990-2005. The target group of countries is the set of 25 EU
members, 15 till 2004 plus the 10 countries which joined EU later on. The
TW-means of the statistical correlation coefficients are taken as the weights
(links) of a fully connected network having the countries as nodes. Thereafter
we define and introduce the overlapping index of weighted network nodes. A
cluster structure of EU countries is derived from the statistically relevant
eigenvalues and eigenvectors of the adjacency matrix. This may be considered to
yield some information about the structure, stability and evolution of the EU
country clusters in a macroeconomic sense.",Eur. Phys. J. B 63 (2008) 533-539
cross-border merger GDP,http://arxiv.org/abs/1112.4708v1,"Transformation Networks: How Innovation and the Availability of
  Technology can Increase Economic Performance","A transformation network describes how one set of resources can be
transformed into another via technological processes. Transformation networks
in economics are useful because they can highlight areas for future
innovations, both in terms of new products, new production techniques, or
better efficiency. They also make it easy to detect areas where an economy
might be fragile. In this paper, we use computational simulations to
investigate how the density of a transformation network affects the economic
performance, as measured by the gross domestic product (GDP), of an artificial
economy. Our results show that on average, the GDP of our economy increases as
the density of the transformation network increases. We also find that while
the average performance increases, the maximum possible performance decreases
and the minimum possible performance increases.",
cross-border merger GDP,http://arxiv.org/abs/0911.1044v1,"Macro-level Indicators of the Relations between Research Funding and
  Research Output","In response to the call for a science of science policy, we discuss the
contribution of indicators at the macro-level of nations from a scientometric
perspective. In addition to global trends such as the rise of China, one can
relate percentages of world share of publications to government expenditure in
academic research. The marginal costs of improving one's share are increasing
over time. Countries differ considerably in terms of the efficiency of turning
(financial) input into bibliometrically measurable output. Both funding schemes
and disciplinary portfolios differ among countries. A price per paper can
nevertheless be estimated. The percentages of GDP spent on academic research in
different nations are significantly correlated to historical contingencies such
as the percentage of researchers in the population. The institutional dynamics
make strategic objectives such as the Lisbon objective of the EU--that is,
spending 3% of GDP for R&D in 2010--unrealistic.","Loet Leydesdorff & Caroline Wagner, Macro-level Indicators of the
  Relations between Research Funding and Research Output; Journal of
  Informetrics 3(4) (2009), 353-362"
cross-border merger GDP,http://arxiv.org/abs/1902.05218v1,"Regional economic status inference from information flow and talent
  mobility","Novel data has been leveraged to estimate socioeconomic status in a timely
manner, however, direct comparison on the use of social relations and talent
movements remains rare. In this letter, we estimate the regional economic
status based on the structural features of the two networks. One is the online
information flow network built on the following relations on social media, and
the other is the offline talent mobility network built on the anonymized resume
data of job seekers with higher education. We find that while the structural
features of both networks are relevant to economic status, the talent mobility
network in a relatively smaller size exhibits a stronger predictive power for
the gross domestic product (GDP). In particular, a composite index of
structural features can explain up to about 84% of the variance in GDP. The
result suggests future socioeconomic studies to pay more attention to the
cost-effective talent mobility data.","EPL (Europhysics Letters), 125(6) (2019) 68002"
cross-border merger GDP,http://arxiv.org/abs/1906.01997v3,MEDEAS-World model calibration for the study of the energy transition,"MEDEAS (Modelling the Energy Development under Environmental And
Socioeconomic constraint) World is a new global-aggregated
energy-economy-environmental model, which runs from 1995 to 2050. In this work,
we tested the MEDEAS world model to reproduce the IPCC (International Panel on
Climate Change) GHG (Green House Gases) emission pathways consistent with 2
{\deg}C Global Warming. We achieved parameter optimizations of the MEDEAS model
related to different scenarios until 2050. We chose to provide a sensitivity
analysis on the parameters that directly influence the emission curves focusing
on the annual growth of the RES (Renewable Energy Sources), GDP (Gross Domestic
Product) and annual population growth. From such an analysis, it has been
possible to infer the large impact of GDP on the emission scenarios.",
cross-border merger GDP,http://arxiv.org/abs/1905.02383v3,Gaussian Differential Privacy,"Differential privacy has seen remarkable success as a rigorous and practical
formalization of data privacy in the past decade. This privacy definition and
its divergence based relaxations, however, have several acknowledged
weaknesses, either in handling composition of private algorithms or in
analyzing important primitives like privacy amplification by subsampling.
Inspired by the hypothesis testing formulation of privacy, this paper proposes
a new relaxation, which we term `$f$-differential privacy' ($f$-DP). This
notion of privacy has a number of appealing properties and, in particular,
avoids difficulties associated with divergence based relaxations. First, $f$-DP
preserves the hypothesis testing interpretation. In addition, $f$-DP allows for
lossless reasoning about composition in an algebraic fashion. Moreover, we
provide a powerful technique to import existing results proven for original DP
to $f$-DP and, as an application, obtain a simple subsampling theorem for
$f$-DP.
  In addition to the above findings, we introduce a canonical single-parameter
family of privacy notions within the $f$-DP class that is referred to as
`Gaussian differential privacy' (GDP), defined based on testing two shifted
Gaussians. GDP is focal among the $f$-DP class because of a central limit
theorem we prove. More precisely, the privacy guarantees of \emph{any}
hypothesis testing based definition of privacy (including original DP)
converges to GDP in the limit under composition. The CLT also yields a
computationally inexpensive tool for analyzing the exact composition of private
algorithms.
  Taken together, this collection of attractive properties render $f$-DP a
mathematically coherent, analytically tractable, and versatile framework for
private data analysis. Finally, we demonstrate the use of the tools we develop
by giving an improved privacy analysis of noisy stochastic gradient descent.",
cross-border merger GDP,http://arxiv.org/abs/1908.02584v1,"Economic Power, Population, and the Size of Astronomical Community","The number of astronomers for a country registered to the IAU is known to
have a correlation with the GDP. However, the robustness of this relationship
can be doubted, because the fraction of astronomers joining the IAU differs
from country to country. Here we revisit this correlation by using the recent
data updated as of 2017, and then we find a similar correlation by using the
total enumeration of astronomers and astrophysicists with PhD degrees and
working in each country, instead of adopting the number of IAU members. We
confirm the existence of two subgroup in the correlation. One group consists of
European advanced countries having long history of modern astronomy, while the
other group consists of countries having experienced recent rapid economic
development. In order to find causation in the correlation, we obtain the
long-term variations of the number of astronomers, population, and the GDP for
a number of countries to find that the number of astronomers per citizen for
recently developing countries has increased more rapidly as GDP per capita
increased, than that for fully developed countries. We collect a demographic
data of the Korean astronomical community. From these findings we estimate the
proper size of the Korean astronomical community by considering the society's
economic power and population. The current number of PhD astronomers working in
Korea is approximately 310, but it should be 550 that is large enough to be
comparable and competitive to the sizes of Spainish, Canadian, and Japanese
astronomical communities. We discuss on the way how to overcome the
vulnerability of the Korean astronomical community, based on the statistics of
national R&D expenditure structure comparing with that of other major advanced
countries.",
cross-border merger GDP,http://arxiv.org/abs/physics/0504099v1,"G7 country Gross Domestic Product (GDP) time correlations. A graph
  network analysis","The correlation between G7 countries has been analysed on the basis of Gross
Domestic Product using different distance functions i.e. discrete, linear
correlation and distribution distance. The distance matrics is analysed by
various graph methods and the percolation threshold is calculated. The
globalization process understood as increas of correlation has been observed.
The applications of different distance function discussed.","Practical Fruits of Econophysics, H. Takayasu, Ed. (Springer,
  Tokyo, 2006) pp. 312-316"
cross-border merger GDP,http://arxiv.org/abs/0708.3463v1,A Neural Networks Model of the Venezuelan Economy,"Besides an indicator of the GDP, the Central Bank of Venezuela generates the
so called Monthly Economic Activity General Indicator. The a priori knowledge
of this indicator, which represents and sometimes even anticipates the
economy's fluctuations, could be helpful in developing public policies and in
investment decision making. The purpose of this study is forecasting the IGAEM
through non parametric methods, an approach that has proven effective in a wide
variety of problems in economics and finance.",
cross-border merger GDP,http://arxiv.org/abs/1010.4288v2,"Modeling the Effects of Drug Binding on the Dynamic Instability of
  Microtubules","We propose a stochastic model that accounts for the growth, catastrophe and
rescue processes of steady state microtubules assembled from MAP-free tubulin.
Both experimentally and theoretically we study the perturbation of microtubule
dynamic instability by S-methyl-D-DM1, a synthetic derivative of the
microtubule-targeted agent maytansine and a potential anticancer agent. We find
that to be an effective suppressor of microtubule dynamics a drug must
primarily suppress the loss of GDP tubulin from the microtubule tip.",Phys. Biol. 8:056004 (2011)
cross-border merger CPI,http://arxiv.org/abs/1206.3282v1,Improving the Accuracy and Efficiency of MAP Inference for Markov Logic,"In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in
two different MAP inference methods: the current method of choice for MAP
inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming.",
cross-border merger CPI,http://arxiv.org/abs/1006.4622v2,"A High-Resolution Human Contact Network for Infectious Disease
  Transmission","The most frequent infectious diseases in humans - and those with the highest
potential for rapid pandemic spread - are usually transmitted via droplets
during close proximity interactions (CPIs). Despite the importance of this
transmission route, very little is known about the dynamic patterns of CPIs.
Using wireless sensor network technology, we obtained high-resolution data of
CPIs during a typical day at an American high school, permitting the
reconstruction of the social network relevant for infectious disease
transmission. At a 94% coverage, we collected 762,868 CPIs at a maximal
distance of 3 meters among 788 individuals. The data revealed a high density
network with typical small world properties and a relatively homogenous
distribution of both interaction time and interaction partners among subjects.
Computer simulations of the spread of an influenza-like disease on the weighted
contact graph are in good agreement with absentee data during the most recent
influenza season. Analysis of targeted immunization strategies suggested that
contact network data are required to design strategies that are significantly
more effective than random immunization. Immunization strategies based on
contact network data were most effective at high vaccination coverage.",
cross-border merger CPI,http://arxiv.org/abs/1405.2878v1,Approximate Policy Iteration Schemes: A Comparison,"We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on several approximate
variations of the Policy Iteration algorithm: Approximate Policy Iteration,
Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search
by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),
and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all
algorithms, we describe performance bounds, and make a comparison by paying a
particular attention to the concentrability constants involved, the number of
iterations and the memory required. Our analysis highlights the following
points: 1) The performance guarantee of CPI can be arbitrarily better than that
of API/API($\alpha$), but this comes at the cost of a relative---exponential in
$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$
enjoys the best of both worlds: its performance guarantee is similar to that of
CPI, but within a number of iterations similar to that of API. 3) Contrary to
API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$
is proportional to their number of iterations, which may be problematic when
the discount factor $\gamma$ is close to 1 or the approximation error
$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make
an overall trade-off between memory and performance. Simulations with these
schemes confirm our analysis.",
cross-border merger CPI,http://arxiv.org/abs/1705.08775v2,"A Control Performance Index for Multicopters Under Off-nominal
  Conditions","In order to prevent loss of control (LOC) accidents,the real-time control
performance monitoring problem is studied for multicopters. Different from the
existing work, this paper does not try to monitor the performance of the
controllers directly. In turn, the disturbances of multicopters under
off-nominal conditions are estimated to affect a proposed index to tell the
user whether the multicopter will be LOC or not. Firstly, a new degree of
controllability (DoC) will be proposed for multicopters subject to control
constrains and off-nominal conditions. Then a control performance index (CPI)
is defined based on the new DoC to reflect the control performance for
multicopters. Besides, the proposed CPI is applied to a new switching control
framework to guide the control decision of multicopter under off-nominal
conditions. Finally, simulation and experimental results show the effectiveness
of the CPI and the proposed switching control framework.",
cross-border merger CPI,http://arxiv.org/abs/1906.09784v1,Deep Conservative Policy Iteration,"Conservative Policy Iteration (CPI) is a founding algorithm of Approximate
Dynamic Programming (ADP). Its core principle is to stabilize greediness
through stochastic mixtures of consecutive policies. It comes with strong
theoretical guarantees, and inspired approaches in deep Reinforcement Learning
(RL). However, CPI itself has rarely been implemented, never with neural
networks, and only experimented on toy problems. In this paper, we show how CPI
can be practically combined with deep RL with discrete actions. We also
introduce adaptive mixture rates inspired by the theory. We experiment
thoroughly the resulting algorithm on the simple Cartpole problem, and validate
the proposed method on a representative subset of Atari games. Overall, this
work suggests that revisiting classic ADP may lead to improved and more stable
deep RL algorithms.",
cross-border merger CPI,http://arxiv.org/abs/0708.3387v1,"The Impact of Noise Correlation and Channel Phase Information on the
  Data-Rate of the Single-Symbol ML Decodable Distributed STBCs","Very recently, we proposed the row-monomial distributed orthogonal space-time
block codes (DOSTBCs) and showed that the row-monomial DOSTBCs achieved
approximately twice higher bandwidth efficiency than the repetitionbased
cooperative strategy [1]. However, we imposed two limitations on the
row-monomial DOSTBCs. The first one was that the associated matrices of the
codes must be row-monomial. The other was the assumption that the relays did
not have any channel state information (CSI) of the channels from the source to
the relays, although this CSI could be readily obtained at the relays without
any additional pilot signals or any feedback overhead. In this paper, we first
remove the row-monomial limitation; but keep the CSI limitation. In this case,
we derive an upper bound of the data-rate of the DOSTBC and it is larger than
that of the row-monomial DOSTBCs in [1]. Secondly, we abandon the CSI
limitation; but keep the row-monomial limitation. Specifically, we propose the
row-monomial DOSTBCs with channel phase information (DOSTBCs-CPI) and derive an
upper bound of the data-rate of those codes. The rowmonomial DOSTBCs-CPI have
higher data-rate than the DOSTBCs and the row-monomial DOSTBCs. Furthermore, we
find the actual row-monomial DOSTBCs-CPI which achieve the upper bound of the
data-rate.",
cross-border merger CPI,http://arxiv.org/abs/1306.0539v1,"On the Performance Bounds of some Policy Search Dynamic Programming
  Algorithms","We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on Policy Search algorithms,
that compute an approximately optimal policy by following the standard Policy
Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,
2002; Lazaric et al., 2010). We describe existing and a few new performance
bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et
al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)
(Kakade and Langford, 2002). By paying a particular attention to the
concentrability constants involved in such guarantees, we notably argue that
the guarantee of CPI is much better than that of DPI, but this comes at the
cost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of time
complexity. We then describe an algorithm, Non-Stationary Direct Policy
Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search
by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon
situation or 2) a simplified version of the Non-Stationary PI with growing
period of Scherrer and Lesner (2012). We provide an analysis of this algorithm,
that shows in particular that it enjoys the best of both worlds: its
performance guarantee is similar to that of CPI, but within a time complexity
similar to that of DPI.",
cross-border merger CPI,http://arxiv.org/abs/1710.05944v1,Neuro Fuzzy Modelling for Prediction of Consumer Price Index,"Economic indicators such as Consumer Price Index (CPI) have frequently used
in predicting future economic wealth for financial policy makers of respective
country. Most central banks, on guidelines of research studies, have recently
adopted an inflation targeting monetary policy regime, which accounts for high
requirement for effective prediction model of consumer price index. However,
prediction accuracy by numerous studies is still low, which raises a need for
improvement. This manuscript presents findings of study that use neuro fuzzy
technique to design a machine-learning model that train and test data to
predict a univariate time series CPI. The study establishes a matrix of monthly
CPI data from secondary data source of Tanzania National Bureau of Statistics
from January 2000 to December 2015 as case study and thereafter conducted
simulation experiments on MATLAB whereby ninety five percent (95%) of data used
to train the model and five percent (5%) for testing. Furthermore, the study
use root mean square error (RMSE) and mean absolute percentage error (MAPE) as
error metrics for model evaluation. The results show that the neuro fuzzy model
have an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2,
2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to
existing research studies.","International Journal of Artificial Intelligence and Applications
  (IJAIA), Vol.8, No.5, September 2017"
cross-border merger CPI,http://arxiv.org/abs/1405.1750v1,Numerical simulation of turbulent duct flows with constant power input,"The numerical simulation of a flow through a duct requires an externally
specified forcing that makes the fluid flow against viscous friction. To this
aim, it is customary to enforce a constant value for either the flow rate (CFR)
or the pressure gradient (CPG). When comparing a laminar duct flow before and
after a geometrical modification that induces a change of the viscous drag,
both approaches (CFR and CPG) lead to a change of the power input across the
comparison. Similarly, when carrying out the (DNS and LES) numerical simulation
of unsteady turbulent flows, the power input is not constant over time.
Carrying out a simulation at constant power input (CPI) is thus a further
physically sound option, that becomes particularly appealing in the context of
flow control, where a comparison between control-on and control-off conditions
has to be made.
  We describe how to carry out a CPI simulation, and start with defining a new
power-related Reynolds number, whose velocity scale is the bulk flow that can
be attained with a given pumping power in the laminar regime. Under the CPI
condition, we derive a relation that is equivalent to the
Fukagata--Iwamoto--Kasagi relation valid for CFR (and to its extension valid
for CPG), that presents the additional advantage of natively including the
required control power. The implementation of the CPI approach is then
exemplified in the standard case of a plane turbulent channel flow, and then
further applied to a flow control case, where the spanwise-oscillating wall is
used for skin friction drag reduction. For this low-Reynolds number flow, using
90% of the available power for the pumping system and the remaining 10% for the
control system is found to be the optimum share that yields the largest
increase of the flow rate above the reference case, where 100% of the power
goes to the pump.",
cross-border merger CPI,http://arxiv.org/abs/1709.00310v3,"Detection via simultaneous trajectory estimation and long time
  integration","In this work, we consider the detection of manoeuvring small objects with
radars. Such objects induce low signal to noise ratio (SNR) reflections in the
received signal. We consider both co-located and separated transmitter/receiver
pairs, i.e., mono-static and bi-static configurations, respectively, as well as
multi-static settings involving both types. We propose a detection approach
which is capable of coherently integrating these reflections within a coherent
processing interval (CPI) in all these configurations and continuing
integration for an arbitrarily long time across consecutive CPIs. We estimate
the complex value of the reflection coefficients for integration while
simultaneously estimating the object trajectory. Compounded with this is the
estimation of the unknown time reference shift of the separated transmitters
necessary for coherent processing. Detection is made by using the resulting
integration value in a Neyman-Pearson test against a constant false alarm rate
threshold. We demonstrate the efficacy of our approach in a simulation example
with a very low SNR object which cannot be detected with conventional
techniques.",
cross-border merger CPI,http://arxiv.org/abs/1907.07776v1,CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers,"Memory controller scheduling is crucial in multicore processors, where DRAM
bandwidth is shared. Since increased number of requests from multiple cores of
processors becomes a source of bottleneck, scheduling the requests efficiently
is necessary to utilize all the computing power these processors offer.
However, current multicore processors are using traditional memory controllers,
which are designed for single-core processors. They are unable to adapt to
changing characteristics of memory workloads that run simultaneously on
multiple cores. Existing schedulers may disrupt locality and bank parallelism
among data requests coming from different cores. Hence, novel memory
controllers that consider and adapt to the memory access characteristics, and
share memory resources efficiently and fairly are necessary. We introduce
Core-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses
Reinforcement Learning (RL) to alter its scheduling strategy dynamically at
runtime. Our scheduler utilizes locality among data requests from multiple
cores and exploits parallelism in accessing multiple banks of DRAM. CADS is
also able to share the DRAM while guaranteeing fairness to all cores accessing
memory. Using CADS policy, we achieve 20% better cycles per instruction (CPI)
in running memory intensive and compute intensive PARSEC parallel benchmarks
simultaneously, and 16% better CPI with SPEC 2006 benchmarks.",
cross-border merger CPI,http://arxiv.org/abs/physics/0008035v2,2.856-GHz Modulation of Conventional Triode Electron Gun,"For the generation of picosecond (< 100 ps) electron beam pulses, we studied
the RF modulation of a conventional triode electron gun. The feasibility study
for this scheme has been experimentally investigated by modulating a triode gun
of the Y-824 cathode-grid (KG) structure provided by the CPI Eimac, with
2.856-GHz pulsed RF generated by a solid-state amplifier (SSA). In this paper,
we present the methods and results of this investigation.",eConf C000821 (2000) MOB18
cross-border merger CPI,http://arxiv.org/abs/0807.4219v1,Statistical properties of world investment networks,"We have performed a detailed investigation on the world investment networks
constructed from the Coordinated Portfolio Investment Survey (CPIS) data of the
International Monetary Fund, ranging from 2001 to 2006. The distributions of
degrees and node strengthes are scale-free. The weight distributions can be
well modeled by the Weibull distribution. The maximum flow spanning trees of
the world investment networks possess two universal allometric scaling
relations, independent of time and the investment type. The topological scaling
exponent is $1.17\pm0.02$ and the flow scaling exponent is $1.03\pm0.01$.","Physica A 388 (12), 2450-2460 (2009)"
cross-border merger CPI,http://arxiv.org/abs/1507.02456v2,Towards Log-Linear Logics with Concrete Domains,"We present $\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an
extension of the log-linear description logics $\mathcal{EL}^{++}$-LL with
concrete domains, nominals, and instances. We use Markov logic networks (MLNs)
in order to find the most probable, classified and coherent $\mathcal{EL}^{++}$
ontology from an $\mathcal{MEL}^{++}$ knowledge base. In particular, we develop
a novel way to deal with concrete domains (also known as datatypes) by
extending MLN's cutting plane inference (CPI) algorithm.",
cross-border merger CPI,http://arxiv.org/abs/1504.04974v1,Understanding Big Data Analytic Workloads on Modern Processors,"Big data analytics applications play a significant role in data centers, and
hence it has become increasingly important to understand their behaviors in
order to further improve the performance of data center computer systems, in
which characterizing representative workloads is a key practical problem. In
this paper, after investigating three most impor- tant application domains in
terms of page views and daily visitors, we chose 11 repre- sentative data
analytics workloads and characterized their micro-architectural behaviors by
using hardware performance counters, so as to understand the impacts and
implications of data analytics workloads on the systems equipped with modern
superscalar out-of-order processors. Our study reveals that big data analytics
applications themselves share many inherent characteristics, which place them
in a different class from traditional workloads and scale-out services. To
further understand the characteristics of big data analytics work- loads we
performed a correlation analysis of CPI (cycles per instruction) with other
micro- architecture level characteristics and an investigation of the big data
software stack impacts on application behaviors. Our correlation analysis
showed that even though big data ana- lytics workloads own notable pipeline
front end stalls, the main factors affecting the CPI performance are long
latency data accesses rather than the front end stalls. Our software stack
investigation found that the typical big data software stack significantly
contributes to the front end stalls and incurs bigger working set. Finally we
gave several recommen- dations for architects, programmers and big data system
designers with the knowledge acquired from this paper.",
cross-border merger CPI,http://arxiv.org/abs/1901.06103v1,"Exploring Semi-supervised Variational Autoencoders for Biomedical
  Relation Extraction","The biomedical literature provides a rich source of knowledge such as
protein-protein interactions (PPIs), drug-drug interactions (DDIs) and
chemical-protein interactions (CPIs). Biomedical relation extraction aims to
automatically extract biomedical relations from biomedical text for various
biomedical research. State-of-the-art methods for biomedical relation
extraction are primarily based on supervised machine learning and therefore
depend on (sufficient) labeled data. However, creating large sets of training
data is prohibitively expensive and labor-intensive, especially so in
biomedicine as domain knowledge is required. In contrast, there is a large
amount of unlabeled biomedical text available in PubMed. Hence, computational
methods capable of employing unlabeled data to reduce the burden of manual
annotation are of particular interest in biomedical relation extraction. We
present a novel semi-supervised approach based on variational autoencoder (VAE)
for biomedical relation extraction. Our model consists of the following three
parts, a classifier, an encoder and a decoder. The classifier is implemented
using multi-layer convolutional neural networks (CNNs), and the encoder and
decoder are implemented using both bidirectional long short-term memory
networks (Bi-LSTMs) and CNNs, respectively. The semi-supervised mechanism
allows our model to learn features from both the labeled and unlabeled data. We
evaluate our method on multiple public PPI, DDI and CPI corpora. Experimental
results show that our method effectively exploits the unlabeled data to improve
the performance and reduce the dependence on labeled data. To our best
knowledge, this is the first semi-supervised VAE-based method for (biomedical)
relation extraction. Our results suggest that exploiting such unlabeled data
can be greatly beneficial to improved performance in various biomedical
relation extraction.",
cross-border merger CPI,http://arxiv.org/abs/1309.7119v3,"Stock price direction prediction by directly using prices data: an
  empirical study on the KOSPI and HSI","The prediction of a stock market direction may serve as an early
recommendation system for short-term investors and as an early financial
distress warning system for long-term shareholders. Many stock prediction
studies focus on using macroeconomic indicators, such as CPI and GDP, to train
the prediction model. However, daily data of the macroeconomic indicators are
almost impossible to obtain. Thus, those methods are difficult to be employed
in practice. In this paper, we propose a method that directly uses prices data
to predict market index direction and stock price direction. An extensive
empirical study of the proposed method is presented on the Korean Composite
Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual
constituents included in the indices. The experimental results show notably
high hit ratios in predicting the movements of the individual constituents in
the KOSPI and HIS.",
cross-border merger CPI,http://arxiv.org/abs/1602.07367v1,"Classical realization of dispersion-canceled, artifact-free, and
  background-free optical coherence tomography","Quantum-optical coherence tomography (Q-OCT) provides a dispersion-canceled
axial-imaging method, but its practical use is limited by the weakness of the
light source and by artifacts in the images. A recent study using chirped-pulse
interferometry (CPI) has demonstrated dispersion-canceled and artifact-free OCT
with a classical system; however, unwanted background signals still remain
after removing the artifacts. Here, we propose a classical optical method that
realizes dispersion-canceled, artifact-free, and background-free OCT. We employ
a time-reversed system for Q-OCT with transform-limited input laser pulses to
achieve dispersion-canceled OCT with a classical system. We have also
introduced a subtraction method to remove artifacts and background signals.
With these methods, we experimentally demonstrated dispersion-canceled,
artifact-free, and background-free axial imaging of a coverglass and
cross-sectional imaging of the surface of a coin.","Optics Express Vol. 24, Issue 8, pp. 8280-8289 (2016)"
cross-border merger CPI,http://arxiv.org/abs/1607.02818v1,Equation-free analysis of a dynamically evolving multigraph,"In order to illustrate the adaptation of traditional continuum numerical
techniques to the study of complex network systems, we use the equation-free
framework to analyze a dynamically evolving multigraph. This approach is based
on coupling short intervals of direct dynamic network simulation with
appropriately-defined lifting and restriction operators, mapping the detailed
network description to suitable macroscopic (coarse-grained) variables and
back. This enables the acceleration of direct simulations through Coarse
Projective Integration (CPI), as well as the identification of coarse
stationary states via a Newton-GMRES method. We also demonstrate the use of
data-mining, both linear (principal component analysis, PCA) and nonlinear
(diffusion maps, DMAPS) to determine good macroscopic variables (observables)
through which one can coarse-grain the model. These results suggest methods for
decreasing simulation times of dynamic real-world systems such as
epidemiological network models. Additionally, the data-mining techniques could
be applied to a diverse class of problems to search for a succint,
low-dimensional description of the system in a small number of variables.",
cross-border merger CPI,http://arxiv.org/abs/1212.2044v2,Macro-Economic Time Series Modeling and Interaction Networks,"Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.","Applications of Evolutionary Computation, LNCS 6625 (Springer
  Berlin Heidelberg), pp. 101-110 (2011)"
cross-border merger CPI,http://arxiv.org/abs/1903.00191v1,"MIPS-Core Application Specific Instruction-Set Processor for IDEA
  Cryptography - Comparison between Single-Cycle and Multi-Cycle Architectures","A single-cycle processor completes the execution of an instruction in only
one clock cycle. However, its clock period is usually rather long. On the
contrary, although clock frequency is higher in a multi-cycle processor, it
takes several clock cycles to finish an instruction. Therefore, their runtime
efficiencies depend on which program is executed. This paper presents a new
processor for International Data Encryption Algorithm (IDEA) cryptography. The
new design is an Application Specific Instruction-set Processor (ASIP) in which
both general-purpose and special instructions are supported. It is a
single-cycle MIPS-core architecture, whose average Clocks Per Instruction (CPI)
is 1. Furthermore, a comparison is provided in this paper to show the
differences between the proposed single-cycle processor and another comparable
multi-cycle crypto processor. FPGA implementation results show that both
architectures have almost the same encoding/decoding throughput. However, the
previous processor consumes nearly twice as many resources as the new one does.",
cross-border merger CPI,http://arxiv.org/abs/1909.02769v1,"Adaptive Trust Region Policy Optimization: Global Convergence and Faster
  Rates for Regularized MDPs","Trust region policy optimization (TRPO) is a popular and empirically
successful policy search algorithm in Reinforcement Learning (RL) in which a
surrogate problem, that restricts consecutive policies to be `close' to one
another, is iteratively solved. Nevertheless, TRPO has been considered a
heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show
that the adaptive scaling mechanism used in TRPO is in fact the natural ""RL
version"" of traditional trust-region methods from convex analysis. We first
analyze TRPO in the planning setting, in which we have access to the model and
the entire state space. Then, we consider sample-based TRPO and establish
$\tilde O(1/\sqrt{N})$ convergence rate to the global optimum. Importantly, the
adaptive scaling mechanism allows us to analyze TRPO in {\em regularized MDPs}
for which we prove fast rates of $\tilde O(1/N)$, much like results in convex
optimization. This is the first result in RL of better rates when regularizing
the instantaneous cost or reward.",
cross-border merger CPI,http://arxiv.org/abs/1011.0534v1,Merger Dynamics in Three-Agent Games,"We present the effect of mergers in the dynamics of the three-agent model
studied by Ben-Naim, Kahng and Kim and by Rador and Mungan. Mergers are
possible in three-agent games because two agents can combine forces against the
third player and thus increase their probability to win a competition. We
implement mergers in this three-agent model via resolving merger and no-merger
units of competition in terms of a two-agent unit. This way one needs only a
single parameter which we have called the competitiveness parameter. We have
presented an analytical solution in the fully competitive limit. In this limit
the score distribution of agents is stratified and self-similar.","Eur. Phys. J. B 83, 289 - 299, 2011"
cross-border merger CPI,http://arxiv.org/abs/1301.5653v2,"A High-power 650 MHz CW Magnetron Transmitter for Intensity Frontier
  Superconducting Accelerators","A concept of a 650 MHz CW magnetron transmitter with fast control in phase
and power, based on two-stage injection-locked CW magnetrons, has been proposed
to drive Superconducting Cavities (SC) for intensity-frontier accelerators. The
concept is based on a theoretical model considering a magnetron as a forced
oscillator and experimentally verified with a 2.5 MW pulsed magnetron. To
fulfill fast control of phase and output power requirements of SC accelerators,
both two-stage injection-locked CW magnetrons are combined with a 3-dB hybrid.
Fast control in output power is achieved by varying the input phase of one of
the magnetrons. For output power up to 250 kW we expect the output/input power
ratio to be about 35 to 40 dB in CW or quasi-CW mode with long pulse duration.
All magnetrons of the transmitter should be based on commercially available
models to decrease the cost of the system. An experimental model using 1 kW,
CW, S-band, injection-locked magnetrons with a 3-dB hybrid combiner has been
developed and built for study. A description of the model, simulations, and
experimental results are presented and discussed in this work.",
cross-border merger CPI,http://arxiv.org/abs/1809.00730v1,"Global energy fluxes in fully-developed turbulent channels with flow
  control","This paper addresses the integral energy fluxes in natural and controlled
turbulent channel flows, where active skin-friction drag reduction techniques
allow a more efficient use of the available power. We study whether the
increased efficiency shows any general trend in how energy is dissipated by the
mean velocity field (mean dissipation) and by the fluctuating velocity field
(turbulent dissipation).
  Direct Numerical Simulations (DNS) of different control strategies are
performed at Constant Power Input (CPI), so that at statistical equilibrium
each flow (either uncontrolled or controlled by different means) has the same
power input, hence the same global energy flux and, by definition, the same
total energy dissipation rate. The simulations reveal that changes in mean and
turbulent energy dissipation rates can be of either sign in a successfully
controlled flow.
  A quantitative description of these changes is made possible by a new
decomposition of the total dissipation, stemming from an extended Reynolds
decomposition, where the mean velocity is split into a laminar component and a
deviation from it. Thanks to the analytical expressions of the laminar
quantities, exact relationships are derived that link the achieved flow rate
increase and all energy fluxes in the flow system with two wall-normal
integrals of the Reynolds shear stress and the Reynolds number. The dependence
of the energy fluxes on the Reynolds number is elucidated with a simple model
in which the control-dependent changes of the Reynolds shear stress are
accounted for via a modification of the mean velocity profile. The physical
meaning of the energy fluxes stemming from the new decomposition unveils their
inter-relations and connection to flow control, so that a clear target for flow
control can be identified.",
cross-border merger wages,http://arxiv.org/abs/1909.12338v1,Hardware Design and Analysis of the ACE and WAGE Ciphers,"This paper presents the hardware design and analysis of ACE and WAGE, two
candidate ciphers for the NIST Lightweight Cryptography standardization. Both
ciphers use sLiSCP's unified sponge duplex mode. ACE has an internal state of
320 bits, uses three 64 bit Simeck boxes, and implements both authenticated
encryption and hashing. WAGE is based on the Welch-Gong stream cipher and
provides authenticated encryption. WAGE has 259 bits of state, two 7 bit
Welch-Gong permutations, and four lightweight 7 bit S-boxes. ACE and WAGE have
the same external interface and follow the same I/O protocol to transition
between phases. The paper illustrates how a hardware perspective influenced key
aspects of the ACE and WAGE algorithms. The paper reports area, power, and
energy results for both serial and parallel (unrolled) implementations using
four different ASIC libraries: two 65 nm libraries, a 90 nm library, and a 130
nm library. ACE implementations range from a throughput of 0.5 bits-per-clock
cycle (bpc) and an area of 4210 GE (averaged across the four ASIC libraries) up
to 4 bpc and 7260 GE. WAGE results range from 0.57 bpc with 2920 GE to 4.57 bpc
with 11080 GE.",
cross-border merger wages,http://arxiv.org/abs/1001.0627v1,The Labor Economics of Paid Crowdsourcing,"Crowdsourcing is a form of ""peer production"" in which work traditionally
performed by an employee is outsourced to an ""undefined, generally large group
of people in the form of an open call."" We present a model of workers supplying
labor to paid crowdsourcing projects. We also introduce a novel method for
estimating a worker's reservation wage--the smallest wage a worker is willing
to accept for a task and the key parameter in our labor supply model. It shows
that the reservation wages of a sample of workers from Amazon's Mechanical Turk
(AMT) are approximately log normally distributed, with a median wage of
$1.38/hour. At the median wage, the point elasticity of extensive labor supply
is 0.43. We discuss how to use our calibrated model to make predictions in
applied work. Two experimental tests of the model show that many workers
respond rationally to offered incentives. However, a non-trivial fraction of
subjects appear to set earnings targets. These ""target earners"" consider not
just the offered wage--which is what the rational model predicts--but also
their proximity to earnings goals. Interestingly, a number of workers clearly
prefer earning total amounts evenly divisible by 5, presumably because these
amounts make good targets.",
cross-border merger wages,http://arxiv.org/abs/1609.09067v1,Using Big Data to Decode Private Sector Wage Growth,"The U.S. labor market is dynamic and complex, and understanding wage data
across different segments of the workforce is critical to providing
policymakers and business leaders with actionable insights. There is no labor
index that assesses the labor market performance at such a detailed level as
the ADP Research Institute's Workforce Vitality Report (WVR). Drawing on the
actual, aggregated and anonymous payroll data of 24 million Americans paid by
ADP, the WVR looks at key dynamics and market indicators including wage growth,
hours worked and turnover rate. Unlike other data sets, the WVR calculates wage
growth of individual workers on a quarter-to-quarter basis, avoiding the
deviations caused by various workplace occurrences, like when new workers are
hired and older ones retire. In this paper, Dr. Ahu Yildirmaz, head of the ADP
Research Institute, drills down into wage growth by industry, age, gender and
income level, as well as for both job holders and job switchers. Using WVR
data, Ahu walks through those factors contributing to overall shifts in wage
growth, the future of the labor market and what this data means for today's
U.S. workforce.",
cross-border merger wages,http://arxiv.org/abs/1712.05796v2,A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk,"A growing number of people are working as part of on-line crowd work, which
has been characterized by its low wages; yet, we know little about wage
distribution and causes of low/high earnings. We recorded 2,676 workers
performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis
revealed that workers earned a median hourly wage of only ~\$2/h, and only 4%
earned more than \$7.25/h. The average requester pays more than \$11/h,
although lower-paying requesters post much more work. Our wage calculations are
influenced by how unpaid work is included in our wage calculations, e.g., time
spent searching for tasks, working on tasks that are rejected, and working on
tasks that are ultimately not submitted. We further explore the characteristics
of tasks and working patterns that yield higher hourly wages. Our analysis
informs future platform design and worker tools to create a more positive
future for crowd work.",
cross-border merger wages,http://arxiv.org/abs/physics/0506229v1,"The Age-Competency Model to the Study of the Age-Wage Profiles for
  Workers","In this article, I present a new approach and a novel model to the study of
the life cycle of wages. The key idea is that wage can be thought as
remuneration paid for the competency. It is assumed with the approach that
there are three mechanisms acting at micro level and resulting in the change of
workers' competencies during their lives. These are an endogenous growth of
workers' initial competencies; a rate of investments in schooling in the life
cycle of wages; and an effect of relative losses in workers' competencies. The
developed model is to shed light on the processes resulting in the age-wage
profiles seen in mass. The model obeys a nonlinear integro-differential
equation. The found analytic solution of the equation has the form of Fisk PDF
of a special type. The solution and its features are discussed. The regression
technique is used to check the model upon reliability. The model provides
better fitting to the data (Elo and Salonen, 2004) than minceraninan earnings
function (Mincer, 1974) does.",
cross-border merger wages,http://arxiv.org/abs/1706.10097v3,Design Activism for Minimum Wage Crowd Work,"Entry-level crowd work is often reported to pay less than minimum wage. While
this may be appropriate or even necessary, due to various legal, economic, and
pragmatic factors, some Requesters and workers continue to question this status
quo. To promote further discussion on the issue, we survey Requesters and
workers whether they would support restricting tasks to require minimum wage
pay. As a form of design activism, we confronted workers with this dilemma
directly by posting a dummy Mechanical Turk task which told them that they
could not work on it because it paid less than their local minimum wage, and we
invited their feedback. Strikingly, for those workers expressing an opinion,
two-thirds of Indians favored the policy while two-thirds of Americans opposed
it. Though a majority of Requesters supported minimum wage pay, only 20\% would
enforce it. To further empower Requesters, and to ensure that effort or
ignorance are not barriers to change, we provide a simple public API to make it
easy to find a worker's local minimum wage by his/her IP address.",
cross-border merger wages,http://arxiv.org/abs/1903.07032v1,TurkScanner: Predicting the Hourly Wage of Microtasks,"Workers in crowd markets struggle to earn a living. One reason for this is
that it is difficult for workers to accurately gauge the hourly wages of
microtasks, and they consequently end up performing labor with little pay. In
general, workers are provided with little information about tasks, and are left
to rely on noisy signals, such as textual description of the task or rating of
the requester. This study explores various computational methods for predicting
the working times (and thus hourly wages) required for tasks based on data
collected from other workers completing crowd work. We provide the following
contributions. (i) A data collection method for gathering real-world training
data on crowd-work tasks and the times required for workers to complete them;
(ii) TurkScanner: a machine learning approach that predicts the necessary
working time to complete a task (and can thus implicitly provide the expected
hourly wage). We collected 9,155 data records using a web browser extension
installed by 84 Amazon Mechanical Turk workers, and explored the challenge of
accurately recording working times both automatically and by asking workers.
TurkScanner was created using ~150 derived features, and was able to predict
the hourly wages of 69.6% of all the tested microtasks within a 75% error.
Directions for future research include observing the effects of tools on
people's working practices, adapting this approach to a requester tool for
better price setting, and predicting other elements of work (e.g., the
acceptance likelihood and worker task preferences.)",
cross-border merger wages,http://arxiv.org/abs/1810.07781v2,"Responsible team players wanted: an analysis of soft skill requirements
  in job advertisements","During the past decades the importance of soft skills for labour market
outcomes has grown substantially. This carries implications for labour market
inequality, since previous research shows that soft skills are not valued
equally across race and gender. This work explores the role of soft skills in
job advertisements by drawing on methods from computational science as well as
on theoretical and empirical insights from economics, sociology and psychology.
We present a semi-automatic approach based on crowdsourcing and text mining for
extracting a list of soft skills. We find that soft skills are a crucial
component of job ads, especially of low-paid jobs and jobs in female-dominated
professions. Our work shows that soft skills can serve as partial predictors of
the gender composition in job categories and that not all soft skills receive
equal wage returns at the labour market. Especially ""female"" skills are
frequently associated with wage penalties. Our results expand the growing
literature on the association of soft skills on wage inequality and highlight
their importance for occupational gender segregation at labour markets.",
cross-border merger wages,http://arxiv.org/abs/1905.12535v1,Ride-share matching algorithms generate income inequality,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",
cross-border merger wages,http://arxiv.org/abs/1705.07643v5,Near-Feasible Stable Matchings with Budget Constraints,"We consider the matching with contracts framework of Hatfield and Milgrom
when one side (a firm or hospital) can make monetary transfers (offer wages) to
the other (a worker or doctor). In a standard model, monetary transfers are not
restricted. However, we assume that each hospital has a fixed budget; that is,
the total amount of wages allocated by each hospital to the doctors is
constrained. With this constraint, stable matchings may fail to exist and
checking for the existence is hard. To deal with the nonexistence, we focus on
near-feasible matchings that can exceed each hospital budget by a certain
amount, and We introduce a new concept of compatibility. We show that the
compatibility condition is a sufficient condition for the existence of a
near-feasible stable matching in the matching with contracts framework. Under a
slight restriction on hospitals' preferences, we provide mechanisms that
efficiently return a near-feasible stable matching with respect to the actual
amount of wages allocated by each hospital. By sacrificing strategy-proofness,
the best possible bound of budget excess is achieved.",
cross-border merger wages,http://arxiv.org/abs/1601.05664v1,Defining urban agglomerations to detect agglomeration economies,"Agglomeration economies are a persistent subject of debate among economists
and urban planners. Their definition turns on whether or not larger cities and
regions are more efficient and more productive than smaller ones. We complement
existing discussion on agglomeration economies and the urban wage premium here
by providing a sensitivity analysis of estimated coefficients to different
delineations of urban agglomeration as well as to different definitions of the
economic measure that summarises the urban premium. This quantity can consist
of total wages measured at the place of work, or of income registered at the
place of residence. The chosen option influences the scaling behaviour of city
size as well as the spatial distribution of the phenomenon at the city level.
Spatial discrepancies between the distribution of jobs and the distribution of
households at different economic levels makes city definitions crucial to the
estimation of economic relations which vary with city size. We argue this point
by regressing measures of income and wage over about five thousands different
definitions of cities in France, based on our algorithmic aggregation of
administrative spatial units at regular cutoffs which reflect density,
population thresholds and commuting flows. We also go beyond aggregated
observations of wages and income by searching for evidence of larger
inequalities and economic segregation in the largest cities. This paper
therefore considers the spatial and economic complexity of cities with respect
to discussion about how we measure agglomeration economies. It provides a basis
for reflection on alternative ways to model the processes which lead to
observed variations, and this can provide insights for more comprehensive
regional planning.","Cottineau, C., Finance, O., Hatna, E., Arcaute, E., & Batty, M.
  (2018). Defining urban clusters to detect agglomeration economies.
  Environment and Planning B: Urban Analytics and City Science,
  2399808318755146"
cross-border merger wages,http://arxiv.org/abs/1510.05189v2,Causal Falling Rule Lists,"A causal falling rule list (CFRL) is a sequence of if-then rules that
specifies heterogeneous treatment effects, where (i) the order of rules
determines the treatment effect subgroup a subject belongs to, and (ii) the
treatment effect decreases monotonically down the list. A given CFRL
parameterizes a hierarchical bayesian regression model in which the treatment
effects are incorporated as parameters, and assumed constant within
model-specific subgroups. We formulate the search for the CFRL best supported
by the data as a Bayesian model selection problem, where we perform a search
over the space of CFRL models, and approximate the evidence for a given CFRL
model using standard variational techniques. We apply CFRL to a census wage
dataset to identify subgroups of differing wage inequalities between men and
women.",
cross-border merger wages,http://arxiv.org/abs/1404.6103v1,"An Approximate ""Law of One Price"" in Random Assignment Games","Assignment games represent a tractable yet versatile model of two-sided
markets with transfers. We study the likely properties of the core of randomly
generated assignment games. If the joint productivities of every firm and
worker are i.i.d bounded random variables, then with high probability all
workers are paid roughly equal wages, and all firms make similar profits. This
implies that core allocations vary significantly in balanced markets, but that
there is core convergence in even slightly unbalanced markets. For the
benchmark case of uniform distribution, we provide a tight bound for the
workers' share of the surplus under the firm-optimal core allocation. We
present simulation results suggesting that the phenomena analyzed appear even
in medium-sized markets. Finally, we briefly discuss the effects of unbounded
distributions and the ways in which they may affect wage dispersion.",
cross-border merger wages,http://arxiv.org/abs/1802.04680v1,Training and Inference with Integers in Deep Neural Networks,"Researches on deep neural networks with discrete parameters and their
deployment in embedded systems have been active and promising topics. Although
previous works have successfully reduced precision in inference, transferring
both training and inference processes to low-bitwidth integers has not been
demonstrated simultaneously. In this work, we develop a new method termed as
""WAGE"" to discretize both training and inference, where weights (W),
activations (A), gradients (G) and errors (E) among layers are shifted and
linearly constrained to low-bitwidth integers. To perform pure discrete
dataflow for fixed-point devices, we further replace batch normalization by a
constant scaling layer and simplify other components that are arduous for
integer implementation. Improved accuracies can be obtained on multiple
datasets, which indicates that WAGE somehow acts as a type of regularization.
Empirically, we demonstrate the potential to deploy training in hardware
systems such as integer-based deep learning accelerators and neuromorphic chips
with comparable accuracy and higher energy efficiency, which is crucial to
future AI applications in variable scenarios with transfer and continual
learning demands.",
cross-border merger wages,http://arxiv.org/abs/physics/0505173v1,Empirical study and model of personal income,"Personal income distributions in Japan are analyzed empirically and a simple
stochastic model of the income process is proposed. Based on empirical facts,
we propose a minimal two-factor model. Our model of personal income consists of
an asset accumulation process and a wage process. We show that these simple
processes can successfully reproduce the empirical distribution of income. In
particular, the model can reproduce the particular transition of the
distribution shape from the middle part to the tail part. This model also
allows us to derive the tail exponent of the distribution analytically.",
cross-border merger wages,http://arxiv.org/abs/physics/0510248v1,A Harris-Todaro Agent-Based Model to Rural-Urban Migration,"The Harris-Todaro model of the rural-urban migration process is revisited
under an agent-based approach. The migration of the workers is interpreted as a
process of social learning by imitation, formalized by a computational model.
By simulating this model, we observe a transitional dynamics with continuous
growth of the urban fraction of overall population toward an equilibrium. Such
an equilibrium is characterized by stabilization of rural-urban expected wages
differential (generalized Harris-Todaro equilibrium condition), urban
concentration and urban unemployment. These classic results obtained originally
by Harris and Todaro are emergent properties of our model.",
cross-border merger wages,http://arxiv.org/abs/1708.01876v1,The Countries' Relation Formation Problem: I and II,"This paper integrates the studies of various countries' behaviors, e.g.,
waging wars and entering into military alliances, into a general framework of
\emph{countries' relation formation}, which consists of two components, i.e., a
static game and a dynamical system. Aside from being a stand-alone framework
itself, this paper can also be seen as a necessary extension of a recently
developed \emph{countries' power allocation game} in \cite{allocation}. We
establish certain theoretical results, such as pure strategy Nash equilibrium
existence in the static game, and propose several applications of interest made
possible by combining both frameworks of countries' power allocation and
relation formation.",
cross-border merger wages,http://arxiv.org/abs/1803.06563v1,Viewpoint: Artificial Intelligence and Labour,"The welfare of modern societies has been intrinsically linked to wage labour.
With some exceptions, the modern human has to sell her labour-power to be able
reproduce biologically and socially. Thus, a lingering fear of technological
unemployment features predominately as a theme among Artificial Intelligence
researchers. In this short paper we show that, if past trends are anything to
go by, this fear is irrational. On the contrary, we argue that the main problem
humanity will be facing is the normalisation of extremely long working hours.",
cross-border merger wages,http://arxiv.org/abs/1602.01578v2,Modeling the relation between income and commuting distance,"We discuss the distribution of commuting distances and its relation to
income. Using data from Denmark, the UK, and the US, we show that the commuting
distance is (i) broadly distributed with a slow decaying tail that can be
fitted by a power law with exponent $\gamma \approx 3$ and (ii) an average
growing slowly as a power law with an exponent less than one that depends on
the country considered. The classical theory for job search is based on the
idea that workers evaluate the wage of potential jobs as they arrive
sequentially through time, and extending this model with space, we obtain
predictions that are strongly contradicted by our empirical findings. We
propose an alternative model that is based on the idea that workers evaluate
potential jobs based on a quality aspect and that workers search for jobs
sequentially across space. We also assume that the density of potential jobs
depends on the skills of the worker and decreases with the wage. The predicted
distribution of commuting distances decays as $1/r^{3}$ and is independent of
the distribution of the quality of jobs. We find our alternative model to be in
agreement with our data. This type of approach opens new perspectives for the
modeling of mobility.",J. R. Soc. Interface 13:20160306 (2016)
cross-border merger wages,http://arxiv.org/abs/1711.07359v2,Approximately Stable Matchings with Budget Constraints,"This paper considers two-sided matching with budget constraints where one
side (firm or hospital) can make monetary transfers (offer wages) to the other
(worker or doctor). In a standard model, while multiple doctors can be matched
to a single hospital, a hospital has a maximum quota: the number of doctors
assigned to a hospital cannot exceed a certain limit. In our model, a hospital
instead has a fixed budget: the total amount of wages allocated by each
hospital to doctors is constrained. With budget constraints, stable matchings
may fail to exist and checking for the existence is hard. To deal with the
nonexistence of stable matchings, we extend the ""matching with contracts"" model
of Hatfield and Milgrom, so that it handles approximately stable matchings
where each of the hospitals' utilities after deviation can increase by factor
up to a certain amount. We then propose two novel mechanisms that efficiently
return such a stable matching that exactly satisfies the budget constraints. In
particular, by sacrificing strategy-proofness, our first mechanism achieves the
best possible bound. Furthermore, we find a special case such that a simple
mechanism is strategy-proof for doctors, keeping the best possible bound of the
general case.",
cross-border merger wages,http://arxiv.org/abs/1710.06285v2,"Preliminary steps toward a universal economic dynamics for monetary and
  fiscal policy","We consider the relationship between economic activity and intervention,
including monetary and fiscal policy, using a universal dynamic framework.
Central bank policies are designed for growth without excess inflation.
However, unemployment, investment, consumption, and inflation are interlinked.
Understanding dynamics is crucial to assessing the effects of policy,
especially in the aftermath of the financial crisis. Here we lay out a program
of research into monetary and economic dynamics and preliminary steps toward
its execution. We use principles of response theory to derive implications for
policy. We find that the current approach, which considers the overall money
supply, is insufficient to regulate economic growth. While it can achieve some
degree of control, optimizing growth also requires a fiscal policy balancing
monetary injection between two dominant loop flows, the consumption and wages
loop, and investment and returns loop. The balance arises from a composite of
government tax, entitlement, subsidy policies, corporate policies, as well as
monetary policy. We show empirically that a transition occurred in 1980 between
two regimes--an oversupply to the consumption and wages loop, to an oversupply
of the investment and returns loop. The imbalance is manifest in savings and
borrowing by consumers and investors, and in inflation. The latter increased
until 1980, and decreased subsequently, resulting in a zero rate largely
unrelated to the financial crisis. Three recessions and the financial crisis
are part of this dynamic. Optimizing growth now requires shifting the balance.
Our analysis supports advocates of greater income and / or government support
for the poor who use a larger fraction of income for consumption. This promotes
investment due to growth in demand. Otherwise, investment opportunities are
limited, capital remains uninvested, and does not contribute to growth.",
cross-border merger wages,http://arxiv.org/abs/1011.0534v1,Merger Dynamics in Three-Agent Games,"We present the effect of mergers in the dynamics of the three-agent model
studied by Ben-Naim, Kahng and Kim and by Rador and Mungan. Mergers are
possible in three-agent games because two agents can combine forces against the
third player and thus increase their probability to win a competition. We
implement mergers in this three-agent model via resolving merger and no-merger
units of competition in terms of a two-agent unit. This way one needs only a
single parameter which we have called the competitiveness parameter. We have
presented an analytical solution in the fully competitive limit. In this limit
the score distribution of agents is stratified and self-similar.","Eur. Phys. J. B 83, 289 - 299, 2011"
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0108014v1,"What's Fit To Print: The Effect Of Ownership Concentration On Product
  Variety In Daily Newspaper Markets","This paper examines the effect of ownership concentration on product
position, product variety and readership in markets for daily newspapers. US
antitrust policy presumes that mergers reduce the amount and diversity of
content available to consumers. However, the effects of consolidation in
differentiated product markets cannot be determined solely from theory. Because
multi-product firms internalize business stealing, mergers may encourage firms
to reposition products, leading to more, not less, variety. Using data on
reporter assignments from 1993-1999, results show that differentiation and
variety increase with concentration. Moreover, there is evidence that
additional variety increases readership, suggesting that concentration benefits
consumers.",
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0109041v2,Open Access beyond cable: The case of Interactive TV,"In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented ""walled gardens"" offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone",
cross-border merger market campitalisation,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0109048v1,"Competition and Commons: The Post-Telecom Act Public Interest, in and
  after the AOLTW Merger","In asserting a competitive market environment as a justification for
regulatory forbearance, the Telecommunications Act of 1996 finally articulated
a clear standard for the FCC's public interest standard, one of the most
protean concepts in communications. This seeming clarity has not, however,
inhibited intense political conflict over the term. This paper examines public
and regulatory debate over the AOL Time Warner merger as an example of the way
in which the linkage between competitions and commons policy becomes relevant
to communications policy, particularly in relation to mass media, and discusses
interpretations of the public interest in the current FCC. The paper proposes
that the Telecom Act's goal of fostering economic competition among information
service providers, and the democratic ideal of nurturing public relationships
and behaviors can be linked. Competition policy that creates the opportunity
for untrammeled interactivity also provides a sine qua non to nurture the
social phenomenon of the commons. The linked concepts of competition and
commons could also provide useful ways to interpret the public interest in
policy arenas as spectrum allocation and intellectual property.",
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0109111v2,"Quality of service monitoring: Performance metrics across proprietary
  content domains","We propose a quality of service (QoS) monitoring program for broadband access
to measure the impact of proprietary network spaces. Our paper surveys other
QoS policy initiatives, including those in the airline, and wireless and
wireline telephone industries, to situate broadband in the context of other
markets undergoing regulatory devolution. We illustrate how network
architecture can create impediments to open communications, and how QoS
monitoring can detect such effects. We present data from a field test of
QoS-monitoring software now in development. We suggest QoS metrics to gauge
whether information ""walled gardens"" represent a real threat for dividing the
Internet into proprietary spaces.
  To demonstrate our proposal, we are placing our software on the computers of
a sample of broadband subscribers. The software periodically conducts a battery
of tests that assess the quality of connections from the subscriber's computer
to various content sites. Any systematic differences in connection quality
between affiliated and non-affiliated content sites would warrant research into
the behavioral implications of those differences.
  QoS monitoring is timely because the potential for the Internet to break into
a loose network of proprietary content domains appears stronger than ever.
Recent court rulings and policy statements suggest a growing trend towards
relaxed scrutiny of mergers and the easing or elimination of content ownership
rules. This policy environment could lead to a market with a small number of
large, vertically integrated network operators, each pushing its proprietary
content on subscribers.",
cross-border merger market campitalisation,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
cross-border merger market campitalisation,http://arxiv.org/abs/1505.03305v1,Transformation of marketing in the e-commerce,"The article is about transformation of the theory and practice of marketing
in the conditions of e-commerce and network economy. The author considers
Internet-marketing as an independent kind of marketing in the virtual
communicative environment. The basic thesis of the article: the virtual
environment defines marketing transformation, changing methods, priorities and
structure at practice and then theories of marketing.",Practical marketing. 2013. No 1 (191). P. 4-16
cross-border merger market campitalisation,http://arxiv.org/abs/physics/0509090v2,Effects of the globalization in the Korean financial markets,"We study the effect of globalization on the Korean market, one of the
emerging markets. Some characteristics of the Korean market are different from
those of the mature market according to the latest market data, and this is due
to the influence of foreign markets or investors. We concentrate on the market
network structures over the past two decades with knowledge of the history of
the market, and determine the globalization effect and market integration as a
function of time.","J. Korean Phys. Soc. 48, pp.S135-S138 (2006)."
cross-border merger market campitalisation,http://arxiv.org/abs/physics/0512210v1,"Micro-economic Analysis of the Physical Constrained Markets: Game Theory
  Application to Competitive Electricity Markets","Competition has been introduced in the electricity markets with the goal of
reducing prices and improving efficiency. The basic idea which stays behind
this choice is that, in competitive markets, a greater quantity of the good is
exchanged at a lower and a lower price, leading to higher market efficiency.
Electricity markets are pretty different from other commodities mainly due to
the physical constraints related to the network structure that may impact the
market performance. The network structure of the system on which the economic
transactions need to be undertaken poses strict physical and operational
constraints. Strategic interactions among producers that game the market with
the objective of maximizing their producer surplus must be taken into account
when modeling competitive electricity markets. The physical constraints,
specific of the electricity markets, provide additional opportunity of gaming
to the market players. Game theory provides a tool to model such a context.
This paper discussed the application of game theory to physical constrained
electricity markets with the goal of providing tools for assessing the market
performance and pinpointing the critical network constraints that may impact
the market efficiency. The basic models of game theory specifically designed to
represent the electricity markets will be presented. IEEE30 bus test system of
the constrained electricity market will be discussed to show the network
impacts on the market performances in presence of strategic bidding behavior of
the producers.",
cross-border merger market campitalisation,http://arxiv.org/abs/1505.03303v1,Dropshipping - alternative infrastructure of sales and promotion,"An article about the transformation of the theory and practice of marketing
in terms of e-commerce and network economy. The author considers Internet
Marketing as an independent marketing communication in a virtual environment.
The main thesis of the article: virtual environment determines the
transformation of marketing, changing methods, priorities and structure not
only practice, but also the theory of marketing.",Marketing in Russia and Abroad. 2012. No 1 (87). P. 90-104
cross-border merger market campitalisation,http://arxiv.org/abs/1003.0034v1,A New Understanding of Prediction Markets Via No-Regret Learning,"We explore the striking mathematical connections that exist between market
scoring rules, cost function based prediction markets, and no-regret learning.
We show that any cost function based prediction market can be interpreted as an
algorithm for the commonly studied problem of learning from expert advice by
equating trades made in the market with losses observed by the learning
algorithm. If the loss of the market organizer is bounded, this bound can be
used to derive an O(sqrt(T)) regret bound for the corresponding learning
algorithm. We then show that the class of markets with convex cost functions
exactly corresponds to the class of Follow the Regularized Leader learning
algorithms, with the choice of a cost function in the market corresponding to
the choice of a regularizer in the learning problem. Finally, we show an
equivalence between market scoring rules and prediction markets with convex
cost functions. This implies that market scoring rules can also be interpreted
naturally as Follow the Regularized Leader algorithms, and may be of
independent interest. These connections provide new insight into how it is that
commonly studied markets, such as the Logarithmic Market Scoring Rule, can
aggregate opinions into accurate estimates of the likelihood of future events.",
cross-border merger market campitalisation,http://arxiv.org/abs/1206.5252v1,A Utility Framework for Bounded-Loss Market Makers,"We introduce a class of utility-based market makers that always accept orders
at their risk-neutral prices. We derive necessary and sufficient conditions for
such market makers to have bounded loss. We prove that hyperbolic absolute risk
aversion utility market makers are equivalent to weighted pseudospherical
scoring rule market makers. In particular, Hanson's logarithmic scoring rule
market maker corresponds to a negative exponential utility market maker in our
framework. We describe a third equivalent formulation based on maintaining a
cost function that seems most natural for implementation purposes, and we
illustrate how to translate among the three equivalent formulations. We examine
the tradeoff between the market's liquidity and the market maker's worst-case
loss. For a fixed bound on worst-case loss, some market makers exhibit greater
liquidity near uniform prices and some exhibit greater liquidity near extreme
prices, but no market maker can exhibit uniformly greater liquidity in all
regimes. For a fixed minimum liquidity level, we give the lower bound of market
maker's worst-case loss under some regularity conditions.",
cross-border merger market campitalisation,http://arxiv.org/abs/1406.7584v1,"Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of
  Anchoring ""Bias""","In an online prediction market, forecasters who could not see the current
state of the market until they made their own separate estimates moved their
estimates closer to the market forecast when the current state of the market
became known. Their first edits to the market forecast were very similar to the
first edits of forecasters who could always see the current state of the
market, and forecasters in both conditions had similar accuracy. These results
suggest that our more elaborate forecast elicitation method might not improve
forecasts and that any anchoring on the state of the market does not constitute
an error in judgment.",
cross-border merger market campitalisation,http://arxiv.org/abs/0806.4466v1,Klein - Gordon equation for market wealth operations,"In this paper the modified Klein - Gordon equation for market processes is
proposed and solved. It is argued that the oscillations in market propagate
with the light velocity. The initial pulse in the market is damped and for very
large time diffused according to the Fourier law.",
cross-border merger market campitalisation,http://arxiv.org/abs/1505.02766v1,Features of transformation of marketing in e-commerce,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.","Omsk Scientific Bulletin. 2013. No 1 (115). P. 55-58. ISSN
  1813-8225"
cross-border merger market campitalisation,http://arxiv.org/abs/physics/0607071v1,Market Polarization in Presence of Individual Choice Volatility,"Financial markets are subject to long periods of polarized behavior, such as
bull-market or bear-market phases, in which the vast majority of market
participants seem to almost exclusively choose one action (between buying or
selling) over the other. From the point of view of conventional economic
theory, such events are thought to reflect the arrival of ``external news''
that justifies the observed behavior. However, empirical observations of the
events leading up to such market phases, as well events occurring during the
lifetime of such a phase, have often failed to find significant correlation
between news from outside the market and the behavior of the agents comprising
the market. In this paper, we explore the alternative hypothesis that the
occurrence of such market polarizations are due to interactions amongst the
agents in the market, and not due to any influence external to it. In
particular, we present a model where the market (i.e., the aggregate behavior
of all the agents) is observed to become polarized even though individual
agents regularly change their actions (buy or sell) on a time-scale much
shorter than that of the market polarization phase.",
cross-border merger market campitalisation,http://arxiv.org/abs/physics/0608016v2,Market Efficiency in Foreign Exchange Markets,"We investigate the relative market efficiency in financial market data, using
the approximate entropy(ApEn) method for a quantification of randomness in time
series. We used the global foreign exchange market indices for 17 countries
during two periods from 1984 to 1998 and from 1999 to 2004 in order to study
the efficiency of various foreign exchange markets around the market crisis. We
found that on average, the ApEn values for European and North American foreign
exchange markets are larger than those for African and Asian ones except Japan.
We also found that the ApEn for Asian markets increase significantly after the
Asian currency crisis. Our results suggest that the markets with a larger
liquidity such as European and North American foreign exchange markets have a
higher market efficiency than those with a smaller liquidity such as the
African and Asian ones except Japan.",
cross-border merger market campitalisation,http://arxiv.org/abs/1703.10689v1,Computing Equilibrium in Matching Markets,"Market equilibria of matching markets offer an intuitive and fair solution
for matching problems without money with agents who have preferences over the
items. Such a matching market can be viewed as a variation of Fisher market,
albeit with rather peculiar preferences of agents. These preferences can be
described by piece-wise linear concave (PLC) functions, which however, are not
separable (due to each agent only asking for one item), are not monotone, and
do not satisfy the gross substitute property-- increase in price of an item can
result in increased demand for the item. Devanur and Kannan in FOCS 08 showed
that market clearing prices can be found in polynomial time in markets with
fixed number of items and general PLC preferences. They also consider Fischer
markets with fixed number of agents (instead of fixed number of items), and
give a polynomial time algorithm for this case if preferences are separable
functions of the items, in addition to being PLC functions.
  Our main result is a polynomial time algorithm for finding market clearing
prices in matching markets with fixed number of different agent preferences,
despite that the utility corresponding to matching markets is not separable. We
also give a simpler algorithm for the case of matching markets with fixed
number of different items.",
cross-border merger market campitalisation,http://arxiv.org/abs/physics/0505032v1,Automated Trading Systems: Developed and Emerging Capital Markets,"Automated trading systems on developed and emerging capital markets are
studied in this paper. The standard for developed market is automated trading
system with 40-days simple moving average. We tested it for the index SIX
Industrial for 1000 and 730 trading days of the slovak emerging capital market.
The Buy and Hold trading system was 7.80 times more profitable than this etalon
trading system for active trading. Taking of profitable standard trading system
from a developed capital market does not lead to optimal results on the
emerging capital markets. We then studied optimized standard trading system
based on the simple moving average. The parameter of optimization was the
number of weeks. An optimal system was that with 5 weeks. This trading system
has some of its characteristics comparable with the etalon trading system on
the NYSE Composite Index. The emerging market is more risky than the developed
market. The profit on the emerging market is also higher. The range of
optimized system parameter is quite robust. Observed was increase of number of
trades in the range from the 21 weeks to the 25 weeks. This indicates creation
of a new optimal middle range trading system. Results of testing for liquid
shares are quantitatively similar.",
cross-border merger market campitalisation,http://arxiv.org/abs/1508.07272v1,"Market Formation as Transitive Closure: the Evolving Pattern of Trade in
  Music","Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.",
cross-border merger market campitalisation,http://arxiv.org/abs/1809.03684v2,"Visual Attention Model for Cross-sectional Stock Return Prediction and
  End-to-End Multimodal Market Representation Learning","Technical and fundamental analysis are traditional tools used to analyze
individual stocks; however, the finance literature has shown that the price
movement of each individual stock correlates heavily with other stocks,
especially those within the same sector. In this paper we propose a general
purpose market representation that incorporates fundamental and technical
indicators and relationships between individual stocks. We treat the daily
stock market as a ""market image"" where rows (grouped by market sector)
represent individual stocks and columns represent indicators. We apply a
convolutional neural network over this market image to build market features in
a hierarchical way. We use a recurrent neural network, with an attention
mechanism over the market feature maps, to model temporal dynamics in the
market. We show that our proposed model outperforms strong baselines in both
short-term and long-term stock return prediction tasks. We also show another
use for our market image: to construct concise and dense market embeddings
suitable for downstream prediction tasks.",
cross-border merger market campitalisation,http://arxiv.org/abs/cs/0109053v1,Price Increases from Online Privacy,"Consumers value keeping some information about them private from potential
marketers. E-commerce dramatically increases the potential for marketers to
accumulate otherwise private information about potential customers. Online
marketers claim that this information enables them to better market their
products. Policy makers are currently drafting rules to regulate the way in
which these marketers can collect, store, and share this information. However,
there is little evidence yet either of consumers' valuation of their privacy or
of the benefits they might reap through better target marketing. We provide a
framework for measuring a portion of the benefits from allowing marketers to
make better use of consumer information. Target marketing is likely to reduce
consumer search costs, improve consumer product selection decisions, and lower
the marketing costs of goods sold. Our model allows us to estimate the value to
consumers of only the latter, price reductions from more efficient marketing.",
cross-border merger market campitalisation,http://arxiv.org/abs/1202.1712v1,Multi-outcome and Multidimensional Market Scoring Rules,"Hanson's market scoring rules allow us to design a prediction market that
still gives useful information even if we have an illiquid market with a
limited number of budget-constrained agents. Each agent can ""move"" the current
price of a market towards their prediction.
  While this movement still occurs in multi-outcome or multidimensional markets
we show that no market-scoring rule, under reasonable conditions, always moves
the price directly towards beliefs of the agents. We present a modified version
of a market scoring rule for budget-limited traders, and show that it does have
the property that, from any starting position, optimal trade by a
budget-limited trader will result in the market being moved towards the
trader's true belief. This mechanism also retains several attractive strategic
properties of the market scoring rule.",
cross-border merger market campitalisation,http://arxiv.org/abs/1603.07210v2,Computing Equilibria in Markets with Budget-Additive Utilities,"We present the first analysis of Fisher markets with buyers that have
budget-additive utility functions. Budget-additive utilities are elementary
concave functions with numerous applications in online adword markets and
revenue optimization problems. They extend the standard case of linear
utilities and have been studied in a variety of other market models. In
contrast to the frequently studied CES utilities, they have a global satiation
point which can imply multiple market equilibria with quite different
characteristics. Our main result is an efficient combinatorial algorithm to
compute a market equilibrium with a Pareto-optimal allocation of goods. It
relies on a new descending-price approach and, as a special case, also implies
a novel combinatorial algorithm for computing a market equilibrium in linear
Fisher markets. We complement these positive results with a number of hardness
results for related computational questions. We prove that it is NP-hard to
compute a market equilibrium that maximizes social welfare, and it is PPAD-hard
to find any market equilibrium with utility functions with separate satiation
points for each buyer and each good.",
cross-border merger market campitalisation,http://arxiv.org/abs/1403.0648v1,"Multi-period Trading Prediction Markets with Connections to Machine
  Learning","We present a new model for prediction markets, in which we use risk measures
to model agents and introduce a market maker to describe the trading process.
This specific choice on modelling tools brings us mathematical convenience. The
analysis shows that the whole market effectively approaches a global objective,
despite that the market is designed such that each agent only cares about its
own goal. Additionally, the market dynamics provides a sensible algorithm for
optimising the global objective. An intimate connection between machine
learning and our markets is thus established, such that we could 1) analyse a
market by applying machine learning methods to the global objective, and 2)
solve machine learning problems by setting up and running certain markets.",
cross-border merger bond yields,http://arxiv.org/abs/1602.02423v1,Catch bond mechanism in Dynein motor driven collective transport,"Recent experiments have demonstrated that dynein motor exhibits catch bonding
behaviour, in which the unbinding rate of a single dynein decreases with
increasing force, for a certain range of force. Motivated by these experiments,
we propose a model for catch bonding in dynein using a threshold force bond
deformation (TFBD) model wherein catch bonding sets in beyond a critical
applied load force. We study the effect of catch bonding on unidirectional
transport properties of cellular cargo carried by multiple dynein motors within
the framework of this model. We find catch bonding can result in dramatic
changes in the transport properties, which are in sharp contrast to kinesin
driven unidirectional transport, where catch bonding is absent. We predict
that, under certain conditions, the average velocity of the cellular cargo can
actually increase as applied load is increased. We characterize the transport
properties in terms of a velocity profile phase plot in the parameter space of
the catch bond strength and the stall force of the motor. This phase plot
yields predictions that may be experimentally accessed by suitable
modifications of motor transport and binding properties. Our work necessitates
a reexamination of existing theories of collective bidirectional transport of
cellular cargo where the catch bond effect of dynein described in this paper is
expected to play a crucial role.",
cross-border merger bond yields,http://arxiv.org/abs/1808.02951v1,"Constructing a Non-additive Non-interacting Kinetic Energy Functional
  Approximation for Covalent Bonds from Exact Conditions","We present a non-decomposable approximation for the non-additive
non-interacting kinetic energy (NAKE) for covalent bonds based on the exact
behavior of the von Weizs\""{a}cker (vW) functional in regions dominated by one
orbital. This covalent approximation (CA) seamlessly combines the vW and the
Thomas-Fermi (TF) functional with a switching function of the fragment
densities constructed to satisfy exact constraints. It also makes use of
ensembles and fractionally-occupied spin-orbitals to yield highly accurate NAKE
for stretched bonds while outperforming other standard NAKE approximations near
equilibrium bond lengths. We tested the CA within Partition-Density Functional
Theory (P-DFT) and demonstrated its potential to enable fast and accurate P-DFT
calculations.",
cross-border merger bond yields,http://arxiv.org/abs/1407.1998v1,"Wave-function and density functional theory studies of dihydrogen
  complexes","We performed a benchmark study on a series of dihydrogen bond complexes and
constructed a set of reference bond distances and interaction energies. The
test set was employed to assess the performance of several wave-function
correlated and density functional theory methods. We found that second-order
correlation methods describe relatively well the dihydrogen complexes. However,
for high accuracy inclusion of triple contributions is important. On the other
hand, none of the considered density functional methods can simultaneously
yield accurate bond lengths and interaction energies. However, we found that
improved results can be obtained by the inclusion of non-local exchange
contributions.","J. Chem. Theory Comput. 10, 3151 (2014)"
cross-border merger bond yields,http://arxiv.org/abs/physics/0507098v1,Ab initio yield curve dynamics,"We derive an equation of motion for interest-rate yield curves by applying a
minimum Fisher information variational approach to the implied probability
density. By construction, solutions to the equation of motion recover observed
bond prices. More significantly, the form of the resulting equation explains
the success of the Nelson Siegel approach to fitting static yield curves and
the empirically observed modal structure of yield curves. A practical numerical
implementation of this equation of motion is found by using the Karhunen-Loeve
expansion and Galerkin's method to formulate a reduced-order model of yield
curve dynamics.",
cross-border merger bond yields,http://arxiv.org/abs/1311.2467v2,Funding the Search for Extraterrestrial Intelligence with a Lottery Bond,"I propose the establishment of a SETI Lottery Bond to provide a continued
source of funding for the search for extraterrestrial intelligence (SETI). The
SETI Lottery Bond is a fixed rate perpetual bond with a lottery at maturity,
where maturity occurs only upon discovery and confirmation of extraterrestrial
intelligent life. Investors in the SETI Lottery Bond purchase shares that yield
a fixed rate of interest that continues indefinitely until SETI succeeds---at
which point a random subset of shares will be awarded a prize from a lottery
pool. SETI Lottery Bond shares also are transferable, so that investors can
benefact their shares to kin or trade them in secondary markets. The total
capital raised this way will provide a fund to be managed by a financial
institution, with annual payments from this fund to support SETI research, pay
investor interest, and contribute to the lottery fund. Such a plan could
generate several to tens of millions of dollars for SETI research each year,
which would help to revitalize and expand facilities such as the Allen
Telescope Array. The SETI Lottery Bond is a savings product that only can be
offered by a financial institution with authorization to engage in banking and
gaming activities. I therefore suggest that one or more banks offer a
lottery-linked savings product in support of SETI research, with the added
benefit of promoting personal savings and intergenerational wealth building
among individuals.",
cross-border merger bond yields,http://arxiv.org/abs/0903.3502v1,"QM/MM Simulation of the Hydrogen Bond Dynamics of an Adenine:Uracil Base
  Pair in Solution. Geometric Correlations and Infrared Spectrum","Hybrid QM(DFT)/MM molecular dynamics simulations have been carried out for
the Watson-Crick base pair of 9-ethyl-8-phenyladenine and 1-cyclohexyluracil in
deuterochloroform solution at room temperature.
  Trajectories are analyzed putting special attention to the geometric
correlations of the $\NHN$ and $\NHO$ hydrogen bonds in the base pair. Further,
based on empirical correlations between the hydrogen bond bond length and the
fundamental NH stretching frequency its fluctuations are obtained along the
trajectory. Using the time dependent frequencies the infrared lineshape is
determined assuming the validity of a second order cumulant expansion. The
deviations for the fundamental transition frequencies are calculated to amount
to less than 2% as compared with experiment. The width of the spectrum for the
$\NHN$ bond is in reasonable agreement with experiment while that for the
$\NHO$ case is underestimated by the present model. Comparing the performance
of different pseudopotentials it is found that the Troullier-Martins
pseudopotential with a 70 Ry cut-off yields the best agreement.",
cross-border merger bond yields,http://arxiv.org/abs/1607.05249v6,"Performance of a Nonempirical Density Functional on Molecules and
  Hydrogen-Bonded Complexes","Recently, Tao and Mo (TM) derived a meta-generalized gradient approximation
functional based on a model exchange-correlation hole. In this work, the
performance of this functional is assessed on standard test sets, using the
6-311++G(3df,3pd) basis set. These test sets include 223 G3/99 enthalpies of
formation, 99 atomization energies, 76 barrier heights, 58 electron affinities,
8 proton affinities, 96 bond lengths, 82 harmonic vibrational frequencies, 10
hydrogen-bonded molecular complexes, and 22 atomic excitation energies. Our
calculations show that the TM functional can achieve high accuracy for most
properties considered, relative to the LSDA, PBE, and TPSS functionals. In
particular, it yields the best accuracy for proton affinities, harmonic
vibrational frequencies, hydrogen-bonded dissociation energies and bond
lengths, and atomic excitation energies.",
cross-border merger bond yields,http://arxiv.org/abs/1902.06528v1,A local perspective on conjugation of double bonds in acyclic polyenes,"The study is devoted to elaboration of an alternative image of conjugation in
acyclic polyenes as a weak and essentially local delocalization of
initially-localized pairs of electrons ascribed to individual double bonds
(instead of formation of a completely delocalized electron system as usual). To
this end, polyenes are modelled as sets of weakly interacting formally-double
bonds, where the single bonds represent the interaction between the former and
are treated as a perturbation. Mathematically, the above-formulated goal is
realized by means of a particular version of the non-canonical method of
molecular orbitals (MOs) based on the Brillouin theorem and yielding the
expressions both for total energies and for non-canonical (localized) MOs
(NCMOs) directly without any reference to usual (canonical) MOs. In addition,
total energies and NCMOs are interrelated explicitly in the approach applied,
viz. the former are representable via the so-called delocalization coefficients
of the latter. Adaptation of these general results to the above-specified model
of polyene yields coincidence between the conjugation energy (CE) and the total
delocalization energy of all pairs of electrons contained. Moreover, a local
relation follows between constitution of the nearest environment of a certain
bond, delocalization pattern of the respective pair of electrons and
contribution of just this pair to the total CE of the given polyene. As a
result, different stabilities of distinct polyenes (e.g. of isomers) prove to
be accompanied by variable extents of delocalization of separate pairs of
electrons. Linear and cross-conjugated polyene chains are comparatively
analyzed.",
cross-border merger bond yields,http://arxiv.org/abs/1505.04976v1,"Coherent control of bond making: The performance of rationally
  phase-shaped femtosecond laser pulses","The first step in the coherent control of a photoinduced binary reaction is
bond making or photoassociation. We have recently demonstrated coherent control
of bond making in multi-photon femtosecond photoassociation of hot magnesium
atoms, using linearly chirped pulses [Levin et al., arXiv:1411.1542]. The
detected yield of photoassociated magnesium dimers was enhanced by positively
chirped pulses which is explained theoretically by a combination of
purification and chirp-dependent Raman transitions. The yield could be further
enhanced by pulse optimization resulting in pulses with an effective linear
chirp and a sub-pulse structure, where the latter allows for exploiting
vibrational coherences. Here, we systematically explore the efficiency of
phase-shaped pulses for the coherent control of bond making, employing a
parametrization of the spectral phases in the form of cosine functions. We find
up to an order of magnitude enhancement of the yield compared to the unshaped
transform-limited pulse. The highly performing pulses all display an overall
temporally increasing instantaneous frequency and are composed of several
overlapping sub-pulses. The time delay between the first two sub-pulses almost
perfectly fits the vibrational frequency of the generated intermediate
wavepacket.These findings are in agreement with chirp-dependent Raman
transitions and exploitation of vibrational dynamics as underlying control
mechanisms.","J. Phys. B 48, 184004 (2015)"
cross-border merger transaction costs,http://arxiv.org/abs/1702.08478v1,"Risks and Transaction Costs of Distributed-Ledger Fintech: Boundary
  Effects and Consequences","Fintech business models based on distributed ledgers -- and their
smart-contract variants in particular -- offer the prospect of democratizing
access to faster, anywhere-accessible, lower cost, reliable-and-secure
high-quality financial services. In addition to holding great, economically
transformative promise, these business models pose new, little-studied risks
and transaction costs. However, these risks and transaction costs are not
evident during the demonstration and testing phases of development, when
adopters and users are drawn from the community of developers themselves, as
well as from among non-programmer fintech evangelists. Hence, when the new
risks and transaction costs become manifest -- as the fintech business models
are rolled out across the wider economy -- the consequences may also appear to
be new and surprising. The present study represents an effort to get ahead of
these developments by delineating risks and transaction costs inherent in
distributed-ledger- and smart-contracts-based fintech business models. The
analysis focuses on code risk and moral-hazard risk, as well as on
mixed-economy risks and the unintended consequences of replicating
bricks-and-mortar-generation contract forms within the ultra-low
transaction-cost environment of fintech.",
cross-border merger transaction costs,http://arxiv.org/abs/1409.6771v1,Mitigation of Delayed Management Costs in Transaction-Oriented Systems,"Abundant examples of complex transaction-oriented networks (TONs) can be
found in a variety of disciplines, including information and communication
technology, finances, commodity trading, and real estate. A transaction in a
TON is executed as a sequence of subtransactions associated with the network
nodes, and is committed if every subtransaction is committed. A subtransaction
incurs a two-fold overhead on the host node: the fixed transient operational
cost and the cost of long-term management (e.g. archiving and support) that
potentially grows exponentially with the transaction length. If the overall
cost exceeds the node capacity, the node fails and all subtransaction incident
to the node, and their parent distributed transactions, are aborted. A TON
resilience can be measured in terms of either external workloads or intrinsic
node fault rates that cause the TON to partially or fully choke. We demonstrate
that under certain conditions, these two measures are equivalent. We further
show that the exponential growth of the long-term management costs can be
mitigated by adjusting the effective operational cost: in other words, that the
future maintenance costs could be absorbed into the transient operational
costs.",
cross-border merger transaction costs,http://arxiv.org/abs/1604.00103v3,Effect of Bitcoin fee on transaction-confirmation process,"In Bitcoin system, transactions are prioritized according to transaction
fees. Transactions without fees are given low priority and likely to wait for
confirmation. Because the demand of micro payment in Bitcoin is expected to
increase due to low remittance cost, it is important to quantitatively
investigate how transactions with small fees of Bitcoin affect the
transaction-confirmation time. In this paper, we analyze the
transaction-confirmation time by queueing theory. We model the
transaction-confirmation process of Bitcoin as a priority queueing system with
batch service, deriving the mean transaction-confirmation time. Numerical
examples show how the demand of transactions with low fees affects the
transaction-confirmation time. We also consider the effect of the maximum block
size on the transaction-confirmation time.",
cross-border merger transaction costs,http://arxiv.org/abs/1905.00553v1,Empirically Analyzing Ethereum's Gas Mechanism,"Ethereum's Gas mechanism attempts to set transaction fees in accordance with
the computational cost of transaction execution: a cost borne by default by
every node on the network to ensure correct smart contract execution. Gas
encourages users to author transactions that are efficient to execute and in so
doing encourages node diversity, allowing modestly resourced nodes to join and
contribute to the security of the network.
  However, the effectiveness of this scheme relies on Gas costs being correctly
aligned with observed computational costs in reality. In this work, we
performed the first large scale empirical study to understand to what degree
this alignment exists in practice, by collecting and analyzing Tera-bytes worth
of nanosecond-precision transaction execution traces. Besides confirming
potential denial-of-service vectors, our results also shed light on the role of
I/O in transaction costs which remains poorly captured by the current Gas cost
model. Finally, our results suggest that under the current Gas cost model,
nodes with modest computational resources are disadvantaged compared to their
better resourced peers, which we identify as an ongoing threat to node
diversity and network decentralization.",
cross-border merger transaction costs,http://arxiv.org/abs/1407.6876v2,On Partial Wait-Freedom in Transactional Memory,"Transactional memory (TM) is a convenient synchronization tool that allows
concurrent threads to declare sequences of instructions on shared data as
speculative \emph{transactions} with ""all-or-nothing"" semantics. It is known
that dynamic transactional memory cannot provide \emph{wait-free} progress in
the sense that every transaction commits in a finite number of its own steps.
In this paper, we explore the costs of providing wait-freedom to only a
\emph{subset} of transactions. Since most transactional workloads are believed
to be read-dominated, we require that read-only transactions commit in the
wait-free manner, while updating transactions are guaranteed to commit only if
they run in the absence of concurrency. We show that this kind of partial
wait-freedom, combined with attractive requirements like read invisibility or
disjoint-access parallelism, incurs considerable complexity costs.",
cross-border merger transaction costs,http://arxiv.org/abs/physics/0003054v2,A new volatility term in the theory of options with transaction costs,"The introduction of transaction costs into the theory of option pricing could
lead not only to the change of return for options, but also to the change of
the volatility. On the base of assumption of the portfolio analysis, a new
equation for option pricing with transaction costs is derived. A new solution
for the option price is obtained for the time close to expiration date.",
cross-border merger transaction costs,http://arxiv.org/abs/1811.12204v1,"Chiller: Contention-centric Transaction Execution and Data Partitioning
  for Fast Networks","Distributed transactions on high-overhead TCP/IP-based networks were
conventionally considered to be prohibitively expensive and thus were avoided
at all costs. To that end, the primary goal of almost any existing partitioning
scheme is to minimize the number of cross-partition transactions. However, with
the next generation of fast RDMA-enabled networks, this assumption is no longer
valid. In fact, recent work has shown that distributed databases can scale even
when the majority of transactions are cross-partition.
  In this paper, we first make the case that the new bottleneck which hinders
truly scalable transaction processing in modern RDMA-enabled databases is data
contention, and that optimizing for data contention leads to different
partitioning layouts than optimizing for the number of distributed
transactions. We then present Chiller, a new approach to data partitioning and
transaction execution, which minimizes data contention for both local and
distributed transactions. Finally, we evaluate Chiller using TPC-C and a
real-world workload, and show that our partitioning and execution strategy
outperforms traditional partitioning techniques which try to avoid distributed
transactions, by up to a factor of 2 under the same conditions.",
cross-border merger transaction costs,http://arxiv.org/abs/1711.07617v2,Dynamic Distributed Storage for Scaling Blockchains,"Blockchain uses the idea of storing transaction data in the form of a
distributed ledger wherein each node in the network stores a current copy of
the sequence of transactions in the form of a hash chain. This requirement of
storing the entire ledger incurs a high storage cost that grows undesirably
large for high transaction rates and large networks. In this work we use the
ideas of secret key sharing, private key encryption, and distributed storage to
design a coding scheme such that each node stores only a part of the entire
transaction thereby reducing the storage cost to a fraction of its original
cost. When further using dynamic zone allocation, we show the coding scheme can
also improve the integrity of the transaction data in the network over current
schemes. Further, block validation (bitcoin mining) consumes a significant
amount of energy as it is necessary to determine a hash value satisfying a
specific set of constraints; we show that using dynamic distributed storage
reduces these energy costs.",
cross-border merger transaction costs,http://arxiv.org/abs/1703.02722v1,"Scaling Distributed Transaction Processing and Recovery based on
  Dependency Logging","DGCC protocol has been shown to achieve good performance on multi-core
in-memory system. However, distributed transactions complicate the dependency
resolution, and therefore, an effective transaction partitioning strategy is
essential to reduce expensive multi-node distributed transactions. During
failure recovery, log must be examined from the last checkpoint onwards and the
affected transactions are re-executed based on the way they are partitioned and
executed. Existing approaches treat both transaction management and recovery as
two separate problems, even though recovery is dependent on the sequence in
which transactions are executed.
  In this paper, we propose to treat the transaction management and recovery
problems as one. We first propose an efficient Distributed Dependency Graph
based Concurrency Control (DistDGCC) protocol for handling transactions
spanning multiple nodes, and propose a new novel and efficient logging protocol
called Dependency Logging that also makes use of dependency graphs for
efficient logging and recovery. DistDGCC optimizes the average cost for each
distributed transaction by processing transactions in batch. Moreover, it also
reduces the effects of thread blocking caused by distributed transactions and
consequently improves the runtime performance. Further, dependency logging
exploits the same data structure that is used by DistDGCC to reduce the logging
overhead, as well as the logical dependency information to improve the recovery
parallelism. Extensive experiments are conducted to evaluate the performance of
our proposed technique against state-of-the-art techniques. Experimental
results show that DistDGCC is efficient and scalable, and dependency logging
supports fast recovery with marginal runtime overhead. Hence, the overall
system performance is significantly improved as a result.",
cross-border merger transaction costs,http://arxiv.org/abs/1603.00542v1,Repairing Conflicts among MVCC Transactions,"The optimistic variants of MVCC (Multi-Version Concurrency Control) avoid
blocking concurrent transactions at the cost of having a validation phase. Upon
failure in the validation phase, the transaction is usually aborted and
restarted from scratch. The ""abort and restart"" approach becomes a performance
bottleneck for the use cases with high contention objects or long running
transactions. In addition, restarting from scratch creates a negative feedback
loop in the system, because the system incurs additional overhead that may
create even further conflicts.
  In this paper, we propose a novel approach for conflict resolution in MVCC
for in-memory databases. This low overhead approach summarizes the transaction
programs in the form of a dependency graph. The dependency graph also contains
the constructs used in the validation phase of the MVCC algorithm. Then, in the
case of encountering conflicts among transactions, the conflict locations in
the program are quickly detected, and the conflicting transactions are
partially re-executed. This approach maximizes the reuse of the computations
done in the initial execution round, and increases the transaction processing
throughput.",
cross-border merger transaction costs,http://arxiv.org/abs/1712.07564v2,"Transaction Propagation on Permissionless Blockchains: Incentive and
  Routing Mechanisms","Existing permissionless blockchain solutions rely on peer-to-peer propagation
mechanisms, where nodes in a network transfer transaction they received to
their neighbors. Unfortunately, there is no explicit incentive for such
transaction propagation. Therefore, existing propagation mechanisms will not be
sustainable in a fully decentralized blockchain with rational nodes. In this
work, we formally define the problem of incentivizing nodes for transaction
propagation. We propose an incentive mechanism where each node involved in the
propagation of a transaction receives a share of the transaction fee. We also
show that our proposal is Sybil-proof. Furthermore, we combine the incentive
mechanism with smart routing to reduce the communication and storage costs at
the same time. The proposed routing mechanism reduces the redundant transaction
propagation from the size of the network to a factor of average shortest path
length. The routing mechanism is built upon a specific type of consensus
protocol where the round leader who creates the transaction block is known in
advance. Note that our routing mechanism is a generic one and can be adopted
independently from the incentive mechanism.",
cross-border merger transaction costs,http://arxiv.org/abs/physics/0603013v1,Stock mechanics: unification with economy,"Associating stock mechanics to real economy, in terms of volume, number of
transactions, and cost, i.e. money flow for shares, we obtained the fundamental
laws of stock mechanics.",
cross-border merger transaction costs,http://arxiv.org/abs/1103.1302v9,On the Cost of Concurrency in Transactional Memory,"The crux of software transactional memory (STM) is to combine an easy-to-use
programming interface with an efficient utilization of the concurrent-computing
abilities provided by modern machines. But does this combination come with an
inherent cost? We evaluate the cost of concurrency by measuring the amount of
expensive synchronization that must be employed in an STM implementation that
ensures positive concurrency, i.e., allows for concurrent transaction
processing in some executions. We focus on two popular progress conditions that
provide positive concurrency: progressiveness and permissiveness. We show that
in permissive STMs, providing a very high degree of concurrency, a transaction
performs a linear number of expensive synchronization patterns with respect to
its read-set size. In contrast, progressive STMs provide a very small degree of
concurrency but, as we demonstrate, can be implemented using at most one
expensive synchronization pattern per transaction. However, we show that even
in progressive STMs, a transaction has to ""protect"" (e.g., by using locks or
strong synchronization primitives) a linear amount of data with respect to its
write-set size. Our results suggest that looking for high degrees of
concurrency in STM implementations may bring a considerable synchronization
cost.",
cross-border merger transaction costs,http://arxiv.org/abs/1502.04908v2,Progressive Transactional Memory in Time and Space,"Transactional memory (TM) allows concurrent processes to organize sequences
of operations on shared \emph{data items} into atomic transactions. A
transaction may commit, in which case it appears to have executed sequentially
or it may \emph{abort}, in which case no data item is updated.
  The TM programming paradigm emerged as an alternative to conventional
fine-grained locking techniques, offering ease of programming and
compositionality. Though typically themselves implemented using locks, TMs hide
the inherent issues of lock-based synchronization behind a nice transactional
programming interface.
  In this paper, we explore inherent time and space complexity of lock-based
TMs, with a focus of the most popular class of \emph{progressive} lock-based
TMs. We derive that a progressive TM might enforce a read-only transaction to
perform a quadratic (in the number of the data items it reads) number of steps
and access a linear number of distinct memory locations, closing the question
of inherent cost of \emph{read validation} in TMs. We then show that the total
number of \emph{remote memory references} (RMRs) that take place in an
execution of a progressive TM in which $n$ concurrent processes perform
transactions on a single data item might reach $\Omega(n \log n)$, which
appears to be the first RMR complexity lower bound for transactional memory.",
cross-border merger transaction costs,http://arxiv.org/abs/1709.04284v1,"Accelerating Analytical Processing in MVCC using Fine-Granular
  High-Frequency Virtual Snapshotting","Efficient transactional management is a delicate task. As systems face
transactions of inherently different types, ranging from point updates to long
running analytical computations, it is hard to satisfy their individual
requirements with a single processing component. Unfortunately, most systems
nowadays rely on such a single component that implements its parallelism using
multi-version concurrency control (MVCC). While MVCC parallelizes short-running
OLTP transactions very well, it struggles in the presence of mixed workloads
containing long-running scan-centric OLAP queries, as scans have to work their
way through large amounts of versioned data. To overcome this problem, we
propose a system, which reintroduces the concept of heterogeneous transaction
processing: OLAP transactions are outsourced to run on separate (virtual)
snapshots while OLTP transactions run on the most recent representation of the
database. Inside both components, MVCC ensures a high degree of concurrency.
The biggest challenge of such a heterogeneous approach is to generate the
snapshots at a high frequency. Previous approaches heavily suffered from the
tremendous cost of snapshot creation. In our system, we overcome the
restrictions of the OS by introducing a custom system call vm_snapshot, that is
hand-tailored to our precise needs: it allows fine-granular snapshot creation
at very high frequencies, rendering the snapshot creation phase orders of
magnitudes faster than state-of-the-art approaches. Our experimental evaluation
on a heterogeneous workload based on TPC-H transactions and handcrafted OLTP
transactions shows that our system enables significantly higher analytical
transaction throughputs on mixed workloads than homogeneous approaches. In this
sense, we introduce a system that accelerates Analytical processing by
introducing custom Kernel functionalities: AnKerDB.",
cross-border merger transaction costs,http://arxiv.org/abs/1907.02669v1,On the Cost of Concurrency in Hybrid Transactional Memory,"State-of-the-art \emph{software transactional memory (STM)} implementations
achieve good performance by carefully avoiding the overhead of
\emph{incremental validation} (i.e., re-reading previously read data items to
avoid inconsistency) while still providing \emph{progressiveness} (allowing
transactional aborts only due to \emph{data conflicts}). Hardware transactional
memory (HTM) implementations promise even better performance, but offer no
progress guarantees. Thus, they must be combined with STMs, leading to
\emph{hybrid} TMs (HyTMs) in which hardware transactions must be
\emph{instrumented} (i.e., access metadata) to detect contention with software
transactions.
  We show that, unlike in progressive STMs, software transactions in
progressive HyTMs cannot avoid incremental validation. In fact, this result
holds even if hardware transactions can \emph{read} metadata
\emph{non-speculatively}. We then present \emph{opaque} HyTM algorithms
providing \emph{progressiveness for a subset of transactions} that are optimal
in terms of hardware instrumentation. We explore the concurrency vs. hardware
instrumentation vs. software validation trade-offs for these algorithms. Our
experiments with Intel and IBM POWER8 HTMs seem to suggest that (i) the
\emph{cost of concurrency} also exists in practice, (ii) it is important to
implement HyTMs that provide progressiveness for a maximal set of transactions
without incurring high hardware instrumentation overhead or using global
contending bottlenecks and (iii) there is no easy way to derive more efficient
HyTMs by taking advantage of non-speculative accesses within hardware.",
cross-border merger transaction costs,http://arxiv.org/abs/1405.5689v3,Inherent Limitations of Hybrid Transactional Memory,"Several Hybrid Transactional Memory (HyTM) schemes have recently been
proposed to complement the fast, but best-effort, nature of Hardware
Transactional Memory (HTM) with a slow, reliable software backup. However, the
fundamental limitations of building a HyTM with nontrivial concurrency between
hardware and software transactions are still not well understood.
  In this paper, we propose a general model for HyTM implementations, which
captures the ability of hardware transactions to buffer memory accesses, and
allows us to formally quantify and analyze the amount of overhead
(instrumentation) of a HyTM scheme. We prove the following: (1) it is
impossible to build a strictly serializable HyTM implementation that has both
uninstrumented reads and writes, even for weak progress guarantees, and (2)
under reasonable assumptions, in any opaque progressive HyTM, a hardware
transaction must incur instrumentation costs linear in the size of its data
set. We further provide two upper bound implementations whose instrumentation
costs are optimal with respect to their progress guarantees. In sum, this paper
captures for the first time an inherent trade-off between the degree of
concurrency a HyTM provides between hardware and software transactions, and the
amount of instrumentation overhead the implementation must incur.",
cross-border merger transaction costs,http://arxiv.org/abs/1604.02103v1,"Cooperative Planning of Renewable Generations for Interconnected
  Microgrids","We study the renewable energy generations in Hong Kong based on realistic
meteorological data, and find that different renewable sources exhibit diverse
time-varying and location-dependent profiles. To efficiently explore and
utilize the diverse renewable energy generations, we propose a theoretical
framework for the cooperative planning of renewable generations in a system of
interconnected microgrids. The cooperative framework considers the
self-interested behaviors of microgrids, and incorporates both their long-term
investment costs and short-term operational costs over the planning horizon.
Specifically, interconnected microgrids jointly decide where and how much to
deploy renewable energy generations, and how to split the associated investment
cost. We show that the cooperative framework minimizes the overall system cost.
We also design a fair cost sharing method based on Nash bargaining to
incentivize cooperative planning, such that all microgrids will benefit from
cooperative planning. Using realistic data obtained from the Hong Kong
observatory, we validate the cooperative planning framework, and demonstrate
that all microgrids benefit through the cooperation, and the overall system
cost is reduced by 35.9% compared to the noncooperative planning benchmark.","IEEE Transactions on Smart Grid, 7(5), Pages: 2486-2496, 2016"
cross-border inflation,http://arxiv.org/abs/1805.07011v2,A shadowing-based inflation scheme for ensemble data assimilation,"Artificial ensemble inflation is a common technique in ensemble data
assimilation, whereby the ensemble covariance is periodically increased in
order to prevent deviation of the ensemble from the observations and possible
ensemble collapse. This manuscript introduces a new form of covariance
inflation for ensemble data assimilation based upon shadowing ideas from
dynamical systems theory. We present results from a low order nonlinear chaotic
system that supports using shadowing inflation, demonstrating that shadowing
inflation is more robust to parameter tuning than standard multiplicative
covariance inflation, outperforming in observation-sparse scenarios and often
leading to longer forecast shadowing times.",
cross-border inflation,http://arxiv.org/abs/1412.2107v1,Inflation and the Higgs Scalar,"This note makes a self-contained exposition of the basic facts of big bang
cosmology as they relate to inflation. The fundamental problems with that model
are then explored. A quartic scalar potential model of inflation is evaluated
which provides the solution of those problems and makes predictions which will
soon be definitively tested. The possibility that the recently discovered
fundamental Higgs scalar field drives inflation is explored.",
cross-border GDP,http://arxiv.org/abs/physics/0607139v2,"Clusters or networks of economies? A macroeconomy study through GDP
  fluctuation correlations","We follow up on the study of correlations between GDP's of rich countries. We
analyze web-downloaded data on GDP that we use as individual wealth signatures
of the country economical state. We calculate the yearly fluctuations of the
GDP. We look for forward and backward correlations between such fluctuations.
The system is represented by an evolving network, nodes being the GDP
fluctuations (or countries) at different times.
  In order to extract structures from the network, we focus on filtering the
time delayed correlations by removing the least correlated links. This
percolation idea-based method reveals the emergence of connections, that are
visualized by a branching representation. Note that the network is made of
weighted and directed links when taking into account a delay time. Such a
measure of collective habits does not fit the usual expectations defined by
politicians or economists.",Physica A 382 (2007) 16-21
cross-border GDP,http://arxiv.org/abs/1609.00876v1,Statistical Dynamics of Regional Populations and Economies,"A practical statistical analysis on the regional populations and GDPs of
China is conducted. The result shows that the distribution of the populations
and that of the GDPs obeys the shifted power law, respectively. To understand
these characteristics, a generalized Langevin equation describing variation of
population is proposed based on the correlation between population and GDP as
well as the random fluctuations of the related factors. The equation is
transformed into the Fokker-Plank equation, and the solution demonstrates a
transform of population distribution from the normal Gaussian distribution to a
shifted power law. It also suggests a critical point of time at which the
transform occurs. The shifted power law distribution in the supercritical
situation is qualitatively in accordance with the practical result. The
distribution of the GDPs is derived based on the Cobb-Douglas production
function, and presents a change from a shifted power law to the Gaussian
distribution. This result indicates that the regional GDP distribution of our
society will be the Gaussian distribution in the future. The analysis on the
growth trend of economy suggests it will become a reality. These theoretical
attempts may draw a historical picture of our world in the aspects of
population and economy.",
cross-border GDP,http://arxiv.org/abs/physics/0701030v1,Interplay between topology and dynamics in the World Trade Web,"We present an empirical analysis of the network formed by the trade
relationships between all world countries, or World Trade Web (WTW). Each
(directed) link is weighted by the amount of wealth flowing between two
countries, and each country is characterized by the value of its Gross Domestic
Product (GDP). By analysing a set of year-by-year data covering the time
interval 1950-2000, we show that the dynamics of all GDP values and the
evolution of the WTW (trade flow and topology) are tightly coupled. The
probability that two countries are connected depends on their GDP values,
supporting recent theoretical models relating network topology to the presence
of a `hidden' variable (or fitness). On the other hand, the topology is shown
to determine the GDP values due to the exchange between countries. This leads
us to a new framework where the fitness value is a dynamical variable
determining, and at the same time depending on, network topology in a
continuous feedback.","Eur. Phys. J. B 57,159-164 (2007)"
cross-border GDP,http://arxiv.org/abs/1308.5572v1,"Insight into the Properties of the UK Power Consumption Using a Linear
  Regression and Wavelet Transform Approach","In this paper, the relationship between the Gross Domestic Product (GDP), air
temperature variations and power consumption is evaluated using the linear
regression and Wavelet Coherence (WTC) approach on a 1971-2011 time series for
the United Kingdom (UK). The results based on the linear regression approach
indicate that some 66% variability of the UK electricity demand can be
explained by the quarterly GDP variations, while only 11% of the quarterly
changes of the UK electricity demand are caused by seasonal air temperature
variations. WTC however, can detect the period of time when GDP and air
temperature significantly correlate with electricity demand and the results of
the wavelet correlation at different time scales indicate that a significant
correlation is to be found on a long-term basis for GDP and on an annual basis
for seasonal air-temperature variations. This approach provides an insight into
the properties of the impact of the main factors on power consumption on the
basis of which the power system development or operation planning and
forecasting the power consumption can be improved.","Elektrotehniski vestnik/Electrotechnical review 79(5), 278-283,
  2012"
cross-border GDP,http://arxiv.org/abs/physics/0607098v1,"Cluster structure of EU-15 countries derived from the correlation matrix
  analysis of macroeconomic index fluctuations","The statistical distances between countries, calculated for various moving
average time windows, are mapped into the ultrametric subdominant space as in
classical Minimal Spanning Tree methods. The Moving Average Minimal Length Path
(MAMLP) algorithm allows a decoupling of fluctuations with respect to the mass
center of the system from the movement of the mass center itself. A Hamiltonian
representation given by a factor graph is used and plays the role of cost
function. The present analysis pertains to 11 macroeconomic (ME) indicators,
namely the GDP (x1), Final Consumption Expenditure (x2), Gross Capital
Formation (x3), Net Exports (x4), Consumer Price Index (y1), Rates of Interest
of the Central Banks (y2), Labour Force (z1), Unemployment (z2), GDP/hour
worked (z3), GDP/capita (w1) and Gini coefficient (w2). The target group of
countries is composed of 15 EU countries, data taken between 1995 and 2004. By
two different methods (the Bipartite Factor Graph Analysis and the Correlation
Matrix Eigensystem Analysis) it is found that the strongly correlated countries
with respect to the macroeconomic indicators fluctuations can be partitioned
into stable clusters.",Eur. Phys. J B 57 (2007) 139-146
cross-border GDP,http://arxiv.org/abs/physics/0607180v1,"How Do Output Growth Rate Distributions Look Like? Some Time-Series
  Evidence on OECD Countries","This paper investigates the statistical properties of within-country GDP and
industrial production (IP) growth rate distributions. Many empirical
contributions have recently pointed out that cross-section growth rates of
firms, industries and countries all follow Laplace distributions. In this work,
we test whether also within-country, time-series GDP and IP growth rates can be
approximated by tent-shaped distributions. We fit output growth rates with the
exponential-power (Subbotin) family of densities, which includes as particular
cases both the Gaussian and the Laplace distributions. We find that, for a
large number of OECD countries including the U.S., both GDP and IP growth rates
are Laplace distributed. Moreover, we show that fat-tailed distributions
robustly emerge even after controlling for outliers, autocorrelation and
heteroscedasticity.",
cross-border GDP,http://arxiv.org/abs/0802.4170v1,"Cluster Expansion Method for Evolving Weighted Networks Having
  Vector-like Nodes","The Cluster Variation Method known in statistical mechanics and condensed
matter is revived for weighted bipartite networks. The decomposition of a
Hamiltonian through a finite number of components, whence serving to define
variable clusters, is recalled. As an illustration the network built from data
representing correlations between (4) macro-economic features, i.e. the so
called $vector$ $components$, of 15 EU countries, as (function) nodes, is
discussed. We show that statistical physics principles, like the maximum
entropy criterion points to clusters, here in a (4) variable phase space: Gross
Domestic Product (GDP), Final Consumption Expenditure (FCE), Gross Capital
Formation (GCF) and Net Exports (NEX). It is observed that the $maximum$
entropy corresponds to a cluster which does $not$ explicitly include the GDP
but only the other (3) ''axes'', i.e. consumption, investment and trade
components. On the other hand, the $minimal$ entropy clustering scheme is
obtained from a coupling necessarily including GDP and FCE. The results confirm
intuitive economic theory and practice expectations at least as regards
geographical connexions. The technique can of course be applied to many other
cases in the physics of socio-economy networks.",Acta Phys. Pol. A 114 (2008) 491-499
cross-border GDP,http://arxiv.org/abs/1006.5269v1,Allometric Scaling of Countries,"As huge complex systems consisting of geographic regions, natural resources,
people and economic entities, countries follow the allometric scaling law which
is ubiquitous in ecological, urban systems. We systematically investigated the
allometric scaling relationships between a large number of macroscopic
properties and geographic (area), demographic (population) and economic (GDP,
gross domestic production) sizes of countries respectively. We found that most
of the economic, trade, energy consumption, communication related properties
have significant super-linear (the exponent is larger than 1) or nearly linear
allometric scaling relations with GDP. Meanwhile, the geographic (arable area,
natural resources, etc.), demographic(labor force, military age population,
etc.) and transportation-related properties (road length, airports) have
significant and sub-linear (the exponent is smaller than 1) allometric scaling
relations with area. Several differences of power law relations with respect to
population between countries and cities were pointed out. Firstly, population
increases sub-linearly with area in countries. Secondly, GDP increases linearly
in countries but not super-linearly as in cities. Finally, electricity or oil
consumptions per capita increases with population faster than cities.","Physica A: Statistical Mechanics and its Applications. Volume 389,
  Issue 21, 1 November 2010, Pages 4887-4896"
cross-border GDP,http://arxiv.org/abs/1603.02734v1,"Codebook Design for Millimeter-Wave Channel Estimation with Hybrid
  Precoding Structure","In this paper, we study hierarchical codebook design for channel estimation
in millimeter-wave (mmWave) communications with a hybrid precoding structure.
Due to the limited saturation power of mmWave power amplifier (PA), we take the
per-antenna power constraint (PAPC) into consideration. We first propose a
metric, i.e., generalized detection probability (GDP), to evaluate the quality
of \emph{an arbitrary codeword}. This metric not only enables an optimization
approach for mmWave codebook design, but also can be used to compare the
performance of two different codewords/codebooks. To the best of our knowledge,
GDP is the first metric particularly for mmWave codebook design for channel
estimation. We then propose an approach to design a hierarchical codebook
exploiting BeaM Widening with Multi-RF-chain Sub-array technique (BMW-MS). To
obtain crucial parameters of BMW-MS, we provide two solutions, namely a
low-complexity search (LCS) solution to optimize the GDP metric and a
closed-form (CF) solution to pursue a flat beam pattern. Performance
comparisons show that BMW-MS/LCS and BMW-MS/CF achieve very close performances,
and they outperform the existing alternatives under the PAPC.",
cross-border GDP,http://arxiv.org/abs/1710.07382v1,Partially Coherent Ptychography by Gradient Decomposition of the Probe,"Coherent ptychographic imaging experiments often discard over 99.9 % of the
flux from a light source to define the coherence of an illumination. Even when
coherent flux is sufficient, the stability required during an exposure is
another important limiting factor. Partial coherence analysis can considerably
reduce these limitations. A partially coherent illumination can often be
written as the superposition of a single coherent illumination convolved with a
separable translational kernel. In this paper we propose the Gradient
Decomposition of the Probe (GDP), a model that exploits translational kernel
separability, coupling the variances of the kernel with the transverse
coherence. We describe an efficient first-order splitting algorithm GDP-ADMM to
solve the proposed nonlinear optimization problem. Numerical experiments
demonstrate the effectiveness of the proposed method with Gaussian and binary
kernel functions in fly-scan measurements. Remarkably, GDP-ADMM produces
satisfactory results even when the ratio between kernel width and beam size is
more than one, or when the distance between successive acquisitions is twice as
large as the beam width.","Acta Crystallographica Section A: Foundations and Advances 74 (3),
  157-169 (2018)"
cross-border GDP,http://arxiv.org/abs/0909.4786v1,"Worldwide Use and Impact of the NASA Astrophysics Data System Digital
  Library","By combining data from the text, citation, and reference databases with data
from the ADS readership logs we have been able to create Second Order
Bibliometric Operators, a customizable class of collaborative filters which
permits substantially improved accuracy in literature queries.
  Using the ADS usage logs along with membership statistics from the
International Astronomical Union and data on the population and gross domestic
product (GDP) we develop an accurate model for world-wide basic research where
the number of scientists in a country is proportional to the GDP of that
country, and the amount of basic research done by a country is proportional to
the number of scientists in that country times that country's per capita GDP.
  We introduce the concept of utility time to measure the impact of the
ADS/URANIA and the electronic astronomical library on astronomical research. We
find that in 2002 it amounted to the equivalent of 736 FTE researchers, or $250
Million, or the astronomical research done in France.
  Subject headings: digital libraries; bibliometrics; sociology of science;
information retrieval","The Journal of the American Society for Information Science and
  Technology, Vol. 56, p. 36. (2005)"
cross-border GDP,http://arxiv.org/abs/1505.05321v1,Regional Development Classification Model using Decision Tree Approach,"Regional development classification is one way to look at differences in
levels of development outcomes. Some frequently used methods are the shift
share, Gain index, the Iindex Williamson and Klassen typology. The development
of science in the field of data mining, offers a new way for regional
development data classification. This study discusses how the decision tree is
used to classify the level of development based on indicators of regional gross
domestic product (GDP). GDP Data Central Java and Banten used in this study.
Before the data is entered into the decision tree forming algorithm, both the
provincial GDP data are classified using Klassen typology. Three decision tree
algorithms, namely J48, NBTRee and REPTree tested in this study using
cross-validation evaluation, then selected one of the best performing
algorithms. The results show that the J48 has a better accuracy rate which is
equal to 85.18% compared to the algorithm NBTRee and REPTree. Testing the model
is done to the six districts / municipalities in the province of Banten, and
shows that there are two districts / cities are still at the development of the
status quadrant relatively underdeveloped regions, namely Kota Tangerang and
Kabupaten Tangerang. As for the Central Java Province, Kendal, Magelang,
Pemalang, Rembang, Semarang and Wonosobo are an area with a quadrant of
development also on the status of the region is relatively underdeveloped.
Classification model that has been developed is able to classify the level of
development fast and easy to enter data directly into the decision tree is
formed. This study can be used as an alternative decision support for policy
makers in order to determine the future direction of development.","International Journal of Computer Applications Volume 114, No. 8,
  March 2015"
cross-border GDP,http://arxiv.org/abs/1608.00275v1,"Metastable Features of Economic Networks and Responses to Exogenous
  Shocks","It has been proved that network structure plays an important role in
addressing a collective behaviour. In this paper we consider a network of firms
and corporations and study its metastable features in an Ising based model. In
our model, we observe that if in a recession the government imposes a demand
shock to stimulate the network, metastable features shape its response.
Actually we find that there is a minimum bound where demand shocks with a size
below it are unable to trigger the market out from recession. We then
investigate the impact of network characteristics on this minimum bound. We
surprisingly observe that in a Watts-Strogatz network though the minimum bound
depends on the average of the degrees, when translated into the economics
language, such a bound is independent of the average degrees. This bound is
about $0.44 \Delta$GDP, where $\Delta$GDP is the gap of GDP between recession
and expansion. We examine our suggestions for the cases of the United States
and the European Union in the recent recession, and compare them with the
imposed stimulations. While stimulation in the US has been above our threshold,
in the EU it has been far below our threshold. Beside providing a minimum bound
for a successful stimulation, our study on the metastable features suggests
that in the time of crisis there is a ""golden time passage"" in which the
minimum bound for successful stimulation can be much lower. So, our study
strongly suggests stimulations to be started within this time passage.","PloS one 11 (10), e0160363 (2016)"
cross-border GDP,http://arxiv.org/abs/1902.09872v1,Economic geography and the scaling of urban and regional income in India,"We undertake an exploration of the economic income (Gross Domestic Product,
GDP) of Indian districts and cities based on scaling analyses of the dependence
of these quantities on associated population size. Scaling analysis provides a
straightforward method for the identification of network effects in
socioeconomic organization, which are the tell-tale of cities and urbanization.
For districts, a sub-state regional administrative division in India, we find
almost linear scaling of GDP with population, a result quite different from
urban functional units in other national contexts. Using deviations from
scaling, we explore the behavior of these regional units to find strong
distinct geographic patterns of economic behavior. We characterize these
patterns in detail and connect them to the literature on regional economic
development for a diverse subcontinental nation such as India. Given the
paucity of economic data for Urban Agglomerations in India, we use a set of
assumptions to create a new dataset of GDP based on districts, for large
cities. This reveals superlinear scaling of income with city size, as expected
from theory, while displaying similar underlying patterns of economic geography
observed for district economic performance. This analysis of the economic
performance of Indian cities is severely limited by the absence of
higher-fidelity, direct city level economic data. We discuss the need for
standardized and consistent estimates of the size and change in urban economies
in India, and point to a number of proxies that can be explored to develop such
indicators.",
cross-border GDP,http://arxiv.org/abs/1711.10883v1,"An Analytical Framework for Understanding the Intensity of Religious
  Fundamentalism","This paper examines the process of emergence of religious fundamentalism
through development parameters. Therefore this research work reflects an
analytical discussion on how the level of religious fundamentalism can be
explained by the economic, political administrative and legal parameters such
as GDP, Employment to Population ratio, Government Effectiveness, Voice &
Accountability, Rule of Law (World Justice Project Report) and Rule of law
(Governance Indicators).",
cross-border GDP,http://arxiv.org/abs/physics/0101078v1,A Unifying Hypothesis for the Conformational Change of Tubulin,"Microtubule dynamic instability arises from the hydrolysis of GTP bound to
the beta-monomer of the tubulin dimer. The conformational change induced by
hydrolysis is unknown, but microtubules disassemble into protofilaments of
GDP-bound tubulin that curve away from the microtubule axis. This paper
presents the unfolding of a portion of the tubulin molecule into the
microtubule interior as a plausible, unifying explanation for diverse
structural and kinetic features of microtubules. This is the first specific
structural hypothesis for the hydrolysis induced conformational change of
tubulin that simultaneously explains weakening of lateral bonds, bending about
longitudinal bonds, changes in protofilament supertwist associated with GTP
hydrolysis, structural features of GDP-tubulin double rings, faster disassembly
at higher temperatures and slower disassembly in the presence of glycerol and
deuterium oxide. The hypothesis suggests further theoretical investigations and
direct experimental tests.",
cross-border GDP,http://arxiv.org/abs/0710.5447v1,"Clusters in weighted macroeconomic networks : the EU case. Introducing
  the overlapping index of GDP/capita fluctuation correlations","GDP/capita correlations are investigated in various time windows (TW), for
the time interval 1990-2005. The target group of countries is the set of 25 EU
members, 15 till 2004 plus the 10 countries which joined EU later on. The
TW-means of the statistical correlation coefficients are taken as the weights
(links) of a fully connected network having the countries as nodes. Thereafter
we define and introduce the overlapping index of weighted network nodes. A
cluster structure of EU countries is derived from the statistically relevant
eigenvalues and eigenvectors of the adjacency matrix. This may be considered to
yield some information about the structure, stability and evolution of the EU
country clusters in a macroeconomic sense.",Eur. Phys. J. B 63 (2008) 533-539
cross-border GDP,http://arxiv.org/abs/1112.4708v1,"Transformation Networks: How Innovation and the Availability of
  Technology can Increase Economic Performance","A transformation network describes how one set of resources can be
transformed into another via technological processes. Transformation networks
in economics are useful because they can highlight areas for future
innovations, both in terms of new products, new production techniques, or
better efficiency. They also make it easy to detect areas where an economy
might be fragile. In this paper, we use computational simulations to
investigate how the density of a transformation network affects the economic
performance, as measured by the gross domestic product (GDP), of an artificial
economy. Our results show that on average, the GDP of our economy increases as
the density of the transformation network increases. We also find that while
the average performance increases, the maximum possible performance decreases
and the minimum possible performance increases.",
cross-border GDP,http://arxiv.org/abs/0911.1044v1,"Macro-level Indicators of the Relations between Research Funding and
  Research Output","In response to the call for a science of science policy, we discuss the
contribution of indicators at the macro-level of nations from a scientometric
perspective. In addition to global trends such as the rise of China, one can
relate percentages of world share of publications to government expenditure in
academic research. The marginal costs of improving one's share are increasing
over time. Countries differ considerably in terms of the efficiency of turning
(financial) input into bibliometrically measurable output. Both funding schemes
and disciplinary portfolios differ among countries. A price per paper can
nevertheless be estimated. The percentages of GDP spent on academic research in
different nations are significantly correlated to historical contingencies such
as the percentage of researchers in the population. The institutional dynamics
make strategic objectives such as the Lisbon objective of the EU--that is,
spending 3% of GDP for R&D in 2010--unrealistic.","Loet Leydesdorff & Caroline Wagner, Macro-level Indicators of the
  Relations between Research Funding and Research Output; Journal of
  Informetrics 3(4) (2009), 353-362"
cross-border GDP,http://arxiv.org/abs/1902.05218v1,"Regional economic status inference from information flow and talent
  mobility","Novel data has been leveraged to estimate socioeconomic status in a timely
manner, however, direct comparison on the use of social relations and talent
movements remains rare. In this letter, we estimate the regional economic
status based on the structural features of the two networks. One is the online
information flow network built on the following relations on social media, and
the other is the offline talent mobility network built on the anonymized resume
data of job seekers with higher education. We find that while the structural
features of both networks are relevant to economic status, the talent mobility
network in a relatively smaller size exhibits a stronger predictive power for
the gross domestic product (GDP). In particular, a composite index of
structural features can explain up to about 84% of the variance in GDP. The
result suggests future socioeconomic studies to pay more attention to the
cost-effective talent mobility data.","EPL (Europhysics Letters), 125(6) (2019) 68002"
cross-border GDP,http://arxiv.org/abs/1906.01997v3,MEDEAS-World model calibration for the study of the energy transition,"MEDEAS (Modelling the Energy Development under Environmental And
Socioeconomic constraint) World is a new global-aggregated
energy-economy-environmental model, which runs from 1995 to 2050. In this work,
we tested the MEDEAS world model to reproduce the IPCC (International Panel on
Climate Change) GHG (Green House Gases) emission pathways consistent with 2
{\deg}C Global Warming. We achieved parameter optimizations of the MEDEAS model
related to different scenarios until 2050. We chose to provide a sensitivity
analysis on the parameters that directly influence the emission curves focusing
on the annual growth of the RES (Renewable Energy Sources), GDP (Gross Domestic
Product) and annual population growth. From such an analysis, it has been
possible to infer the large impact of GDP on the emission scenarios.",
cross-border GDP,http://arxiv.org/abs/1905.02383v3,Gaussian Differential Privacy,"Differential privacy has seen remarkable success as a rigorous and practical
formalization of data privacy in the past decade. This privacy definition and
its divergence based relaxations, however, have several acknowledged
weaknesses, either in handling composition of private algorithms or in
analyzing important primitives like privacy amplification by subsampling.
Inspired by the hypothesis testing formulation of privacy, this paper proposes
a new relaxation, which we term `$f$-differential privacy' ($f$-DP). This
notion of privacy has a number of appealing properties and, in particular,
avoids difficulties associated with divergence based relaxations. First, $f$-DP
preserves the hypothesis testing interpretation. In addition, $f$-DP allows for
lossless reasoning about composition in an algebraic fashion. Moreover, we
provide a powerful technique to import existing results proven for original DP
to $f$-DP and, as an application, obtain a simple subsampling theorem for
$f$-DP.
  In addition to the above findings, we introduce a canonical single-parameter
family of privacy notions within the $f$-DP class that is referred to as
`Gaussian differential privacy' (GDP), defined based on testing two shifted
Gaussians. GDP is focal among the $f$-DP class because of a central limit
theorem we prove. More precisely, the privacy guarantees of \emph{any}
hypothesis testing based definition of privacy (including original DP)
converges to GDP in the limit under composition. The CLT also yields a
computationally inexpensive tool for analyzing the exact composition of private
algorithms.
  Taken together, this collection of attractive properties render $f$-DP a
mathematically coherent, analytically tractable, and versatile framework for
private data analysis. Finally, we demonstrate the use of the tools we develop
by giving an improved privacy analysis of noisy stochastic gradient descent.",
cross-border GDP,http://arxiv.org/abs/1908.02584v1,"Economic Power, Population, and the Size of Astronomical Community","The number of astronomers for a country registered to the IAU is known to
have a correlation with the GDP. However, the robustness of this relationship
can be doubted, because the fraction of astronomers joining the IAU differs
from country to country. Here we revisit this correlation by using the recent
data updated as of 2017, and then we find a similar correlation by using the
total enumeration of astronomers and astrophysicists with PhD degrees and
working in each country, instead of adopting the number of IAU members. We
confirm the existence of two subgroup in the correlation. One group consists of
European advanced countries having long history of modern astronomy, while the
other group consists of countries having experienced recent rapid economic
development. In order to find causation in the correlation, we obtain the
long-term variations of the number of astronomers, population, and the GDP for
a number of countries to find that the number of astronomers per citizen for
recently developing countries has increased more rapidly as GDP per capita
increased, than that for fully developed countries. We collect a demographic
data of the Korean astronomical community. From these findings we estimate the
proper size of the Korean astronomical community by considering the society's
economic power and population. The current number of PhD astronomers working in
Korea is approximately 310, but it should be 550 that is large enough to be
comparable and competitive to the sizes of Spainish, Canadian, and Japanese
astronomical communities. We discuss on the way how to overcome the
vulnerability of the Korean astronomical community, based on the statistics of
national R&D expenditure structure comparing with that of other major advanced
countries.",
cross-border GDP,http://arxiv.org/abs/physics/0504099v1,"G7 country Gross Domestic Product (GDP) time correlations. A graph
  network analysis","The correlation between G7 countries has been analysed on the basis of Gross
Domestic Product using different distance functions i.e. discrete, linear
correlation and distribution distance. The distance matrics is analysed by
various graph methods and the percolation threshold is calculated. The
globalization process understood as increas of correlation has been observed.
The applications of different distance function discussed.","Practical Fruits of Econophysics, H. Takayasu, Ed. (Springer,
  Tokyo, 2006) pp. 312-316"
cross-border GDP,http://arxiv.org/abs/0708.3463v1,A Neural Networks Model of the Venezuelan Economy,"Besides an indicator of the GDP, the Central Bank of Venezuela generates the
so called Monthly Economic Activity General Indicator. The a priori knowledge
of this indicator, which represents and sometimes even anticipates the
economy's fluctuations, could be helpful in developing public policies and in
investment decision making. The purpose of this study is forecasting the IGAEM
through non parametric methods, an approach that has proven effective in a wide
variety of problems in economics and finance.",
cross-border GDP,http://arxiv.org/abs/1010.4288v2,"Modeling the Effects of Drug Binding on the Dynamic Instability of
  Microtubules","We propose a stochastic model that accounts for the growth, catastrophe and
rescue processes of steady state microtubules assembled from MAP-free tubulin.
Both experimentally and theoretically we study the perturbation of microtubule
dynamic instability by S-methyl-D-DM1, a synthetic derivative of the
microtubule-targeted agent maytansine and a potential anticancer agent. We find
that to be an effective suppressor of microtubule dynamics a drug must
primarily suppress the loss of GDP tubulin from the microtubule tip.",Phys. Biol. 8:056004 (2011)
cross-border GDP,http://arxiv.org/abs/1310.6808v1,Gender Classification Using Gradient Direction Pattern,"A novel methodology for gender classification is presented in this paper. It
extracts feature from local region of a face using gray color intensity
difference. The facial area is divided into sub-regions and GDP histogram
extracted from those regions are concatenated into a single vector to represent
the face. The classification accuracy obtained by using support vector machine
has outperformed all traditional feature descriptors for gender classification.
It is evaluated on the images collected from FERET database and obtained very
high accuracy.","Sci.Int(Lahore),25(4),797-799,2013 ISSN 1013-5316; CODEN: SINTE 8"
cross-border GDP,http://arxiv.org/abs/1807.03364v2,A global consumer-led strategy to tackle climate change,"A successful response to climate change needs vast investments in low-carbon
research, energy, and sustainable development. Governments can drive research,
provide environmental regulation, and accelerate global development, but the
necessary low-carbon investments of 2-3% GDP have yet to materialise. A new
strategy to tackle climate change through consumer and government action is
outlined. It relies on ethical investments for sustainable development and
low-carbon energy, and a voluntarily financed low-carbon fund for adaptation to
climate change. Together these enable a global response through individual
actions and investments. With OECD savings exceeding 5% of disposable household
income, ethical savings alone have considerable potential.",
cross-border CPI,http://arxiv.org/abs/1206.3282v1,Improving the Accuracy and Efficiency of MAP Inference for Markov Logic,"In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in
two different MAP inference methods: the current method of choice for MAP
inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming.",
cross-border CPI,http://arxiv.org/abs/1006.4622v2,"A High-Resolution Human Contact Network for Infectious Disease
  Transmission","The most frequent infectious diseases in humans - and those with the highest
potential for rapid pandemic spread - are usually transmitted via droplets
during close proximity interactions (CPIs). Despite the importance of this
transmission route, very little is known about the dynamic patterns of CPIs.
Using wireless sensor network technology, we obtained high-resolution data of
CPIs during a typical day at an American high school, permitting the
reconstruction of the social network relevant for infectious disease
transmission. At a 94% coverage, we collected 762,868 CPIs at a maximal
distance of 3 meters among 788 individuals. The data revealed a high density
network with typical small world properties and a relatively homogenous
distribution of both interaction time and interaction partners among subjects.
Computer simulations of the spread of an influenza-like disease on the weighted
contact graph are in good agreement with absentee data during the most recent
influenza season. Analysis of targeted immunization strategies suggested that
contact network data are required to design strategies that are significantly
more effective than random immunization. Immunization strategies based on
contact network data were most effective at high vaccination coverage.",
cross-border CPI,http://arxiv.org/abs/1405.2878v1,Approximate Policy Iteration Schemes: A Comparison,"We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on several approximate
variations of the Policy Iteration algorithm: Approximate Policy Iteration,
Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search
by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),
and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all
algorithms, we describe performance bounds, and make a comparison by paying a
particular attention to the concentrability constants involved, the number of
iterations and the memory required. Our analysis highlights the following
points: 1) The performance guarantee of CPI can be arbitrarily better than that
of API/API($\alpha$), but this comes at the cost of a relative---exponential in
$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$
enjoys the best of both worlds: its performance guarantee is similar to that of
CPI, but within a number of iterations similar to that of API. 3) Contrary to
API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$
is proportional to their number of iterations, which may be problematic when
the discount factor $\gamma$ is close to 1 or the approximation error
$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make
an overall trade-off between memory and performance. Simulations with these
schemes confirm our analysis.",
cross-border CPI,http://arxiv.org/abs/1705.08775v2,"A Control Performance Index for Multicopters Under Off-nominal
  Conditions","In order to prevent loss of control (LOC) accidents,the real-time control
performance monitoring problem is studied for multicopters. Different from the
existing work, this paper does not try to monitor the performance of the
controllers directly. In turn, the disturbances of multicopters under
off-nominal conditions are estimated to affect a proposed index to tell the
user whether the multicopter will be LOC or not. Firstly, a new degree of
controllability (DoC) will be proposed for multicopters subject to control
constrains and off-nominal conditions. Then a control performance index (CPI)
is defined based on the new DoC to reflect the control performance for
multicopters. Besides, the proposed CPI is applied to a new switching control
framework to guide the control decision of multicopter under off-nominal
conditions. Finally, simulation and experimental results show the effectiveness
of the CPI and the proposed switching control framework.",
cross-border CPI,http://arxiv.org/abs/1906.09784v1,Deep Conservative Policy Iteration,"Conservative Policy Iteration (CPI) is a founding algorithm of Approximate
Dynamic Programming (ADP). Its core principle is to stabilize greediness
through stochastic mixtures of consecutive policies. It comes with strong
theoretical guarantees, and inspired approaches in deep Reinforcement Learning
(RL). However, CPI itself has rarely been implemented, never with neural
networks, and only experimented on toy problems. In this paper, we show how CPI
can be practically combined with deep RL with discrete actions. We also
introduce adaptive mixture rates inspired by the theory. We experiment
thoroughly the resulting algorithm on the simple Cartpole problem, and validate
the proposed method on a representative subset of Atari games. Overall, this
work suggests that revisiting classic ADP may lead to improved and more stable
deep RL algorithms.",
cross-border CPI,http://arxiv.org/abs/0708.3387v1,"The Impact of Noise Correlation and Channel Phase Information on the
  Data-Rate of the Single-Symbol ML Decodable Distributed STBCs","Very recently, we proposed the row-monomial distributed orthogonal space-time
block codes (DOSTBCs) and showed that the row-monomial DOSTBCs achieved
approximately twice higher bandwidth efficiency than the repetitionbased
cooperative strategy [1]. However, we imposed two limitations on the
row-monomial DOSTBCs. The first one was that the associated matrices of the
codes must be row-monomial. The other was the assumption that the relays did
not have any channel state information (CSI) of the channels from the source to
the relays, although this CSI could be readily obtained at the relays without
any additional pilot signals or any feedback overhead. In this paper, we first
remove the row-monomial limitation; but keep the CSI limitation. In this case,
we derive an upper bound of the data-rate of the DOSTBC and it is larger than
that of the row-monomial DOSTBCs in [1]. Secondly, we abandon the CSI
limitation; but keep the row-monomial limitation. Specifically, we propose the
row-monomial DOSTBCs with channel phase information (DOSTBCs-CPI) and derive an
upper bound of the data-rate of those codes. The rowmonomial DOSTBCs-CPI have
higher data-rate than the DOSTBCs and the row-monomial DOSTBCs. Furthermore, we
find the actual row-monomial DOSTBCs-CPI which achieve the upper bound of the
data-rate.",
cross-border CPI,http://arxiv.org/abs/1306.0539v1,"On the Performance Bounds of some Policy Search Dynamic Programming
  Algorithms","We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on Policy Search algorithms,
that compute an approximately optimal policy by following the standard Policy
Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,
2002; Lazaric et al., 2010). We describe existing and a few new performance
bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et
al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)
(Kakade and Langford, 2002). By paying a particular attention to the
concentrability constants involved in such guarantees, we notably argue that
the guarantee of CPI is much better than that of DPI, but this comes at the
cost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of time
complexity. We then describe an algorithm, Non-Stationary Direct Policy
Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search
by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon
situation or 2) a simplified version of the Non-Stationary PI with growing
period of Scherrer and Lesner (2012). We provide an analysis of this algorithm,
that shows in particular that it enjoys the best of both worlds: its
performance guarantee is similar to that of CPI, but within a time complexity
similar to that of DPI.",
cross-border CPI,http://arxiv.org/abs/1710.05944v1,Neuro Fuzzy Modelling for Prediction of Consumer Price Index,"Economic indicators such as Consumer Price Index (CPI) have frequently used
in predicting future economic wealth for financial policy makers of respective
country. Most central banks, on guidelines of research studies, have recently
adopted an inflation targeting monetary policy regime, which accounts for high
requirement for effective prediction model of consumer price index. However,
prediction accuracy by numerous studies is still low, which raises a need for
improvement. This manuscript presents findings of study that use neuro fuzzy
technique to design a machine-learning model that train and test data to
predict a univariate time series CPI. The study establishes a matrix of monthly
CPI data from secondary data source of Tanzania National Bureau of Statistics
from January 2000 to December 2015 as case study and thereafter conducted
simulation experiments on MATLAB whereby ninety five percent (95%) of data used
to train the model and five percent (5%) for testing. Furthermore, the study
use root mean square error (RMSE) and mean absolute percentage error (MAPE) as
error metrics for model evaluation. The results show that the neuro fuzzy model
have an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2,
2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to
existing research studies.","International Journal of Artificial Intelligence and Applications
  (IJAIA), Vol.8, No.5, September 2017"
cross-border CPI,http://arxiv.org/abs/1405.1750v1,Numerical simulation of turbulent duct flows with constant power input,"The numerical simulation of a flow through a duct requires an externally
specified forcing that makes the fluid flow against viscous friction. To this
aim, it is customary to enforce a constant value for either the flow rate (CFR)
or the pressure gradient (CPG). When comparing a laminar duct flow before and
after a geometrical modification that induces a change of the viscous drag,
both approaches (CFR and CPG) lead to a change of the power input across the
comparison. Similarly, when carrying out the (DNS and LES) numerical simulation
of unsteady turbulent flows, the power input is not constant over time.
Carrying out a simulation at constant power input (CPI) is thus a further
physically sound option, that becomes particularly appealing in the context of
flow control, where a comparison between control-on and control-off conditions
has to be made.
  We describe how to carry out a CPI simulation, and start with defining a new
power-related Reynolds number, whose velocity scale is the bulk flow that can
be attained with a given pumping power in the laminar regime. Under the CPI
condition, we derive a relation that is equivalent to the
Fukagata--Iwamoto--Kasagi relation valid for CFR (and to its extension valid
for CPG), that presents the additional advantage of natively including the
required control power. The implementation of the CPI approach is then
exemplified in the standard case of a plane turbulent channel flow, and then
further applied to a flow control case, where the spanwise-oscillating wall is
used for skin friction drag reduction. For this low-Reynolds number flow, using
90% of the available power for the pumping system and the remaining 10% for the
control system is found to be the optimum share that yields the largest
increase of the flow rate above the reference case, where 100% of the power
goes to the pump.",
cross-border CPI,http://arxiv.org/abs/1709.00310v3,"Detection via simultaneous trajectory estimation and long time
  integration","In this work, we consider the detection of manoeuvring small objects with
radars. Such objects induce low signal to noise ratio (SNR) reflections in the
received signal. We consider both co-located and separated transmitter/receiver
pairs, i.e., mono-static and bi-static configurations, respectively, as well as
multi-static settings involving both types. We propose a detection approach
which is capable of coherently integrating these reflections within a coherent
processing interval (CPI) in all these configurations and continuing
integration for an arbitrarily long time across consecutive CPIs. We estimate
the complex value of the reflection coefficients for integration while
simultaneously estimating the object trajectory. Compounded with this is the
estimation of the unknown time reference shift of the separated transmitters
necessary for coherent processing. Detection is made by using the resulting
integration value in a Neyman-Pearson test against a constant false alarm rate
threshold. We demonstrate the efficacy of our approach in a simulation example
with a very low SNR object which cannot be detected with conventional
techniques.",
cross-border CPI,http://arxiv.org/abs/1907.07776v1,CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers,"Memory controller scheduling is crucial in multicore processors, where DRAM
bandwidth is shared. Since increased number of requests from multiple cores of
processors becomes a source of bottleneck, scheduling the requests efficiently
is necessary to utilize all the computing power these processors offer.
However, current multicore processors are using traditional memory controllers,
which are designed for single-core processors. They are unable to adapt to
changing characteristics of memory workloads that run simultaneously on
multiple cores. Existing schedulers may disrupt locality and bank parallelism
among data requests coming from different cores. Hence, novel memory
controllers that consider and adapt to the memory access characteristics, and
share memory resources efficiently and fairly are necessary. We introduce
Core-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses
Reinforcement Learning (RL) to alter its scheduling strategy dynamically at
runtime. Our scheduler utilizes locality among data requests from multiple
cores and exploits parallelism in accessing multiple banks of DRAM. CADS is
also able to share the DRAM while guaranteeing fairness to all cores accessing
memory. Using CADS policy, we achieve 20% better cycles per instruction (CPI)
in running memory intensive and compute intensive PARSEC parallel benchmarks
simultaneously, and 16% better CPI with SPEC 2006 benchmarks.",
cross-border CPI,http://arxiv.org/abs/physics/0008035v2,2.856-GHz Modulation of Conventional Triode Electron Gun,"For the generation of picosecond (< 100 ps) electron beam pulses, we studied
the RF modulation of a conventional triode electron gun. The feasibility study
for this scheme has been experimentally investigated by modulating a triode gun
of the Y-824 cathode-grid (KG) structure provided by the CPI Eimac, with
2.856-GHz pulsed RF generated by a solid-state amplifier (SSA). In this paper,
we present the methods and results of this investigation.",eConf C000821 (2000) MOB18
cross-border CPI,http://arxiv.org/abs/0807.4219v1,Statistical properties of world investment networks,"We have performed a detailed investigation on the world investment networks
constructed from the Coordinated Portfolio Investment Survey (CPIS) data of the
International Monetary Fund, ranging from 2001 to 2006. The distributions of
degrees and node strengthes are scale-free. The weight distributions can be
well modeled by the Weibull distribution. The maximum flow spanning trees of
the world investment networks possess two universal allometric scaling
relations, independent of time and the investment type. The topological scaling
exponent is $1.17\pm0.02$ and the flow scaling exponent is $1.03\pm0.01$.","Physica A 388 (12), 2450-2460 (2009)"
cross-border CPI,http://arxiv.org/abs/1507.02456v2,Towards Log-Linear Logics with Concrete Domains,"We present $\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an
extension of the log-linear description logics $\mathcal{EL}^{++}$-LL with
concrete domains, nominals, and instances. We use Markov logic networks (MLNs)
in order to find the most probable, classified and coherent $\mathcal{EL}^{++}$
ontology from an $\mathcal{MEL}^{++}$ knowledge base. In particular, we develop
a novel way to deal with concrete domains (also known as datatypes) by
extending MLN's cutting plane inference (CPI) algorithm.",
cross-border CPI,http://arxiv.org/abs/1504.04974v1,Understanding Big Data Analytic Workloads on Modern Processors,"Big data analytics applications play a significant role in data centers, and
hence it has become increasingly important to understand their behaviors in
order to further improve the performance of data center computer systems, in
which characterizing representative workloads is a key practical problem. In
this paper, after investigating three most impor- tant application domains in
terms of page views and daily visitors, we chose 11 repre- sentative data
analytics workloads and characterized their micro-architectural behaviors by
using hardware performance counters, so as to understand the impacts and
implications of data analytics workloads on the systems equipped with modern
superscalar out-of-order processors. Our study reveals that big data analytics
applications themselves share many inherent characteristics, which place them
in a different class from traditional workloads and scale-out services. To
further understand the characteristics of big data analytics work- loads we
performed a correlation analysis of CPI (cycles per instruction) with other
micro- architecture level characteristics and an investigation of the big data
software stack impacts on application behaviors. Our correlation analysis
showed that even though big data ana- lytics workloads own notable pipeline
front end stalls, the main factors affecting the CPI performance are long
latency data accesses rather than the front end stalls. Our software stack
investigation found that the typical big data software stack significantly
contributes to the front end stalls and incurs bigger working set. Finally we
gave several recommen- dations for architects, programmers and big data system
designers with the knowledge acquired from this paper.",
cross-border CPI,http://arxiv.org/abs/1901.06103v1,"Exploring Semi-supervised Variational Autoencoders for Biomedical
  Relation Extraction","The biomedical literature provides a rich source of knowledge such as
protein-protein interactions (PPIs), drug-drug interactions (DDIs) and
chemical-protein interactions (CPIs). Biomedical relation extraction aims to
automatically extract biomedical relations from biomedical text for various
biomedical research. State-of-the-art methods for biomedical relation
extraction are primarily based on supervised machine learning and therefore
depend on (sufficient) labeled data. However, creating large sets of training
data is prohibitively expensive and labor-intensive, especially so in
biomedicine as domain knowledge is required. In contrast, there is a large
amount of unlabeled biomedical text available in PubMed. Hence, computational
methods capable of employing unlabeled data to reduce the burden of manual
annotation are of particular interest in biomedical relation extraction. We
present a novel semi-supervised approach based on variational autoencoder (VAE)
for biomedical relation extraction. Our model consists of the following three
parts, a classifier, an encoder and a decoder. The classifier is implemented
using multi-layer convolutional neural networks (CNNs), and the encoder and
decoder are implemented using both bidirectional long short-term memory
networks (Bi-LSTMs) and CNNs, respectively. The semi-supervised mechanism
allows our model to learn features from both the labeled and unlabeled data. We
evaluate our method on multiple public PPI, DDI and CPI corpora. Experimental
results show that our method effectively exploits the unlabeled data to improve
the performance and reduce the dependence on labeled data. To our best
knowledge, this is the first semi-supervised VAE-based method for (biomedical)
relation extraction. Our results suggest that exploiting such unlabeled data
can be greatly beneficial to improved performance in various biomedical
relation extraction.",
cross-border CPI,http://arxiv.org/abs/1309.7119v3,"Stock price direction prediction by directly using prices data: an
  empirical study on the KOSPI and HSI","The prediction of a stock market direction may serve as an early
recommendation system for short-term investors and as an early financial
distress warning system for long-term shareholders. Many stock prediction
studies focus on using macroeconomic indicators, such as CPI and GDP, to train
the prediction model. However, daily data of the macroeconomic indicators are
almost impossible to obtain. Thus, those methods are difficult to be employed
in practice. In this paper, we propose a method that directly uses prices data
to predict market index direction and stock price direction. An extensive
empirical study of the proposed method is presented on the Korean Composite
Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual
constituents included in the indices. The experimental results show notably
high hit ratios in predicting the movements of the individual constituents in
the KOSPI and HIS.",
cross-border CPI,http://arxiv.org/abs/1602.07367v1,"Classical realization of dispersion-canceled, artifact-free, and
  background-free optical coherence tomography","Quantum-optical coherence tomography (Q-OCT) provides a dispersion-canceled
axial-imaging method, but its practical use is limited by the weakness of the
light source and by artifacts in the images. A recent study using chirped-pulse
interferometry (CPI) has demonstrated dispersion-canceled and artifact-free OCT
with a classical system; however, unwanted background signals still remain
after removing the artifacts. Here, we propose a classical optical method that
realizes dispersion-canceled, artifact-free, and background-free OCT. We employ
a time-reversed system for Q-OCT with transform-limited input laser pulses to
achieve dispersion-canceled OCT with a classical system. We have also
introduced a subtraction method to remove artifacts and background signals.
With these methods, we experimentally demonstrated dispersion-canceled,
artifact-free, and background-free axial imaging of a coverglass and
cross-sectional imaging of the surface of a coin.","Optics Express Vol. 24, Issue 8, pp. 8280-8289 (2016)"
cross-border CPI,http://arxiv.org/abs/1607.02818v1,Equation-free analysis of a dynamically evolving multigraph,"In order to illustrate the adaptation of traditional continuum numerical
techniques to the study of complex network systems, we use the equation-free
framework to analyze a dynamically evolving multigraph. This approach is based
on coupling short intervals of direct dynamic network simulation with
appropriately-defined lifting and restriction operators, mapping the detailed
network description to suitable macroscopic (coarse-grained) variables and
back. This enables the acceleration of direct simulations through Coarse
Projective Integration (CPI), as well as the identification of coarse
stationary states via a Newton-GMRES method. We also demonstrate the use of
data-mining, both linear (principal component analysis, PCA) and nonlinear
(diffusion maps, DMAPS) to determine good macroscopic variables (observables)
through which one can coarse-grain the model. These results suggest methods for
decreasing simulation times of dynamic real-world systems such as
epidemiological network models. Additionally, the data-mining techniques could
be applied to a diverse class of problems to search for a succint,
low-dimensional description of the system in a small number of variables.",
cross-border CPI,http://arxiv.org/abs/1212.2044v2,Macro-Economic Time Series Modeling and Interaction Networks,"Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.","Applications of Evolutionary Computation, LNCS 6625 (Springer
  Berlin Heidelberg), pp. 101-110 (2011)"
cross-border CPI,http://arxiv.org/abs/1903.00191v1,"MIPS-Core Application Specific Instruction-Set Processor for IDEA
  Cryptography - Comparison between Single-Cycle and Multi-Cycle Architectures","A single-cycle processor completes the execution of an instruction in only
one clock cycle. However, its clock period is usually rather long. On the
contrary, although clock frequency is higher in a multi-cycle processor, it
takes several clock cycles to finish an instruction. Therefore, their runtime
efficiencies depend on which program is executed. This paper presents a new
processor for International Data Encryption Algorithm (IDEA) cryptography. The
new design is an Application Specific Instruction-set Processor (ASIP) in which
both general-purpose and special instructions are supported. It is a
single-cycle MIPS-core architecture, whose average Clocks Per Instruction (CPI)
is 1. Furthermore, a comparison is provided in this paper to show the
differences between the proposed single-cycle processor and another comparable
multi-cycle crypto processor. FPGA implementation results show that both
architectures have almost the same encoding/decoding throughput. However, the
previous processor consumes nearly twice as many resources as the new one does.",
cross-border CPI,http://arxiv.org/abs/1909.02769v1,"Adaptive Trust Region Policy Optimization: Global Convergence and Faster
  Rates for Regularized MDPs","Trust region policy optimization (TRPO) is a popular and empirically
successful policy search algorithm in Reinforcement Learning (RL) in which a
surrogate problem, that restricts consecutive policies to be `close' to one
another, is iteratively solved. Nevertheless, TRPO has been considered a
heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show
that the adaptive scaling mechanism used in TRPO is in fact the natural ""RL
version"" of traditional trust-region methods from convex analysis. We first
analyze TRPO in the planning setting, in which we have access to the model and
the entire state space. Then, we consider sample-based TRPO and establish
$\tilde O(1/\sqrt{N})$ convergence rate to the global optimum. Importantly, the
adaptive scaling mechanism allows us to analyze TRPO in {\em regularized MDPs}
for which we prove fast rates of $\tilde O(1/N)$, much like results in convex
optimization. This is the first result in RL of better rates when regularizing
the instantaneous cost or reward.",
cross-border CPI,http://arxiv.org/abs/1301.5653v2,"A High-power 650 MHz CW Magnetron Transmitter for Intensity Frontier
  Superconducting Accelerators","A concept of a 650 MHz CW magnetron transmitter with fast control in phase
and power, based on two-stage injection-locked CW magnetrons, has been proposed
to drive Superconducting Cavities (SC) for intensity-frontier accelerators. The
concept is based on a theoretical model considering a magnetron as a forced
oscillator and experimentally verified with a 2.5 MW pulsed magnetron. To
fulfill fast control of phase and output power requirements of SC accelerators,
both two-stage injection-locked CW magnetrons are combined with a 3-dB hybrid.
Fast control in output power is achieved by varying the input phase of one of
the magnetrons. For output power up to 250 kW we expect the output/input power
ratio to be about 35 to 40 dB in CW or quasi-CW mode with long pulse duration.
All magnetrons of the transmitter should be based on commercially available
models to decrease the cost of the system. An experimental model using 1 kW,
CW, S-band, injection-locked magnetrons with a 3-dB hybrid combiner has been
developed and built for study. A description of the model, simulations, and
experimental results are presented and discussed in this work.",
cross-border CPI,http://arxiv.org/abs/1809.00730v1,"Global energy fluxes in fully-developed turbulent channels with flow
  control","This paper addresses the integral energy fluxes in natural and controlled
turbulent channel flows, where active skin-friction drag reduction techniques
allow a more efficient use of the available power. We study whether the
increased efficiency shows any general trend in how energy is dissipated by the
mean velocity field (mean dissipation) and by the fluctuating velocity field
(turbulent dissipation).
  Direct Numerical Simulations (DNS) of different control strategies are
performed at Constant Power Input (CPI), so that at statistical equilibrium
each flow (either uncontrolled or controlled by different means) has the same
power input, hence the same global energy flux and, by definition, the same
total energy dissipation rate. The simulations reveal that changes in mean and
turbulent energy dissipation rates can be of either sign in a successfully
controlled flow.
  A quantitative description of these changes is made possible by a new
decomposition of the total dissipation, stemming from an extended Reynolds
decomposition, where the mean velocity is split into a laminar component and a
deviation from it. Thanks to the analytical expressions of the laminar
quantities, exact relationships are derived that link the achieved flow rate
increase and all energy fluxes in the flow system with two wall-normal
integrals of the Reynolds shear stress and the Reynolds number. The dependence
of the energy fluxes on the Reynolds number is elucidated with a simple model
in which the control-dependent changes of the Reynolds shear stress are
accounted for via a modification of the mean velocity profile. The physical
meaning of the energy fluxes stemming from the new decomposition unveils their
inter-relations and connection to flow control, so that a clear target for flow
control can be identified.",
cross-border wages,http://arxiv.org/abs/1909.12338v1,Hardware Design and Analysis of the ACE and WAGE Ciphers,"This paper presents the hardware design and analysis of ACE and WAGE, two
candidate ciphers for the NIST Lightweight Cryptography standardization. Both
ciphers use sLiSCP's unified sponge duplex mode. ACE has an internal state of
320 bits, uses three 64 bit Simeck boxes, and implements both authenticated
encryption and hashing. WAGE is based on the Welch-Gong stream cipher and
provides authenticated encryption. WAGE has 259 bits of state, two 7 bit
Welch-Gong permutations, and four lightweight 7 bit S-boxes. ACE and WAGE have
the same external interface and follow the same I/O protocol to transition
between phases. The paper illustrates how a hardware perspective influenced key
aspects of the ACE and WAGE algorithms. The paper reports area, power, and
energy results for both serial and parallel (unrolled) implementations using
four different ASIC libraries: two 65 nm libraries, a 90 nm library, and a 130
nm library. ACE implementations range from a throughput of 0.5 bits-per-clock
cycle (bpc) and an area of 4210 GE (averaged across the four ASIC libraries) up
to 4 bpc and 7260 GE. WAGE results range from 0.57 bpc with 2920 GE to 4.57 bpc
with 11080 GE.",
cross-border wages,http://arxiv.org/abs/1001.0627v1,The Labor Economics of Paid Crowdsourcing,"Crowdsourcing is a form of ""peer production"" in which work traditionally
performed by an employee is outsourced to an ""undefined, generally large group
of people in the form of an open call."" We present a model of workers supplying
labor to paid crowdsourcing projects. We also introduce a novel method for
estimating a worker's reservation wage--the smallest wage a worker is willing
to accept for a task and the key parameter in our labor supply model. It shows
that the reservation wages of a sample of workers from Amazon's Mechanical Turk
(AMT) are approximately log normally distributed, with a median wage of
$1.38/hour. At the median wage, the point elasticity of extensive labor supply
is 0.43. We discuss how to use our calibrated model to make predictions in
applied work. Two experimental tests of the model show that many workers
respond rationally to offered incentives. However, a non-trivial fraction of
subjects appear to set earnings targets. These ""target earners"" consider not
just the offered wage--which is what the rational model predicts--but also
their proximity to earnings goals. Interestingly, a number of workers clearly
prefer earning total amounts evenly divisible by 5, presumably because these
amounts make good targets.",
cross-border wages,http://arxiv.org/abs/1609.09067v1,Using Big Data to Decode Private Sector Wage Growth,"The U.S. labor market is dynamic and complex, and understanding wage data
across different segments of the workforce is critical to providing
policymakers and business leaders with actionable insights. There is no labor
index that assesses the labor market performance at such a detailed level as
the ADP Research Institute's Workforce Vitality Report (WVR). Drawing on the
actual, aggregated and anonymous payroll data of 24 million Americans paid by
ADP, the WVR looks at key dynamics and market indicators including wage growth,
hours worked and turnover rate. Unlike other data sets, the WVR calculates wage
growth of individual workers on a quarter-to-quarter basis, avoiding the
deviations caused by various workplace occurrences, like when new workers are
hired and older ones retire. In this paper, Dr. Ahu Yildirmaz, head of the ADP
Research Institute, drills down into wage growth by industry, age, gender and
income level, as well as for both job holders and job switchers. Using WVR
data, Ahu walks through those factors contributing to overall shifts in wage
growth, the future of the labor market and what this data means for today's
U.S. workforce.",
cross-border wages,http://arxiv.org/abs/1712.05796v2,A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk,"A growing number of people are working as part of on-line crowd work, which
has been characterized by its low wages; yet, we know little about wage
distribution and causes of low/high earnings. We recorded 2,676 workers
performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis
revealed that workers earned a median hourly wage of only ~\$2/h, and only 4%
earned more than \$7.25/h. The average requester pays more than \$11/h,
although lower-paying requesters post much more work. Our wage calculations are
influenced by how unpaid work is included in our wage calculations, e.g., time
spent searching for tasks, working on tasks that are rejected, and working on
tasks that are ultimately not submitted. We further explore the characteristics
of tasks and working patterns that yield higher hourly wages. Our analysis
informs future platform design and worker tools to create a more positive
future for crowd work.",
cross-border wages,http://arxiv.org/abs/physics/0506229v1,"The Age-Competency Model to the Study of the Age-Wage Profiles for
  Workers","In this article, I present a new approach and a novel model to the study of
the life cycle of wages. The key idea is that wage can be thought as
remuneration paid for the competency. It is assumed with the approach that
there are three mechanisms acting at micro level and resulting in the change of
workers' competencies during their lives. These are an endogenous growth of
workers' initial competencies; a rate of investments in schooling in the life
cycle of wages; and an effect of relative losses in workers' competencies. The
developed model is to shed light on the processes resulting in the age-wage
profiles seen in mass. The model obeys a nonlinear integro-differential
equation. The found analytic solution of the equation has the form of Fisk PDF
of a special type. The solution and its features are discussed. The regression
technique is used to check the model upon reliability. The model provides
better fitting to the data (Elo and Salonen, 2004) than minceraninan earnings
function (Mincer, 1974) does.",
cross-border wages,http://arxiv.org/abs/1706.10097v3,Design Activism for Minimum Wage Crowd Work,"Entry-level crowd work is often reported to pay less than minimum wage. While
this may be appropriate or even necessary, due to various legal, economic, and
pragmatic factors, some Requesters and workers continue to question this status
quo. To promote further discussion on the issue, we survey Requesters and
workers whether they would support restricting tasks to require minimum wage
pay. As a form of design activism, we confronted workers with this dilemma
directly by posting a dummy Mechanical Turk task which told them that they
could not work on it because it paid less than their local minimum wage, and we
invited their feedback. Strikingly, for those workers expressing an opinion,
two-thirds of Indians favored the policy while two-thirds of Americans opposed
it. Though a majority of Requesters supported minimum wage pay, only 20\% would
enforce it. To further empower Requesters, and to ensure that effort or
ignorance are not barriers to change, we provide a simple public API to make it
easy to find a worker's local minimum wage by his/her IP address.",
cross-border wages,http://arxiv.org/abs/1903.07032v1,TurkScanner: Predicting the Hourly Wage of Microtasks,"Workers in crowd markets struggle to earn a living. One reason for this is
that it is difficult for workers to accurately gauge the hourly wages of
microtasks, and they consequently end up performing labor with little pay. In
general, workers are provided with little information about tasks, and are left
to rely on noisy signals, such as textual description of the task or rating of
the requester. This study explores various computational methods for predicting
the working times (and thus hourly wages) required for tasks based on data
collected from other workers completing crowd work. We provide the following
contributions. (i) A data collection method for gathering real-world training
data on crowd-work tasks and the times required for workers to complete them;
(ii) TurkScanner: a machine learning approach that predicts the necessary
working time to complete a task (and can thus implicitly provide the expected
hourly wage). We collected 9,155 data records using a web browser extension
installed by 84 Amazon Mechanical Turk workers, and explored the challenge of
accurately recording working times both automatically and by asking workers.
TurkScanner was created using ~150 derived features, and was able to predict
the hourly wages of 69.6% of all the tested microtasks within a 75% error.
Directions for future research include observing the effects of tools on
people's working practices, adapting this approach to a requester tool for
better price setting, and predicting other elements of work (e.g., the
acceptance likelihood and worker task preferences.)",
cross-border wages,http://arxiv.org/abs/1810.07781v2,"Responsible team players wanted: an analysis of soft skill requirements
  in job advertisements","During the past decades the importance of soft skills for labour market
outcomes has grown substantially. This carries implications for labour market
inequality, since previous research shows that soft skills are not valued
equally across race and gender. This work explores the role of soft skills in
job advertisements by drawing on methods from computational science as well as
on theoretical and empirical insights from economics, sociology and psychology.
We present a semi-automatic approach based on crowdsourcing and text mining for
extracting a list of soft skills. We find that soft skills are a crucial
component of job ads, especially of low-paid jobs and jobs in female-dominated
professions. Our work shows that soft skills can serve as partial predictors of
the gender composition in job categories and that not all soft skills receive
equal wage returns at the labour market. Especially ""female"" skills are
frequently associated with wage penalties. Our results expand the growing
literature on the association of soft skills on wage inequality and highlight
their importance for occupational gender segregation at labour markets.",
cross-border wages,http://arxiv.org/abs/1905.12535v1,Ride-share matching algorithms generate income inequality,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",
cross-border wages,http://arxiv.org/abs/1705.07643v5,Near-Feasible Stable Matchings with Budget Constraints,"We consider the matching with contracts framework of Hatfield and Milgrom
when one side (a firm or hospital) can make monetary transfers (offer wages) to
the other (a worker or doctor). In a standard model, monetary transfers are not
restricted. However, we assume that each hospital has a fixed budget; that is,
the total amount of wages allocated by each hospital to the doctors is
constrained. With this constraint, stable matchings may fail to exist and
checking for the existence is hard. To deal with the nonexistence, we focus on
near-feasible matchings that can exceed each hospital budget by a certain
amount, and We introduce a new concept of compatibility. We show that the
compatibility condition is a sufficient condition for the existence of a
near-feasible stable matching in the matching with contracts framework. Under a
slight restriction on hospitals' preferences, we provide mechanisms that
efficiently return a near-feasible stable matching with respect to the actual
amount of wages allocated by each hospital. By sacrificing strategy-proofness,
the best possible bound of budget excess is achieved.",
cross-border wages,http://arxiv.org/abs/1601.05664v1,Defining urban agglomerations to detect agglomeration economies,"Agglomeration economies are a persistent subject of debate among economists
and urban planners. Their definition turns on whether or not larger cities and
regions are more efficient and more productive than smaller ones. We complement
existing discussion on agglomeration economies and the urban wage premium here
by providing a sensitivity analysis of estimated coefficients to different
delineations of urban agglomeration as well as to different definitions of the
economic measure that summarises the urban premium. This quantity can consist
of total wages measured at the place of work, or of income registered at the
place of residence. The chosen option influences the scaling behaviour of city
size as well as the spatial distribution of the phenomenon at the city level.
Spatial discrepancies between the distribution of jobs and the distribution of
households at different economic levels makes city definitions crucial to the
estimation of economic relations which vary with city size. We argue this point
by regressing measures of income and wage over about five thousands different
definitions of cities in France, based on our algorithmic aggregation of
administrative spatial units at regular cutoffs which reflect density,
population thresholds and commuting flows. We also go beyond aggregated
observations of wages and income by searching for evidence of larger
inequalities and economic segregation in the largest cities. This paper
therefore considers the spatial and economic complexity of cities with respect
to discussion about how we measure agglomeration economies. It provides a basis
for reflection on alternative ways to model the processes which lead to
observed variations, and this can provide insights for more comprehensive
regional planning.","Cottineau, C., Finance, O., Hatna, E., Arcaute, E., & Batty, M.
  (2018). Defining urban clusters to detect agglomeration economies.
  Environment and Planning B: Urban Analytics and City Science,
  2399808318755146"
cross-border wages,http://arxiv.org/abs/1510.05189v2,Causal Falling Rule Lists,"A causal falling rule list (CFRL) is a sequence of if-then rules that
specifies heterogeneous treatment effects, where (i) the order of rules
determines the treatment effect subgroup a subject belongs to, and (ii) the
treatment effect decreases monotonically down the list. A given CFRL
parameterizes a hierarchical bayesian regression model in which the treatment
effects are incorporated as parameters, and assumed constant within
model-specific subgroups. We formulate the search for the CFRL best supported
by the data as a Bayesian model selection problem, where we perform a search
over the space of CFRL models, and approximate the evidence for a given CFRL
model using standard variational techniques. We apply CFRL to a census wage
dataset to identify subgroups of differing wage inequalities between men and
women.",
cross-border wages,http://arxiv.org/abs/1404.6103v1,"An Approximate ""Law of One Price"" in Random Assignment Games","Assignment games represent a tractable yet versatile model of two-sided
markets with transfers. We study the likely properties of the core of randomly
generated assignment games. If the joint productivities of every firm and
worker are i.i.d bounded random variables, then with high probability all
workers are paid roughly equal wages, and all firms make similar profits. This
implies that core allocations vary significantly in balanced markets, but that
there is core convergence in even slightly unbalanced markets. For the
benchmark case of uniform distribution, we provide a tight bound for the
workers' share of the surplus under the firm-optimal core allocation. We
present simulation results suggesting that the phenomena analyzed appear even
in medium-sized markets. Finally, we briefly discuss the effects of unbounded
distributions and the ways in which they may affect wage dispersion.",
cross-border wages,http://arxiv.org/abs/1802.04680v1,Training and Inference with Integers in Deep Neural Networks,"Researches on deep neural networks with discrete parameters and their
deployment in embedded systems have been active and promising topics. Although
previous works have successfully reduced precision in inference, transferring
both training and inference processes to low-bitwidth integers has not been
demonstrated simultaneously. In this work, we develop a new method termed as
""WAGE"" to discretize both training and inference, where weights (W),
activations (A), gradients (G) and errors (E) among layers are shifted and
linearly constrained to low-bitwidth integers. To perform pure discrete
dataflow for fixed-point devices, we further replace batch normalization by a
constant scaling layer and simplify other components that are arduous for
integer implementation. Improved accuracies can be obtained on multiple
datasets, which indicates that WAGE somehow acts as a type of regularization.
Empirically, we demonstrate the potential to deploy training in hardware
systems such as integer-based deep learning accelerators and neuromorphic chips
with comparable accuracy and higher energy efficiency, which is crucial to
future AI applications in variable scenarios with transfer and continual
learning demands.",
cross-border wages,http://arxiv.org/abs/physics/0505173v1,Empirical study and model of personal income,"Personal income distributions in Japan are analyzed empirically and a simple
stochastic model of the income process is proposed. Based on empirical facts,
we propose a minimal two-factor model. Our model of personal income consists of
an asset accumulation process and a wage process. We show that these simple
processes can successfully reproduce the empirical distribution of income. In
particular, the model can reproduce the particular transition of the
distribution shape from the middle part to the tail part. This model also
allows us to derive the tail exponent of the distribution analytically.",
cross-border wages,http://arxiv.org/abs/physics/0510248v1,A Harris-Todaro Agent-Based Model to Rural-Urban Migration,"The Harris-Todaro model of the rural-urban migration process is revisited
under an agent-based approach. The migration of the workers is interpreted as a
process of social learning by imitation, formalized by a computational model.
By simulating this model, we observe a transitional dynamics with continuous
growth of the urban fraction of overall population toward an equilibrium. Such
an equilibrium is characterized by stabilization of rural-urban expected wages
differential (generalized Harris-Todaro equilibrium condition), urban
concentration and urban unemployment. These classic results obtained originally
by Harris and Todaro are emergent properties of our model.",
cross-border wages,http://arxiv.org/abs/1708.01876v1,The Countries' Relation Formation Problem: I and II,"This paper integrates the studies of various countries' behaviors, e.g.,
waging wars and entering into military alliances, into a general framework of
\emph{countries' relation formation}, which consists of two components, i.e., a
static game and a dynamical system. Aside from being a stand-alone framework
itself, this paper can also be seen as a necessary extension of a recently
developed \emph{countries' power allocation game} in \cite{allocation}. We
establish certain theoretical results, such as pure strategy Nash equilibrium
existence in the static game, and propose several applications of interest made
possible by combining both frameworks of countries' power allocation and
relation formation.",
cross-border wages,http://arxiv.org/abs/1803.06563v1,Viewpoint: Artificial Intelligence and Labour,"The welfare of modern societies has been intrinsically linked to wage labour.
With some exceptions, the modern human has to sell her labour-power to be able
reproduce biologically and socially. Thus, a lingering fear of technological
unemployment features predominately as a theme among Artificial Intelligence
researchers. In this short paper we show that, if past trends are anything to
go by, this fear is irrational. On the contrary, we argue that the main problem
humanity will be facing is the normalisation of extremely long working hours.",
cross-border wages,http://arxiv.org/abs/1602.01578v2,Modeling the relation between income and commuting distance,"We discuss the distribution of commuting distances and its relation to
income. Using data from Denmark, the UK, and the US, we show that the commuting
distance is (i) broadly distributed with a slow decaying tail that can be
fitted by a power law with exponent $\gamma \approx 3$ and (ii) an average
growing slowly as a power law with an exponent less than one that depends on
the country considered. The classical theory for job search is based on the
idea that workers evaluate the wage of potential jobs as they arrive
sequentially through time, and extending this model with space, we obtain
predictions that are strongly contradicted by our empirical findings. We
propose an alternative model that is based on the idea that workers evaluate
potential jobs based on a quality aspect and that workers search for jobs
sequentially across space. We also assume that the density of potential jobs
depends on the skills of the worker and decreases with the wage. The predicted
distribution of commuting distances decays as $1/r^{3}$ and is independent of
the distribution of the quality of jobs. We find our alternative model to be in
agreement with our data. This type of approach opens new perspectives for the
modeling of mobility.",J. R. Soc. Interface 13:20160306 (2016)
cross-border wages,http://arxiv.org/abs/1711.07359v2,Approximately Stable Matchings with Budget Constraints,"This paper considers two-sided matching with budget constraints where one
side (firm or hospital) can make monetary transfers (offer wages) to the other
(worker or doctor). In a standard model, while multiple doctors can be matched
to a single hospital, a hospital has a maximum quota: the number of doctors
assigned to a hospital cannot exceed a certain limit. In our model, a hospital
instead has a fixed budget: the total amount of wages allocated by each
hospital to doctors is constrained. With budget constraints, stable matchings
may fail to exist and checking for the existence is hard. To deal with the
nonexistence of stable matchings, we extend the ""matching with contracts"" model
of Hatfield and Milgrom, so that it handles approximately stable matchings
where each of the hospitals' utilities after deviation can increase by factor
up to a certain amount. We then propose two novel mechanisms that efficiently
return such a stable matching that exactly satisfies the budget constraints. In
particular, by sacrificing strategy-proofness, our first mechanism achieves the
best possible bound. Furthermore, we find a special case such that a simple
mechanism is strategy-proof for doctors, keeping the best possible bound of the
general case.",
cross-border wages,http://arxiv.org/abs/1710.06285v2,"Preliminary steps toward a universal economic dynamics for monetary and
  fiscal policy","We consider the relationship between economic activity and intervention,
including monetary and fiscal policy, using a universal dynamic framework.
Central bank policies are designed for growth without excess inflation.
However, unemployment, investment, consumption, and inflation are interlinked.
Understanding dynamics is crucial to assessing the effects of policy,
especially in the aftermath of the financial crisis. Here we lay out a program
of research into monetary and economic dynamics and preliminary steps toward
its execution. We use principles of response theory to derive implications for
policy. We find that the current approach, which considers the overall money
supply, is insufficient to regulate economic growth. While it can achieve some
degree of control, optimizing growth also requires a fiscal policy balancing
monetary injection between two dominant loop flows, the consumption and wages
loop, and investment and returns loop. The balance arises from a composite of
government tax, entitlement, subsidy policies, corporate policies, as well as
monetary policy. We show empirically that a transition occurred in 1980 between
two regimes--an oversupply to the consumption and wages loop, to an oversupply
of the investment and returns loop. The imbalance is manifest in savings and
borrowing by consumers and investors, and in inflation. The latter increased
until 1980, and decreased subsequently, resulting in a zero rate largely
unrelated to the financial crisis. Three recessions and the financial crisis
are part of this dynamic. Optimizing growth now requires shifting the balance.
Our analysis supports advocates of greater income and / or government support
for the poor who use a larger fraction of income for consumption. This promotes
investment due to growth in demand. Otherwise, investment opportunities are
limited, capital remains uninvested, and does not contribute to growth.",
cross-border wages,http://arxiv.org/abs/physics/0506021v1,An agent-based model to rural-urban migration analysis,"In this paper we analyze the rural-urban migration phenomena as it is usually
observed in economies which are in the early stages of industrialization. The
analysis is conducted by means of a statistical mechanics approach which builds
a computational agent-based model. Agents are placed on a lattice and the
connections among them are described via an Ising like model. Simulations on
this computational model show some emergent properties that are common in
developing economies, such as a transitional dynamics characterized by
continuous growth of urban population, followed by the equalization of expected
wages between rural and urban sectors (Harris-Todaro equilibrium condition),
urban concentration and increasing of per capita income.",
cross-border wages,http://arxiv.org/abs/physics/0512268v4,"The origin of Iraq's nuclear weapons program: Technical reality and
  Western hypocrisy","This report is based on a series of papers written between 1980 and 2005 on
the origin of Iraq's nuclear weapons program, which was known to one of the
authors in the late 1970s already, as well as to a number of other physicists,
who independently tried without success to inform their governments and the
public.
  It is concluded that at no point did the Western governments effectively try
to stop Iraq's nuclear weapons program, which suggests that its existence was
useful as a foreign policy tool, as is confirmed by its use as a major
justification to wage two wars on Iraq.",
cross-border wages,http://arxiv.org/abs/1401.1302v1,Optimization in Knowledge-Intensive Crowdsourcing,"We present SmartCrowd, a framework for optimizing collaborative
knowledge-intensive crowdsourcing. SmartCrowd distinguishes itself by
accounting for human factors in the process of assigning tasks to workers.
Human factors designate workers' expertise in different skills, their expected
minimum wage, and their availability. In SmartCrowd, we formulate task
assignment as an optimization problem, and rely on pre-indexing workers and
maintaining the indexes adaptively, in such a way that the task assignment
process gets optimized both qualitatively, and computation time-wise. We
present rigorous theoretical analyses of the optimization problem and propose
optimal and approximation algorithms. We finally perform extensive performance
and quality experiments using real and synthetic data to demonstrate that
adaptive indexing in SmartCrowd is necessary to achieve efficient high quality
task assignment.",
cross-border wages,http://arxiv.org/abs/1410.3811v2,"Applications of statistical physics distributions to several types of
  income","This paper explores several types of income which have not been explored so
far by authors who tackled income and wealth distribution using Statistical
Physics. The main types of income we plan to analyze are income before
redistribution (or gross income), income of retired people (or pensions), and
income of active people (mostly wages). The distributions used to analyze
income distributions are Fermi-Dirac distribution and polynomial distribution
(as this is present in describing the behavior of dynamic systems in certain
aspects). The data we utilize for our analysis are from France and the UK. We
find that both distributions are robust in describing these varieties of
income. The main finding we consider to be the applicability of these
distributions to pensions, which are not regulated entirely by market
mechanisms.",
cross-border wages,http://arxiv.org/abs/1111.1429v1,Promoting Industry-University Partnership in Information Technology,"It is becoming increasingly difficult for Nigerian universities to go it
alone in terms of serving as a citadel of learning, coping with the huge wage
bill and competing with their peers in other parts of the world, due to
competitive, economic and other pressures. As a consequence, Nigerian
universities are left with no option than to carry their industrial partners
along in terms of research and development through the formation of
partnerships for their mutual benefit. Since the industries are established for
profit making and the universities for knowledge enhancement, such partnerships
would help in spreading the costs in terms of provision of knowledge and costs
of research. This paper discusses the various types of partnerships involving
industries and universities, the benefits derived and a possible model for the
working of such a partnership which could be adapted to other sectors and
countries in sub-Saharan Africa.",
cross-border wages,http://arxiv.org/abs/1403.5641v1,"Control over adversarial packet-dropping communication networks
  revisited","We revisit a one-step control problem over an adversarial packet-dropping
link. The link is modeled as a set of binary channels controlled by a strategic
jammer whose intention is to wage a `denial of service' attack on the plant by
choosing a most damaging channel-switching strategy. The paper introduces a
class of zero-sum games between the jammer and controller as a scenario for
such attack, and derives necessary and sufficient conditions for these games to
have a nontrivial saddle-point equilibrium. At this equilibrium, the jammer's
optimal policy is to randomize in a region of the plant's state space, thus
requiring the controller to undertake a nontrivial response which is different
from what one would expect in a standard stochastic control problem over a
packet dropping channel.",
cross-border wages,http://arxiv.org/abs/1404.4895v1,A matheuristic approach for the Pollution-Routing Problem,"This paper deals with the Pollution-Routing Problem (PRP), a Vehicle Routing
Problem (VRP) with environmental considerations, recently introduced in the
literature by [Bektas and Laporte (2011), Transport. Res. B-Meth. 45 (8),
1232-1250]. The objective is to minimize operational and environmental costs
while respecting capacity constraints and service time windows. Costs are based
on driver wages and fuel consumption, which depends on many factors, such as
travel distance and vehicle load. The vehicle speeds are considered as decision
variables. They complement routing decisions, impacting the total cost, the
travel time between locations, and thus the set of feasible routes. We propose
a method which combines a local search-based metaheuristic with an integer
programming approach over a set covering formulation and a recursive
speed-optimization algorithm. This hybridization enables to integrate more
tightly route and speed decisions. Moreover, two other ""green"" VRP variants,
the Fuel Consumption VRP (FCVRP) and the Energy Minimizing VRP (EMVRP), are
addressed. The proposed method compares very favorably with previous algorithms
from the literature and many new improved solutions are reported.",
cross-border wages,http://arxiv.org/abs/1706.01559v1,"Controller-jammer game models of Denial of Service in control systems
  operating over packet-dropping links","The paper introduces a class of zero-sum games between the adversary and
controller as a scenario for a `denial of service' in a networked control
system. The communication link is modeled as a set of transmission regimes
controlled by a strategic jammer whose intention is to wage an attack on the
plant by choosing a most damaging regime-switching strategy. We demonstrate
that even in the one-step case, the introduced games admit a saddle-point
equilibrium, at which the jammer's optimal policy is to randomize in a region
of the plant's state space, thus requiring the controller to undertake a
nontrivial response which is different from what one would expect in a standard
stochastic control problem over a packet dropping link. The paper derives
conditions for the introduced games to have such a saddle-point equilibrium.
Furthermore, we show that in more general multi-stage games, these conditions
provide `greedy' jamming strategies for the adversary.",
cross-border wages,http://arxiv.org/abs/1801.07512v1,Alonso and the Scaling of Urban Profiles,"The scaling of urban characteristics with total population has become an
important research field since one needs to better understand the challenges of
urban densification. Yet urban scaling research is largely disconnected from
intra-urban structure. In contrast, the monocentric model of Alonso provides a
residential choice-based theory to urban density profiles. However, it is
silent about how these profiles scale with population, thus preventing
empirical scaling studies to anchor in a strong micro-economic theory. This
paper bridges this gap by introducing power laws for land, income and transport
cost in the Alonso model. From this augmented model, we derive the conditions
at which the equilibrium urban structure matches recent empirical findings
about the scaling of urban land and population density profiles in European
cities. We find that the Alonso model is theoretically compatible with the
observed scaling of population density profiles and satisfactorily represents
European cities. This compatibility however challenges current empirical
understanding of wage and transport cost elasticities with population, and
requires a scaling of the housing land profile that is different from the
observed. Our results call for revisiting theories about land development and
housing processes as well as the empirics of agglomeration benefits and
transport costs.",
cross-border wages,http://arxiv.org/abs/1807.05856v1,Proton Radiation Damage Experiment on a Hybrid CMOS Detector,"We report on the initial results of an experiment to determine the effects of
proton radiation damage on an X-ray hybrid CMOS detector (HCD). The device was
irradiated at the Edwards Accelerator Lab at Ohio University with 8 MeV
protons, up to a total absorbed dose of 3 krad(Si) (4.5 $\times$ 10$^9$
protons/cm$^2$). The effects of this radiation on read noise, dark current,
gain, and energy resolution are then analyzed. This exposure is the first of
several which will be used for characterizing detector performance at absorbed
dose levels that are relevant for imaging devices operating in a deep-space
environment.","Proc. SPIE 10709, Proton Radiation Damage Experiment on a Hybrid
  CMOS Detector, 107090L (2018)"
cross-border wages,http://arxiv.org/abs/1808.09325v1,"""Life never matters in the DEMOCRATS MIND"": Examining Strategies of
  Retweeted Social Bots During a Mass Shooting Event","This exploratory study examines the strategies of social bots on Twitter that
were retweeted following a mass shooting event. Using a case study method to
frame our work, we collected over seven million tweets during a one-month
period following a mass shooting in Parkland, Florida. From this dataset, we
selected retweets of content generated by over 400 social bot accounts to
determine what strategies these bots were using and the effectiveness of these
strategies as indicated by the number of retweets. We employed qualitative and
quantitative methods to capture both macro- and micro-level perspectives. Our
findings suggest that bots engage in more diverse strategies than solely waging
disinformation campaigns, including baiting and sharing information. Further,
we found that while bots amplify conversation about mass shootings, humans were
primarily responsible for disseminating bot-generated content. These findings
add depth to the current understanding of bot strategies and their
effectiveness. Understanding these strategies can inform efforts to combat
dubious information as well as more insidious disinformation campaigns.",
cross-border market capitalisation,http://arxiv.org/abs/1803.03088v2,"Classification of cryptocurrency coins and tokens by the dynamics of
  their market capitalisations","We empirically verify that the market capitalisations of coins and tokens in
the cryptocurrency universe follow power-law distributions with significantly
different values, with the tail exponent falling between 0.5 and 0.7 for coins,
and between 1.0 and 1.3 for tokens. We provide a rationale for this, based on a
simple proportional growth with birth & death model previously employed to
describe the size distribution of firms, cities, webpages, etc. We empirically
validate the model and its main predictions, in terms of proportional growth
(Gibrat's law) of the coins and tokens. Estimating the main parameters of the
model, the theoretical predictions for the power-law exponents of coin and
token distributions are in remarkable agreement with the empirical estimations,
given the simplicity of the model. Our results clearly characterize coins as
being ""entrenched incumbents"" and tokens as an ""explosive immature ecosystem"",
largely due to massive and exuberant Initial Coin Offering activity in the
token space. The theory predicts that the exponent for tokens should converge
to 1 in the future, reflecting a more reasonable rate of new entrants
associated with genuine technological innovations.","Royal Society Open Science 5 (9), 2018"
cross-border market capitalisation,http://arxiv.org/abs/1902.04517v2,"Wikipedia and Digital Currencies: Interplay Between Collective Attention
  and Market Performance","The production and consumption of information about Bitcoin and other
digital-, or 'crypto'-, currencies have grown together with their market
capitalisation. However, a systematic investigation of the relationship between
online attention and market dynamics, across multiple digital currencies, is
still lacking. Here, we quantify the interplay between the attention towards
digital currencies in Wikipedia and their market performance. We consider the
entire edit history of currency-related pages, and their view history from July
2015. First, we quantify the evolution of the cryptocurrency presence in
Wikipedia by analysing the editorial activity and the network of co-edited
pages. We find that a small community of tightly connected editors is
responsible for most of the production of information about cryptocurrencies in
Wikipedia. Then, we show that a simple trading strategy informed by Wikipedia
views performs better, in terms of returns on investment, than classic baseline
strategies for most of the covered period. Our results contribute to the recent
literature on the interplay between online information and investment markets,
and we anticipate it will be of interest for researchers as well as investors.",
cross-border market capitalisation,http://arxiv.org/abs/1809.09805v4,"Towards Safer Smart Contracts: A Survey of Languages and Verification
  Methods","With a market capitalisation of over USD 205 billion in just under ten years,
public distributed ledgers have experienced significant adoption. Apart from
novel consensus mechanisms, their success is also accountable to smart
contracts. These programs allow distrusting parties to enter agreements that
are executed autonomously. However, implementation issues in smart contracts
caused severe losses to the users of such contracts. Significant efforts are
taken to improve their security by introducing new programming languages and
advance verification methods. We provide a survey of those efforts in two
parts. First, we introduce several smart contract languages focussing on
security features. To that end, we present an overview concerning paradigm,
type, instruction set, semantics, and metering. Second, we examine verification
tools and methods for smart contract and distributed ledgers. Accordingly, we
introduce their verification approach, level of automation, coverage, and
supported languages. Last, we present future research directions including
formal semantics, verified compilers, and automated verification.",
cross-border market capitalisation,http://arxiv.org/abs/1402.1438v1,Deployment of an Innovative Resource Choice Method for Process Planning,"Designers, process planners and manufacturers naturally consider different
concepts for a same object. The stiffness of production means and the design
specification requirements mark out process planners as responsible of the
coherent integration of all constraints. First, this paper details an
innovative solution of resource choice, applied for aircraft manufacturing
parts. In a second part, key concepts are instanced for the considered
industrial domain. Finally, a digital mock up validates the solution viability
and demonstrates the possibility of an in-process knowledge capitalisation and
use. Formalising the link between Design and Manufacturing allows to hope
enhancements of simultaneous Product / Process developments.","CIRP Journal of Manufacturing Systems 35, 5 (2006) 487-506"
cross-border market capitalisation,http://arxiv.org/abs/1505.03305v1,Transformation of marketing in the e-commerce,"The article is about transformation of the theory and practice of marketing
in the conditions of e-commerce and network economy. The author considers
Internet-marketing as an independent kind of marketing in the virtual
communicative environment. The basic thesis of the article: the virtual
environment defines marketing transformation, changing methods, priorities and
structure at practice and then theories of marketing.",Practical marketing. 2013. No 1 (191). P. 4-16
cross-border market capitalisation,http://arxiv.org/abs/physics/0509090v2,Effects of the globalization in the Korean financial markets,"We study the effect of globalization on the Korean market, one of the
emerging markets. Some characteristics of the Korean market are different from
those of the mature market according to the latest market data, and this is due
to the influence of foreign markets or investors. We concentrate on the market
network structures over the past two decades with knowledge of the history of
the market, and determine the globalization effect and market integration as a
function of time.","J. Korean Phys. Soc. 48, pp.S135-S138 (2006)."
cross-border market capitalisation,http://arxiv.org/abs/physics/0512210v1,"Micro-economic Analysis of the Physical Constrained Markets: Game Theory
  Application to Competitive Electricity Markets","Competition has been introduced in the electricity markets with the goal of
reducing prices and improving efficiency. The basic idea which stays behind
this choice is that, in competitive markets, a greater quantity of the good is
exchanged at a lower and a lower price, leading to higher market efficiency.
Electricity markets are pretty different from other commodities mainly due to
the physical constraints related to the network structure that may impact the
market performance. The network structure of the system on which the economic
transactions need to be undertaken poses strict physical and operational
constraints. Strategic interactions among producers that game the market with
the objective of maximizing their producer surplus must be taken into account
when modeling competitive electricity markets. The physical constraints,
specific of the electricity markets, provide additional opportunity of gaming
to the market players. Game theory provides a tool to model such a context.
This paper discussed the application of game theory to physical constrained
electricity markets with the goal of providing tools for assessing the market
performance and pinpointing the critical network constraints that may impact
the market efficiency. The basic models of game theory specifically designed to
represent the electricity markets will be presented. IEEE30 bus test system of
the constrained electricity market will be discussed to show the network
impacts on the market performances in presence of strategic bidding behavior of
the producers.",
cross-border market capitalisation,http://arxiv.org/abs/1505.03303v1,Dropshipping - alternative infrastructure of sales and promotion,"An article about the transformation of the theory and practice of marketing
in terms of e-commerce and network economy. The author considers Internet
Marketing as an independent marketing communication in a virtual environment.
The main thesis of the article: virtual environment determines the
transformation of marketing, changing methods, priorities and structure not
only practice, but also the theory of marketing.",Marketing in Russia and Abroad. 2012. No 1 (87). P. 90-104
cross-border market capitalisation,http://arxiv.org/abs/1003.0034v1,A New Understanding of Prediction Markets Via No-Regret Learning,"We explore the striking mathematical connections that exist between market
scoring rules, cost function based prediction markets, and no-regret learning.
We show that any cost function based prediction market can be interpreted as an
algorithm for the commonly studied problem of learning from expert advice by
equating trades made in the market with losses observed by the learning
algorithm. If the loss of the market organizer is bounded, this bound can be
used to derive an O(sqrt(T)) regret bound for the corresponding learning
algorithm. We then show that the class of markets with convex cost functions
exactly corresponds to the class of Follow the Regularized Leader learning
algorithms, with the choice of a cost function in the market corresponding to
the choice of a regularizer in the learning problem. Finally, we show an
equivalence between market scoring rules and prediction markets with convex
cost functions. This implies that market scoring rules can also be interpreted
naturally as Follow the Regularized Leader algorithms, and may be of
independent interest. These connections provide new insight into how it is that
commonly studied markets, such as the Logarithmic Market Scoring Rule, can
aggregate opinions into accurate estimates of the likelihood of future events.",
cross-border market capitalisation,http://arxiv.org/abs/1206.5252v1,A Utility Framework for Bounded-Loss Market Makers,"We introduce a class of utility-based market makers that always accept orders
at their risk-neutral prices. We derive necessary and sufficient conditions for
such market makers to have bounded loss. We prove that hyperbolic absolute risk
aversion utility market makers are equivalent to weighted pseudospherical
scoring rule market makers. In particular, Hanson's logarithmic scoring rule
market maker corresponds to a negative exponential utility market maker in our
framework. We describe a third equivalent formulation based on maintaining a
cost function that seems most natural for implementation purposes, and we
illustrate how to translate among the three equivalent formulations. We examine
the tradeoff between the market's liquidity and the market maker's worst-case
loss. For a fixed bound on worst-case loss, some market makers exhibit greater
liquidity near uniform prices and some exhibit greater liquidity near extreme
prices, but no market maker can exhibit uniformly greater liquidity in all
regimes. For a fixed minimum liquidity level, we give the lower bound of market
maker's worst-case loss under some regularity conditions.",
cross-border market capitalisation,http://arxiv.org/abs/1406.7584v1,"Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of
  Anchoring ""Bias""","In an online prediction market, forecasters who could not see the current
state of the market until they made their own separate estimates moved their
estimates closer to the market forecast when the current state of the market
became known. Their first edits to the market forecast were very similar to the
first edits of forecasters who could always see the current state of the
market, and forecasters in both conditions had similar accuracy. These results
suggest that our more elaborate forecast elicitation method might not improve
forecasts and that any anchoring on the state of the market does not constitute
an error in judgment.",
cross-border market capitalisation,http://arxiv.org/abs/1201.1425v1,"Interconnection of Communities of Practice: A Web Platform for Knowledge
  Management","Our works aim at developing a Web platform to connect various Communities of
Practice (CoPs) and to capitalise on all their knowledge. This platform
addresses CoPs interested in a same general activity, for example tutoring. For
that purpose, we propose a general model of Interconnection of Communities of
Practice (ICP), based on the concept of Constellation of Practice (CCP)
developed by Wenger (1998). The model of ICP was implemented and has been used
to develop the TE-Cap 2 platform which has, as its field of application,
educational tutoring activities. In particular, we propose an indexation and
search tool for the ICP knowledge base. The TE-Cap 2 platform has been used in
real conditions. We present the main results of this descriptive investigation
to validate this work.","Firth International Conference on Knowledge Management and
  Information Sharing (KMIS 2009), Madeira : Portugal (2009)"
cross-border market capitalisation,http://arxiv.org/abs/1201.4949v1,Approximate Message Passing under Finite Alphabet Constraints,"In this paper we consider Basis Pursuit De-Noising (BPDN) problems in which
the sparse original signal is drawn from a finite alphabet. To solve this
problem we propose an iterative message passing algorithm, which capitalises
not only on the sparsity but by means of a prior distribution also on the
discrete nature of the original signal. In our numerical experiments we test
this algorithm in combination with a Rademacher measurement matrix and a
measurement matrix derived from the random demodulator, which enables
compressive sampling of analogue signals. Our results show in both cases
significant performance gains over a linear programming based approach to the
considered BPDN problem. We also compare the proposed algorithm to a similar
message passing based algorithm without prior knowledge and observe an even
larger performance improvement.",
cross-border market capitalisation,http://arxiv.org/abs/1407.0576v1,Novelty Search in Competitive Coevolution,"One of the main motivations for the use of competitive coevolution systems is
their ability to capitalise on arms races between competing species to evolve
increasingly sophisticated solutions. Such arms races can, however, be hard to
sustain, and it has been shown that the competing species often converge
prematurely to certain classes of behaviours. In this paper, we investigate if
and how novelty search, an evolutionary technique driven by behavioural
novelty, can overcome convergence in coevolution. We propose three methods for
applying novelty search to coevolutionary systems with two species: (i) score
both populations according to behavioural novelty; (ii) score one population
according to novelty, and the other according to fitness; and (iii) score both
populations with a combination of novelty and fitness. We evaluate the methods
in a predator-prey pursuit task. Our results show that novelty-based approaches
can evolve a significantly more diverse set of solutions, when compared to
traditional fitness-based coevolution.","Parallel Problem Solving from Nature (PPSN). vol. 8672 LNCS. pp.
  233-242. Springer (2014)"
cross-border market capitalisation,http://arxiv.org/abs/1507.05762v1,"Horn Clauses as an Intermediate Representation for Program Analysis and
  Transformation","Many recent analyses for conventional imperative programs begin by
transforming programs into logic programs, capitalising on existing LP analyses
and simple LP semantics. We propose using logic programs as an intermediate
program representation throughout the compilation process. With restrictions
ensuring determinism and single-modedness, a logic program can easily be
transformed to machine language or other low-level language, while maintaining
the simple semantics that makes it suitable as a language for program analysis
and transformation. We present a simple LP language that enforces determinism
and single-modedness, and show that it makes a convenient program
representation for analysis and transformation.",Theory and Practice of Logic Programming 15 (2015) 526-542
cross-border market capitalisation,http://arxiv.org/abs/1601.03101v1,Magneto-Plasmonic Nanoantennas: Basics and Applications (Review),"Plasmonic nanoantennas is a hot and rapidly expanding research field. Here we
overview basic operating principles and applications of novel magneto-plasmonic
nanoantennas, which are made of ferromagnetic metals and driven not only by
light, but also by external magnetic fields. We demonstrate that
magneto-plasmonic nanoantennas enhance the magneto-optical effects, which
introduces additional degrees of freedom in the control of light at the
nano-scale. This property is used in conceptually new devices such as
magneto-plasmonic rulers, ultra-sensitive biosensors, one-way subwavelength
waveguides and extraordinary optical transmission structures, as well as in
novel biomedical imaging modalities. We also point out that in certain cases
'non-optical' ferromagnetic nanostructures may operate as magneto-plasmonic
nanoantennas. This undesigned extra functionality capitalises on established
optical characterisation techniques of magnetic nanomaterials and it may be
useful for the integration of nanophotonics and nanomagnetism on a single chip.",
cross-border market capitalisation,http://arxiv.org/abs/1606.07138v3,"Airbnb in tourist cities: comparing spatial patterns of hotels and
  peer-to-peer accommodation","In recent years, what has become known as collaborative consumption has
undergone rapid expansion through peer-to-peer (P2P) platforms. In the field of
tourism, a particularly notable example is that of Airbnb. This article
analyses the spatial patterns of Airbnb in Barcelona and compares them with
hotels and sightseeing spots. New sources of data, such as Airbnb listings and
geolocated photographs are used. Analysis of bivariate spatial autocorrelation
reveals a close spatial relationship between Airbnb and hotels, with a marked
centre-periphery pattern, although Airbnb predominates around the main hotel
axis and hotels predominate in some peripheral areas of the city. Another
interesting finding is that Airbnb capitalises more on the advantages of
proximity to the main tourist attractions of the city than does the hotel
sector. Finally, it was possible to detect those parts of the city that have
seen the greatest increase in pressure from tourism related to Airbnb's recent
expansion.","Tourism Management 62:278-291, May 2017"
cross-border market capitalisation,http://arxiv.org/abs/1905.00288v1,Beyond Mobile Apps: A Survey of Technologies for Mental Well-being,"Mental health problems are on the rise globally and strain national health
systems worldwide. Mental disorders are closely associated with fear of stigma,
structural barriers such as financial burden, and lack of available services
and resources which often prohibit the delivery of frequent clinical advice and
monitoring. Technologies for mental well-being exhibit a range of attractive
properties which facilitate the delivery of state of the art clinical
monitoring. This review article provides an overview of traditional techniques
followed by their technological alternatives, sensing devices, behaviour
changing tools, and feedback interfaces. The challenges presented by these
technologies are then discussed with data collection, privacy and battery life
being some of the key issues which need to be carefully considered for the
successful deployment of mental health tool-kits. Finally, the opportunities
this growing research area presents are discussed including the use of portable
tangible interfaces combining sensing and feedback technologies. Capitalising
on the captured data these ubiquitous devices offer, state of the art machine
learning algorithms can lead to the develop",
cross-border market capitalisation,http://arxiv.org/abs/0806.4466v1,Klein - Gordon equation for market wealth operations,"In this paper the modified Klein - Gordon equation for market processes is
proposed and solved. It is argued that the oscillations in market propagate
with the light velocity. The initial pulse in the market is damped and for very
large time diffused according to the Fourier law.",
cross-border market capitalisation,http://arxiv.org/abs/1505.02766v1,Features of transformation of marketing in e-commerce,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.","Omsk Scientific Bulletin. 2013. No 1 (115). P. 55-58. ISSN
  1813-8225"
cross-border market capitalisation,http://arxiv.org/abs/physics/0607071v1,Market Polarization in Presence of Individual Choice Volatility,"Financial markets are subject to long periods of polarized behavior, such as
bull-market or bear-market phases, in which the vast majority of market
participants seem to almost exclusively choose one action (between buying or
selling) over the other. From the point of view of conventional economic
theory, such events are thought to reflect the arrival of ``external news''
that justifies the observed behavior. However, empirical observations of the
events leading up to such market phases, as well events occurring during the
lifetime of such a phase, have often failed to find significant correlation
between news from outside the market and the behavior of the agents comprising
the market. In this paper, we explore the alternative hypothesis that the
occurrence of such market polarizations are due to interactions amongst the
agents in the market, and not due to any influence external to it. In
particular, we present a model where the market (i.e., the aggregate behavior
of all the agents) is observed to become polarized even though individual
agents regularly change their actions (buy or sell) on a time-scale much
shorter than that of the market polarization phase.",
cross-border market capitalisation,http://arxiv.org/abs/physics/0608016v2,Market Efficiency in Foreign Exchange Markets,"We investigate the relative market efficiency in financial market data, using
the approximate entropy(ApEn) method for a quantification of randomness in time
series. We used the global foreign exchange market indices for 17 countries
during two periods from 1984 to 1998 and from 1999 to 2004 in order to study
the efficiency of various foreign exchange markets around the market crisis. We
found that on average, the ApEn values for European and North American foreign
exchange markets are larger than those for African and Asian ones except Japan.
We also found that the ApEn for Asian markets increase significantly after the
Asian currency crisis. Our results suggest that the markets with a larger
liquidity such as European and North American foreign exchange markets have a
higher market efficiency than those with a smaller liquidity such as the
African and Asian ones except Japan.",
cross-border market capitalisation,http://arxiv.org/abs/1703.10689v1,Computing Equilibrium in Matching Markets,"Market equilibria of matching markets offer an intuitive and fair solution
for matching problems without money with agents who have preferences over the
items. Such a matching market can be viewed as a variation of Fisher market,
albeit with rather peculiar preferences of agents. These preferences can be
described by piece-wise linear concave (PLC) functions, which however, are not
separable (due to each agent only asking for one item), are not monotone, and
do not satisfy the gross substitute property-- increase in price of an item can
result in increased demand for the item. Devanur and Kannan in FOCS 08 showed
that market clearing prices can be found in polynomial time in markets with
fixed number of items and general PLC preferences. They also consider Fischer
markets with fixed number of agents (instead of fixed number of items), and
give a polynomial time algorithm for this case if preferences are separable
functions of the items, in addition to being PLC functions.
  Our main result is a polynomial time algorithm for finding market clearing
prices in matching markets with fixed number of different agent preferences,
despite that the utility corresponding to matching markets is not separable. We
also give a simpler algorithm for the case of matching markets with fixed
number of different items.",
cross-border market capitalisation,http://arxiv.org/abs/physics/0505032v1,Automated Trading Systems: Developed and Emerging Capital Markets,"Automated trading systems on developed and emerging capital markets are
studied in this paper. The standard for developed market is automated trading
system with 40-days simple moving average. We tested it for the index SIX
Industrial for 1000 and 730 trading days of the slovak emerging capital market.
The Buy and Hold trading system was 7.80 times more profitable than this etalon
trading system for active trading. Taking of profitable standard trading system
from a developed capital market does not lead to optimal results on the
emerging capital markets. We then studied optimized standard trading system
based on the simple moving average. The parameter of optimization was the
number of weeks. An optimal system was that with 5 weeks. This trading system
has some of its characteristics comparable with the etalon trading system on
the NYSE Composite Index. The emerging market is more risky than the developed
market. The profit on the emerging market is also higher. The range of
optimized system parameter is quite robust. Observed was increase of number of
trades in the range from the 21 weeks to the 25 weeks. This indicates creation
of a new optimal middle range trading system. Results of testing for liquid
shares are quantitatively similar.",
cross-border market capitalisation,http://arxiv.org/abs/1508.07272v1,"Market Formation as Transitive Closure: the Evolving Pattern of Trade in
  Music","Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.",
cross-border market capitalisation,http://arxiv.org/abs/1809.03684v2,"Visual Attention Model for Cross-sectional Stock Return Prediction and
  End-to-End Multimodal Market Representation Learning","Technical and fundamental analysis are traditional tools used to analyze
individual stocks; however, the finance literature has shown that the price
movement of each individual stock correlates heavily with other stocks,
especially those within the same sector. In this paper we propose a general
purpose market representation that incorporates fundamental and technical
indicators and relationships between individual stocks. We treat the daily
stock market as a ""market image"" where rows (grouped by market sector)
represent individual stocks and columns represent indicators. We apply a
convolutional neural network over this market image to build market features in
a hierarchical way. We use a recurrent neural network, with an attention
mechanism over the market feature maps, to model temporal dynamics in the
market. We show that our proposed model outperforms strong baselines in both
short-term and long-term stock return prediction tasks. We also show another
use for our market image: to construct concise and dense market embeddings
suitable for downstream prediction tasks.",
cross-border market capitalisation,http://arxiv.org/abs/cs/0109053v1,Price Increases from Online Privacy,"Consumers value keeping some information about them private from potential
marketers. E-commerce dramatically increases the potential for marketers to
accumulate otherwise private information about potential customers. Online
marketers claim that this information enables them to better market their
products. Policy makers are currently drafting rules to regulate the way in
which these marketers can collect, store, and share this information. However,
there is little evidence yet either of consumers' valuation of their privacy or
of the benefits they might reap through better target marketing. We provide a
framework for measuring a portion of the benefits from allowing marketers to
make better use of consumer information. Target marketing is likely to reduce
consumer search costs, improve consumer product selection decisions, and lower
the marketing costs of goods sold. Our model allows us to estimate the value to
consumers of only the latter, price reductions from more efficient marketing.",
cross-border market capitalisation,http://arxiv.org/abs/1202.1712v1,Multi-outcome and Multidimensional Market Scoring Rules,"Hanson's market scoring rules allow us to design a prediction market that
still gives useful information even if we have an illiquid market with a
limited number of budget-constrained agents. Each agent can ""move"" the current
price of a market towards their prediction.
  While this movement still occurs in multi-outcome or multidimensional markets
we show that no market-scoring rule, under reasonable conditions, always moves
the price directly towards beliefs of the agents. We present a modified version
of a market scoring rule for budget-limited traders, and show that it does have
the property that, from any starting position, optimal trade by a
budget-limited trader will result in the market being moved towards the
trader's true belief. This mechanism also retains several attractive strategic
properties of the market scoring rule.",
cross-border market capitalisation,http://arxiv.org/abs/1603.07210v2,Computing Equilibria in Markets with Budget-Additive Utilities,"We present the first analysis of Fisher markets with buyers that have
budget-additive utility functions. Budget-additive utilities are elementary
concave functions with numerous applications in online adword markets and
revenue optimization problems. They extend the standard case of linear
utilities and have been studied in a variety of other market models. In
contrast to the frequently studied CES utilities, they have a global satiation
point which can imply multiple market equilibria with quite different
characteristics. Our main result is an efficient combinatorial algorithm to
compute a market equilibrium with a Pareto-optimal allocation of goods. It
relies on a new descending-price approach and, as a special case, also implies
a novel combinatorial algorithm for computing a market equilibrium in linear
Fisher markets. We complement these positive results with a number of hardness
results for related computational questions. We prove that it is NP-hard to
compute a market equilibrium that maximizes social welfare, and it is PPAD-hard
to find any market equilibrium with utility functions with separate satiation
points for each buyer and each good.",
cross-border market capitalisation,http://arxiv.org/abs/1403.0648v1,"Multi-period Trading Prediction Markets with Connections to Machine
  Learning","We present a new model for prediction markets, in which we use risk measures
to model agents and introduce a market maker to describe the trading process.
This specific choice on modelling tools brings us mathematical convenience. The
analysis shows that the whole market effectively approaches a global objective,
despite that the market is designed such that each agent only cares about its
own goal. Additionally, the market dynamics provides a sensible algorithm for
optimising the global objective. An intimate connection between machine
learning and our markets is thus established, such that we could 1) analyse a
market by applying machine learning methods to the global objective, and 2)
solve machine learning problems by setting up and running certain markets.",
cross-border bond yields,http://arxiv.org/abs/1602.02423v1,Catch bond mechanism in Dynein motor driven collective transport,"Recent experiments have demonstrated that dynein motor exhibits catch bonding
behaviour, in which the unbinding rate of a single dynein decreases with
increasing force, for a certain range of force. Motivated by these experiments,
we propose a model for catch bonding in dynein using a threshold force bond
deformation (TFBD) model wherein catch bonding sets in beyond a critical
applied load force. We study the effect of catch bonding on unidirectional
transport properties of cellular cargo carried by multiple dynein motors within
the framework of this model. We find catch bonding can result in dramatic
changes in the transport properties, which are in sharp contrast to kinesin
driven unidirectional transport, where catch bonding is absent. We predict
that, under certain conditions, the average velocity of the cellular cargo can
actually increase as applied load is increased. We characterize the transport
properties in terms of a velocity profile phase plot in the parameter space of
the catch bond strength and the stall force of the motor. This phase plot
yields predictions that may be experimentally accessed by suitable
modifications of motor transport and binding properties. Our work necessitates
a reexamination of existing theories of collective bidirectional transport of
cellular cargo where the catch bond effect of dynein described in this paper is
expected to play a crucial role.",
cross-border bond yields,http://arxiv.org/abs/1808.02951v1,"Constructing a Non-additive Non-interacting Kinetic Energy Functional
  Approximation for Covalent Bonds from Exact Conditions","We present a non-decomposable approximation for the non-additive
non-interacting kinetic energy (NAKE) for covalent bonds based on the exact
behavior of the von Weizs\""{a}cker (vW) functional in regions dominated by one
orbital. This covalent approximation (CA) seamlessly combines the vW and the
Thomas-Fermi (TF) functional with a switching function of the fragment
densities constructed to satisfy exact constraints. It also makes use of
ensembles and fractionally-occupied spin-orbitals to yield highly accurate NAKE
for stretched bonds while outperforming other standard NAKE approximations near
equilibrium bond lengths. We tested the CA within Partition-Density Functional
Theory (P-DFT) and demonstrated its potential to enable fast and accurate P-DFT
calculations.",
cross-border bond yields,http://arxiv.org/abs/1407.1998v1,"Wave-function and density functional theory studies of dihydrogen
  complexes","We performed a benchmark study on a series of dihydrogen bond complexes and
constructed a set of reference bond distances and interaction energies. The
test set was employed to assess the performance of several wave-function
correlated and density functional theory methods. We found that second-order
correlation methods describe relatively well the dihydrogen complexes. However,
for high accuracy inclusion of triple contributions is important. On the other
hand, none of the considered density functional methods can simultaneously
yield accurate bond lengths and interaction energies. However, we found that
improved results can be obtained by the inclusion of non-local exchange
contributions.","J. Chem. Theory Comput. 10, 3151 (2014)"
cross-border bond yields,http://arxiv.org/abs/physics/0507098v1,Ab initio yield curve dynamics,"We derive an equation of motion for interest-rate yield curves by applying a
minimum Fisher information variational approach to the implied probability
density. By construction, solutions to the equation of motion recover observed
bond prices. More significantly, the form of the resulting equation explains
the success of the Nelson Siegel approach to fitting static yield curves and
the empirically observed modal structure of yield curves. A practical numerical
implementation of this equation of motion is found by using the Karhunen-Loeve
expansion and Galerkin's method to formulate a reduced-order model of yield
curve dynamics.",
cross-border bond yields,http://arxiv.org/abs/1311.2467v2,Funding the Search for Extraterrestrial Intelligence with a Lottery Bond,"I propose the establishment of a SETI Lottery Bond to provide a continued
source of funding for the search for extraterrestrial intelligence (SETI). The
SETI Lottery Bond is a fixed rate perpetual bond with a lottery at maturity,
where maturity occurs only upon discovery and confirmation of extraterrestrial
intelligent life. Investors in the SETI Lottery Bond purchase shares that yield
a fixed rate of interest that continues indefinitely until SETI succeeds---at
which point a random subset of shares will be awarded a prize from a lottery
pool. SETI Lottery Bond shares also are transferable, so that investors can
benefact their shares to kin or trade them in secondary markets. The total
capital raised this way will provide a fund to be managed by a financial
institution, with annual payments from this fund to support SETI research, pay
investor interest, and contribute to the lottery fund. Such a plan could
generate several to tens of millions of dollars for SETI research each year,
which would help to revitalize and expand facilities such as the Allen
Telescope Array. The SETI Lottery Bond is a savings product that only can be
offered by a financial institution with authorization to engage in banking and
gaming activities. I therefore suggest that one or more banks offer a
lottery-linked savings product in support of SETI research, with the added
benefit of promoting personal savings and intergenerational wealth building
among individuals.",
cross-border bond yields,http://arxiv.org/abs/0903.3502v1,"QM/MM Simulation of the Hydrogen Bond Dynamics of an Adenine:Uracil Base
  Pair in Solution. Geometric Correlations and Infrared Spectrum","Hybrid QM(DFT)/MM molecular dynamics simulations have been carried out for
the Watson-Crick base pair of 9-ethyl-8-phenyladenine and 1-cyclohexyluracil in
deuterochloroform solution at room temperature.
  Trajectories are analyzed putting special attention to the geometric
correlations of the $\NHN$ and $\NHO$ hydrogen bonds in the base pair. Further,
based on empirical correlations between the hydrogen bond bond length and the
fundamental NH stretching frequency its fluctuations are obtained along the
trajectory. Using the time dependent frequencies the infrared lineshape is
determined assuming the validity of a second order cumulant expansion. The
deviations for the fundamental transition frequencies are calculated to amount
to less than 2% as compared with experiment. The width of the spectrum for the
$\NHN$ bond is in reasonable agreement with experiment while that for the
$\NHO$ case is underestimated by the present model. Comparing the performance
of different pseudopotentials it is found that the Troullier-Martins
pseudopotential with a 70 Ry cut-off yields the best agreement.",
cross-border bond yields,http://arxiv.org/abs/1607.05249v6,"Performance of a Nonempirical Density Functional on Molecules and
  Hydrogen-Bonded Complexes","Recently, Tao and Mo (TM) derived a meta-generalized gradient approximation
functional based on a model exchange-correlation hole. In this work, the
performance of this functional is assessed on standard test sets, using the
6-311++G(3df,3pd) basis set. These test sets include 223 G3/99 enthalpies of
formation, 99 atomization energies, 76 barrier heights, 58 electron affinities,
8 proton affinities, 96 bond lengths, 82 harmonic vibrational frequencies, 10
hydrogen-bonded molecular complexes, and 22 atomic excitation energies. Our
calculations show that the TM functional can achieve high accuracy for most
properties considered, relative to the LSDA, PBE, and TPSS functionals. In
particular, it yields the best accuracy for proton affinities, harmonic
vibrational frequencies, hydrogen-bonded dissociation energies and bond
lengths, and atomic excitation energies.",
cross-border bond yields,http://arxiv.org/abs/1902.06528v1,A local perspective on conjugation of double bonds in acyclic polyenes,"The study is devoted to elaboration of an alternative image of conjugation in
acyclic polyenes as a weak and essentially local delocalization of
initially-localized pairs of electrons ascribed to individual double bonds
(instead of formation of a completely delocalized electron system as usual). To
this end, polyenes are modelled as sets of weakly interacting formally-double
bonds, where the single bonds represent the interaction between the former and
are treated as a perturbation. Mathematically, the above-formulated goal is
realized by means of a particular version of the non-canonical method of
molecular orbitals (MOs) based on the Brillouin theorem and yielding the
expressions both for total energies and for non-canonical (localized) MOs
(NCMOs) directly without any reference to usual (canonical) MOs. In addition,
total energies and NCMOs are interrelated explicitly in the approach applied,
viz. the former are representable via the so-called delocalization coefficients
of the latter. Adaptation of these general results to the above-specified model
of polyene yields coincidence between the conjugation energy (CE) and the total
delocalization energy of all pairs of electrons contained. Moreover, a local
relation follows between constitution of the nearest environment of a certain
bond, delocalization pattern of the respective pair of electrons and
contribution of just this pair to the total CE of the given polyene. As a
result, different stabilities of distinct polyenes (e.g. of isomers) prove to
be accompanied by variable extents of delocalization of separate pairs of
electrons. Linear and cross-conjugated polyene chains are comparatively
analyzed.",
cross-border bond yields,http://arxiv.org/abs/1505.04976v1,"Coherent control of bond making: The performance of rationally
  phase-shaped femtosecond laser pulses","The first step in the coherent control of a photoinduced binary reaction is
bond making or photoassociation. We have recently demonstrated coherent control
of bond making in multi-photon femtosecond photoassociation of hot magnesium
atoms, using linearly chirped pulses [Levin et al., arXiv:1411.1542]. The
detected yield of photoassociated magnesium dimers was enhanced by positively
chirped pulses which is explained theoretically by a combination of
purification and chirp-dependent Raman transitions. The yield could be further
enhanced by pulse optimization resulting in pulses with an effective linear
chirp and a sub-pulse structure, where the latter allows for exploiting
vibrational coherences. Here, we systematically explore the efficiency of
phase-shaped pulses for the coherent control of bond making, employing a
parametrization of the spectral phases in the form of cosine functions. We find
up to an order of magnitude enhancement of the yield compared to the unshaped
transform-limited pulse. The highly performing pulses all display an overall
temporally increasing instantaneous frequency and are composed of several
overlapping sub-pulses. The time delay between the first two sub-pulses almost
perfectly fits the vibrational frequency of the generated intermediate
wavepacket.These findings are in agreement with chirp-dependent Raman
transitions and exploitation of vibrational dynamics as underlying control
mechanisms.","J. Phys. B 48, 184004 (2015)"
cross-border bond yields,http://arxiv.org/abs/physics/0212007v1,"Ab Initio Molecular Dynamics Study of Aqueous Solvation of Ethanol and
  Ethylene","The structure and dynamics of aqueous solvation of ethanol and ethylene are
studied by DFT-based Car-Parrinello molecular dynamics. We did not find an
enhancement of the structure of the hydrogen bonded network of hydrating water
molecules. Both ethanol and ethylene can easily be accommodated in the
hydrogen-bonded network of water molecules without altering its structure. This
is supports the conclusion from recent neutron diffraction experiments that
there is no hydrophobic hydration around small hydrophobic groups. Analysis of
the electronic charge distribution using Wannier functions shows that the
dipole moment of ethanol increases from 1.8 D to 3.1 D upon solvation, while
the apolar ethylene molecule attains an average dipole moment of 0.5 D. For
ethylene, we identified configurations with $\pi$-H bonded water molecules,
that have rare four-fold hydrogen-bonded water coordination, yielding
instantaneous dipole moments of ethylene of up to 1 D. The results provide
valuable information for the improvement of empirical force fields, and point
out that for an accurate description of the aqueous solvation of ethanol, and
even of the apolar ethylene, polarizable force fields are required.",
cross-border bond yields,http://arxiv.org/abs/1310.5375v3,"Seeking for ultrashort ""non-bonded"" hydrogen-hydrogen contacts in some
  rigid hydrocarbons and their chlorinated derivatives","In this communication a systematic computational survey on some rigid
hydrocarbon skeletons, e.g. half-cage pentacyclododecanes and
tetracyclododecanes, and their chlorinated derivatives in order to seek for the
so-called ultrashort ""non-bonded"" hydrogen-hydrogen contacts is done. It is
demonstrated that upon a proper choice and modifications of the main
hydrocarbon backbones, and addition of some chlorine atoms instead of the
original hydrogen atoms in parts of the employed hydrocarbons, the resulting
strain triggers structural changes yielding ultrashort hydrogen-hydrogen
contacts with inter-nuclear distances as small as 1.38 Angstrom. Such
ultrashort contacts are clearly less than the world record of an ultrashort
non-bonded hydrogen-hydrogen contact, 1.56 Angstrom, very recently realized
experimentally by Pascal and coworkers in in,in-bis(hydrosilane) [J. Am. Chem.
Soc. 135, 13235 (2013)]. The resulting computed structures as well as the
developed methodology for structure design open the door for constructing a
proper set of molecules for future studies on the nature of the so-called
non-bonded hydrogen-hydrogen interactions that is now an active and
controversial area of research.",Structural Chemistry 25 (2014) 1297
cross-border bond yields,http://arxiv.org/abs/1509.09132v3,Bond percolation on multiplex networks,"We present an analytical approach for bond percolation on multiplex networks
and use it to determine the expected size of the giant connected component and
the value of the critical bond occupation probability in these networks. We
advocate the relevance of these tools to the modeling of multilayer robustness
and contribute to the debate on whether any benefit is to be yielded from
studying a full multiplex structure as opposed to its monoplex projection,
especially in the seemingly irrelevant case of a bond occupation probability
that does not depend on the layer. Although we find that in many cases the
predictions of our theory for multiplex networks coincide with previously
derived results for monoplex networks, we also uncover the remarkable result
that for a certain class of multiplex networks, well described by our theory,
new critical phenomena occur as multiple percolation phase transitions are
present. We provide an instance of this phenomenon in a multipex network
constructed from London rail and European air transportation datasets.","Phys. Rev. X 6, 021002 (2016)"
cross-border bond yields,http://arxiv.org/abs/1805.00958v2,Fabrication of micro fluidic cavities using Si-to-glass anodic bonding,"We demonstrate the fabrication of $\sim$1.08 $\mu$m deep microfluidic
cavities with characteristic size as large as 7 mm $\times$ 11 mm or 11 mm
diameter, using a silicon$-$glass anodic bonding technique that does not
require posts to act as separators to define cavity height. Since the phase
diagram of $^3$He is significantly altered under confinement, posts might act
as pinning centers for phase boundaries. The previous generation of cavities
relied on full wafer-bonding which is more prone to failure and requires dicing
post-bonding, whereas the these cavities are made by bonding a pre-cut piece of
Hoya SD-2 glass to a patterned piece of silicon in which the cavity is defined
by etching. Anodic bonding was carried out at 425 $^{\circ}$C with 200 V, and
we observe that pressurizing the cavity to failure ($>$ 30 bar pressure)
results in glass breaking, rather than the glass-silicon bond separation. In
this article, we discuss the detailed fabrication of the cavity, its edges, and
details of the junction between the coin silver fill line and the silicon base
of the cavity that enables a low internal-friction joint. This feature is
important for mass coupling torsional oscillator experimental assays of the
superfluid inertial contribution where a high quality factor ($Q$) improves
frequency resolution. The surface preparation that yields well-characterized
smooth surfaces to eliminate pinning sites, the use of transparent glass as a
cover permitting optical access, low temperature capability and attachment of
pressure-capable ports for fluid access may be features that are important in
other applications.",
cross-border bond yields,http://arxiv.org/abs/physics/0508085v1,"A simple and surprisingly accurate approach to the chemical bond
  obtained from dimensional scaling","We present a new dimensional scaling transformation of the Schrodinger
equation for the two electron bond. This yields, for the first time, a good
description of the two electron bond via D-scaling. There also emerges, in the
large-D limit, an intuitively appealing semiclassical picture, akin to a
molecular model proposed by Niels Bohr in 1913. In this limit, the electrons
are confined to specific orbits in the scaled space, yet the uncertainty
principle is maintained because the scaling leaves invariant the
position-momentum commutator. A first-order perturbation correction,
proportional to 1/D, substantially improves the agreement with the exact ground
state potential energy curve. The present treatment is very simple
mathematically, yet provides a strikingly accurate description of the potential
energy curves for the lowest singlet, triplet and excited states of H_2. We
find the modified D-scaling method also gives good results for other molecules.
It can be combined advantageously with Hartree-Fock and other conventional
methods.","Physical Review Letters, v. 95, 080401 (2005)"
cross-border bond yields,http://arxiv.org/abs/1609.06187v2,"Determination of Bond Wire Failure Probabilities in Microelectronic
  Packages","This work deals with the computation of industry-relevant bond wire failure
probabilities in microelectronic packages. Under operating conditions, a
package is subject to Joule heating that can lead to electrothermally induced
failures. Manufacturing tolerances result, e.g., in uncertain bond wire
geometries that often induce very small failure probabilities requiring a high
number of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC
sampling scheme that combines the use of an expensive computer model with a
cheap surrogate is used. The fraction of surrogate evaluations is maximized
using an iterative procedure, yielding accurate results at reduced cost.
Moreover, the scheme is non-intrusive, i.e., existing code can be reused. The
algorithm is used to compute the failure probability for an example package and
the computational savings are assessed by performing a surrogate efficiency
study.",
cross-border bond yields,http://arxiv.org/abs/1710.00941v1,"Communication: Truncated non-bonded potentials can yield unphysical
  behavior in molecular dynamics simulations of interfaces","Non-bonded potentials are included in most force fields and therefore widely
used in classical molecular dynamics simulations of materials and interfacial
phenomena. It is commonplace to truncate these potentials for computational
efficiency based on the assumption that errors are negligible for reasonable
cutoffs or compensated for by adjusting other interaction parameters. Arising
from a metadynamics study of the wetting transition of water on a solid
substrate, we find that the influence of the cutoff is unexpectedly strong and
can change the character of the wetting transition from continuous to first
order by creating artificial metastable wetting states. Common cutoff
corrections such as the use of a force switching function, a shifted potential,
or a shifted force do not avoid this. Such a qualitative difference urges
caution and suggests that using truncated non-bonded potentials can induce
unphysical behavior that cannot be fully accounted for by adjusting other
interaction parameters.","J. Chem. Phys. 147 (12), 121102 (2017)"
cross-border bond yields,http://arxiv.org/abs/1410.7866v2,"Valence-bond Non-equilibrium Solvation Model for a Twisting Monomethine
  Cyanine","We propose and analyze a two-state valence-bond model of non-equilibrium
solvation effects on the excited-state twisting reaction of monomethine
cyanines. Suppression of this reaction is thought responsible for
environment-dependent fluorescence yield enhancement in these dyes.
Fluorescence is quenched because twisting is accompanied via the formation of
dark twisted intramolecular charge-transfer (TICT) states. For monomethine
cyanines, where the ground state is a superposition of structures with
different bond and charge localization, there are two possible twisting
pathways with different charge localization in the excited state. For
parameters corresponding to symmetric monomethines, the model predicts two
low-energy twisting channels on the excited-state surface that lead to a
manifold of twisted intramolecular charge-transfer (TICT) states. For typical
monomethines, twisting on the excited state surface will occur with a small
barrier or no barrier. Changes in the solvation configuration can
differentially stabilize TICT states in channels corresponding to different
bonds, and that the position of a conical intersection between adiabatic states
moves in response to solvation to stabilize either one channel or the other.
There is a conical intersection seam that grows along the bottom of the
excited-state potential with increasing solvent polarity. For monomethine
cyanines with modest-sized terminal groups in moderately polar solution, the
bottom of the excited-state potential surface is completely spanned by a
conical intersection seam.","J. Chem. Phys. 142, 084502 (2015)"
cross-border bond yields,http://arxiv.org/abs/1503.08041v1,"Experimental and ab initio studies of the novel piperidine-containing
  acetylene glycols","Synthesis routes of novel piperidine-containing diacetylene are presented.
The new molecules are expected to exhibit plant growth stimulation properties.
In particular, the yield in a situation of drought is expected to increase. The
synthesis makes use of the Favorskii reaction between cycloketones/piperidone
and triple-bond containing glycols. The geometries of the obtained molecules
were determined using nuclear magnetic resonance (NMR). The electronic
structure and geometries of the molecules were studied theoretically using
first-principles calculations based on density functional theory. The
calculated geometries agree very well with the experimentally measured ones,
and also allow us to determine bond lengths, angles and charge distributions
inside the molecules. The stability of the OH-radicals located close to the
triple bond and the piperidine/cyclohexane rings was proven by both
experimental and theoretical analyses. The HOMO/LUMO analysis was done in order
to characterize the electron density of the molecule. The calculations show
that triple bond does not participate in intermolecular reactions which
excludes the instability of novel materials as a reason for low production
rate.",
cross-border bond yields,http://arxiv.org/abs/1111.0662v1,"A neutron diffraction study from 6 to 293 K and a macroscopic-scale
  quantum theory of the hydrogen bonded dimers in the crystal of benzoic acid","The crystal of benzoic acid is comprised of tautomeric centrosymmetric dimers
linked through bistable hydrogen bonds. Statistical disorder of the bonding
protons is excluded by neutron diffraction from 6 K to 293 K. In addition to
diffraction data, vibrational spectra and relaxation rates measured with
solid-state-NMR and quasi-elastic neutron scattering are consistent with
wave-like, rather than particle-like protons. We present a macroscopic-scale
quantum theory for the bonding protons represented by a periodic lattice of
fermions. The adiabatic separation, the exclusion principle, and the
antisymmetry postulate yield a static lattice-state immune to decoherence.
According to the theory of quantum measurements, vibrational spectroscopy and
relaxometry involve realizations of decoherence-free Bloch states for nonlocal
symmetry species that did not exist before the measurement. The eigen states
are fully determined by three temperature-independent parameters which are
effectively measured: the energy difference between tautomer sublattices; the
double-well asymmetry for proton oscillators; the delocalization degree of the
wavefunctions. The spontaneous decay of Bloch states accounts for relaxometry
data. On the other hand, static states realized by elastic scattering account
for diffraction data. We conclude that both quantum and classical physics hold
at every temperature.",
cross-border bond yields,http://arxiv.org/abs/1606.04323v1,Effect of hydrogen bonding on infrared absorption intensity,"We consider how the infrared intensity of an O-H stretch in a hydrogen bonded
complex varies as the strength of the H-bond varies from weak to strong. We
obtain trends for the fundamental and overtone transitions as a function of
donor-acceptor distance R, which is a common measure of H-bond strength. Our
calculations use a simple two-diabatic state model that permits symmetric and
asymmetric bonds, i.e. where the proton affinity of the donor and acceptor are
equal and unequal, respectively. The dipole moment function uses a Mecke form
for the free OH dipole moment, associated with the diabatic states. The
transition dipole moment is calculated using one-dimensional vibrational
eigenstates associated with the H-atom transfer coordinate on the ground state
adiabatic surface of our model. Over 20-fold intensity enhancements for the
fundamental are found for strong H-bonds, where there are significant
non-Condon effects. The isotope effect on the intensity yields a non-monotonic
H/D intensity ratio as a function of R, and is enhanced by the secondary
geometric isotope effect. The first overtone intensity is found to vary
non-monotonically with H-bond strength; strong enhancements are possible for
strong H-bonds. Modifying the dipole moment through the Mecke parameters is
found to have a stronger effect on the overtone than the fundamental. We
compare our findings with those for specific molecular systems analysed through
experiments and theory in earlier works. Our model results compare favourably
for strong and medium strength symmetric H-bonds. However, for weak asymmetric
bonds we find much smaller effects than in earlier work.",Chemical Physics 488 & 489 (2017) Page 43
cross-border bond yields,http://arxiv.org/abs/1804.00780v1,"Predicting Competitive and Non-Competitive Torquoselectivity in
  Ring-Opening Reactions using QTAIM and the Stress Tensor","We present a new vector-based representation of the chemical bond referred to
as the bond-path frame-work set $\mathbb{B} = {p, q, r}$, where $p$, $q$ and
$r$ represent three paths with corresponding eigenvector-following path lengths
$\mathbb{H}^{*},\mathbb{H}$ and the bond-path length from the quantum theory of
atoms in molecules (QTAIM). We find that longer path lengths $\mathbb{H}$ of
the ring-opening bonds predict the preference for the transition state inward
(\textbf{TSIC}) or transition state outward (\textbf{TSOC}) ring opening
reactions in agreement with experiment for all five reactions \textbf{R1-R5}.
Competitiveness and non-competitiveness have traditionally been considered
using activation energies. The activation energy however, for \textbf{R3} does
not satisfactorily determine competitiveness or provide consistent agreement
with experimental yields. We choose a selection of five competitive and
non-competitive reactions; methyl-cyclobutene (\textbf{R1}),
ethyl-methyl-cyclobutene (\textbf{R2}), iso-propyl-methyl-cyclobutene
(\textbf{R3}), ter-butyl-methyl-cyclobutene (\textbf{R4}) and
phenyl-methyl-cyclobutene (R5). Therefore, in this investigation we provide a
new criterion, within the QTAIM framework, to determine whether the reactions
\textbf{R1-R5} are competitive or non-competitive. We that find \textbf{R2},
\textbf{R3} and \textbf{R5} are competitive and \textbf{R1} and \textbf{R4} are
non-competitive reactions in contrast to the results from the activation
energies, calling into question the reliability of activation energies.",
cross-border bond yields,http://arxiv.org/abs/1411.1542v1,Coherent Control of Bond Making,"We demonstrate for the first time coherent control of bond making, a
milestone on the way to coherent control of photo-induced bimolecular chemical
reactions. In strong-field multiphoton femtosecond photoassociation
experiments, we find the yield of detected magnesium dimer molecules to be
enhanced for positively chirped pulses and suppressed for negatively chirped
pulses. Our ab initio model shows that control is achieved by purification via
Franck-Condon filtering combined with chirp-dependent Raman transitions.
Experimental closed-loop phase optimization using a learning algorithm yields
an improved pulse that utilizes vibrational coherent dynamics in addition to
chirp-dependent Raman transitions. Our results show that coherent control of
binary photo-reactions is feasible even under thermal conditions.","Phys. Rev. Lett. 114, 233003 (2015)"
cross-border bond yields,http://arxiv.org/abs/0909.5242v3,"Conical Intersections, Charge Transfer and Photoisomerization Pathway
  Selection in a Minimal Model of a Degenerate Monomethine Dye","We propose a minimal model Hamiltonian for the electronic structure of a
monomethine dye, in order to describe the photoisomerization of such dyes. The
model describes interactions between three diabatic electronic states, each of
which can be associated with a valence bond structure. Monomethine dyes are
characterized by a charge-transfer resonance; the indeterminacy of the
single-double bonding structure dictated by the resonance is reflected in a
duality of photoisomerization pathways corresponding to the different methine
bonds. The possible multiplicity of decay channels complicates mechanistic
models of the effect of the environment on fluorescent quantum yields, as well
as coherent control strategies. We examine the extent and topology of
intersection seams between the electronic states of the dye, and how they
relate to charge localization and selection between different decay pathways.
We find that intersections between the S1 and S0 surfaces only occur for large
twist angles. In contrast, S2/S1 intersections can occur near the Franck-Condon
region. When the molecule has left-right symmetry, all intersections are
associated with con- or dis-rotations and never with single bond twists. For
asymmetric molecules (i.e. where the bridge couples more strongly to one end)
then the S2 and S1 surfaces bias torsion about different bonds. Charge
localization and torsion pathway biasing are correlated. We relate our
observations to several recent experimental and theoretical results, which have
been obtained for dyes with similar structure.",
cross-border transaction costs,http://arxiv.org/abs/1702.08478v1,"Risks and Transaction Costs of Distributed-Ledger Fintech: Boundary
  Effects and Consequences","Fintech business models based on distributed ledgers -- and their
smart-contract variants in particular -- offer the prospect of democratizing
access to faster, anywhere-accessible, lower cost, reliable-and-secure
high-quality financial services. In addition to holding great, economically
transformative promise, these business models pose new, little-studied risks
and transaction costs. However, these risks and transaction costs are not
evident during the demonstration and testing phases of development, when
adopters and users are drawn from the community of developers themselves, as
well as from among non-programmer fintech evangelists. Hence, when the new
risks and transaction costs become manifest -- as the fintech business models
are rolled out across the wider economy -- the consequences may also appear to
be new and surprising. The present study represents an effort to get ahead of
these developments by delineating risks and transaction costs inherent in
distributed-ledger- and smart-contracts-based fintech business models. The
analysis focuses on code risk and moral-hazard risk, as well as on
mixed-economy risks and the unintended consequences of replicating
bricks-and-mortar-generation contract forms within the ultra-low
transaction-cost environment of fintech.",
cross-border transaction costs,http://arxiv.org/abs/1409.6771v1,Mitigation of Delayed Management Costs in Transaction-Oriented Systems,"Abundant examples of complex transaction-oriented networks (TONs) can be
found in a variety of disciplines, including information and communication
technology, finances, commodity trading, and real estate. A transaction in a
TON is executed as a sequence of subtransactions associated with the network
nodes, and is committed if every subtransaction is committed. A subtransaction
incurs a two-fold overhead on the host node: the fixed transient operational
cost and the cost of long-term management (e.g. archiving and support) that
potentially grows exponentially with the transaction length. If the overall
cost exceeds the node capacity, the node fails and all subtransaction incident
to the node, and their parent distributed transactions, are aborted. A TON
resilience can be measured in terms of either external workloads or intrinsic
node fault rates that cause the TON to partially or fully choke. We demonstrate
that under certain conditions, these two measures are equivalent. We further
show that the exponential growth of the long-term management costs can be
mitigated by adjusting the effective operational cost: in other words, that the
future maintenance costs could be absorbed into the transient operational
costs.",
cross-border transaction costs,http://arxiv.org/abs/1604.00103v3,Effect of Bitcoin fee on transaction-confirmation process,"In Bitcoin system, transactions are prioritized according to transaction
fees. Transactions without fees are given low priority and likely to wait for
confirmation. Because the demand of micro payment in Bitcoin is expected to
increase due to low remittance cost, it is important to quantitatively
investigate how transactions with small fees of Bitcoin affect the
transaction-confirmation time. In this paper, we analyze the
transaction-confirmation time by queueing theory. We model the
transaction-confirmation process of Bitcoin as a priority queueing system with
batch service, deriving the mean transaction-confirmation time. Numerical
examples show how the demand of transactions with low fees affects the
transaction-confirmation time. We also consider the effect of the maximum block
size on the transaction-confirmation time.",
cross-border transaction costs,http://arxiv.org/abs/1905.00553v1,Empirically Analyzing Ethereum's Gas Mechanism,"Ethereum's Gas mechanism attempts to set transaction fees in accordance with
the computational cost of transaction execution: a cost borne by default by
every node on the network to ensure correct smart contract execution. Gas
encourages users to author transactions that are efficient to execute and in so
doing encourages node diversity, allowing modestly resourced nodes to join and
contribute to the security of the network.
  However, the effectiveness of this scheme relies on Gas costs being correctly
aligned with observed computational costs in reality. In this work, we
performed the first large scale empirical study to understand to what degree
this alignment exists in practice, by collecting and analyzing Tera-bytes worth
of nanosecond-precision transaction execution traces. Besides confirming
potential denial-of-service vectors, our results also shed light on the role of
I/O in transaction costs which remains poorly captured by the current Gas cost
model. Finally, our results suggest that under the current Gas cost model,
nodes with modest computational resources are disadvantaged compared to their
better resourced peers, which we identify as an ongoing threat to node
diversity and network decentralization.",
cross-border transaction costs,http://arxiv.org/abs/1407.6876v2,On Partial Wait-Freedom in Transactional Memory,"Transactional memory (TM) is a convenient synchronization tool that allows
concurrent threads to declare sequences of instructions on shared data as
speculative \emph{transactions} with ""all-or-nothing"" semantics. It is known
that dynamic transactional memory cannot provide \emph{wait-free} progress in
the sense that every transaction commits in a finite number of its own steps.
In this paper, we explore the costs of providing wait-freedom to only a
\emph{subset} of transactions. Since most transactional workloads are believed
to be read-dominated, we require that read-only transactions commit in the
wait-free manner, while updating transactions are guaranteed to commit only if
they run in the absence of concurrency. We show that this kind of partial
wait-freedom, combined with attractive requirements like read invisibility or
disjoint-access parallelism, incurs considerable complexity costs.",
cross-border transaction costs,http://arxiv.org/abs/physics/0003054v2,A new volatility term in the theory of options with transaction costs,"The introduction of transaction costs into the theory of option pricing could
lead not only to the change of return for options, but also to the change of
the volatility. On the base of assumption of the portfolio analysis, a new
equation for option pricing with transaction costs is derived. A new solution
for the option price is obtained for the time close to expiration date.",
cross-border transaction costs,http://arxiv.org/abs/1811.12204v1,"Chiller: Contention-centric Transaction Execution and Data Partitioning
  for Fast Networks","Distributed transactions on high-overhead TCP/IP-based networks were
conventionally considered to be prohibitively expensive and thus were avoided
at all costs. To that end, the primary goal of almost any existing partitioning
scheme is to minimize the number of cross-partition transactions. However, with
the next generation of fast RDMA-enabled networks, this assumption is no longer
valid. In fact, recent work has shown that distributed databases can scale even
when the majority of transactions are cross-partition.
  In this paper, we first make the case that the new bottleneck which hinders
truly scalable transaction processing in modern RDMA-enabled databases is data
contention, and that optimizing for data contention leads to different
partitioning layouts than optimizing for the number of distributed
transactions. We then present Chiller, a new approach to data partitioning and
transaction execution, which minimizes data contention for both local and
distributed transactions. Finally, we evaluate Chiller using TPC-C and a
real-world workload, and show that our partitioning and execution strategy
outperforms traditional partitioning techniques which try to avoid distributed
transactions, by up to a factor of 2 under the same conditions.",
cross-border transaction costs,http://arxiv.org/abs/1711.07617v2,Dynamic Distributed Storage for Scaling Blockchains,"Blockchain uses the idea of storing transaction data in the form of a
distributed ledger wherein each node in the network stores a current copy of
the sequence of transactions in the form of a hash chain. This requirement of
storing the entire ledger incurs a high storage cost that grows undesirably
large for high transaction rates and large networks. In this work we use the
ideas of secret key sharing, private key encryption, and distributed storage to
design a coding scheme such that each node stores only a part of the entire
transaction thereby reducing the storage cost to a fraction of its original
cost. When further using dynamic zone allocation, we show the coding scheme can
also improve the integrity of the transaction data in the network over current
schemes. Further, block validation (bitcoin mining) consumes a significant
amount of energy as it is necessary to determine a hash value satisfying a
specific set of constraints; we show that using dynamic distributed storage
reduces these energy costs.",
cross-border transaction costs,http://arxiv.org/abs/1703.02722v1,"Scaling Distributed Transaction Processing and Recovery based on
  Dependency Logging","DGCC protocol has been shown to achieve good performance on multi-core
in-memory system. However, distributed transactions complicate the dependency
resolution, and therefore, an effective transaction partitioning strategy is
essential to reduce expensive multi-node distributed transactions. During
failure recovery, log must be examined from the last checkpoint onwards and the
affected transactions are re-executed based on the way they are partitioned and
executed. Existing approaches treat both transaction management and recovery as
two separate problems, even though recovery is dependent on the sequence in
which transactions are executed.
  In this paper, we propose to treat the transaction management and recovery
problems as one. We first propose an efficient Distributed Dependency Graph
based Concurrency Control (DistDGCC) protocol for handling transactions
spanning multiple nodes, and propose a new novel and efficient logging protocol
called Dependency Logging that also makes use of dependency graphs for
efficient logging and recovery. DistDGCC optimizes the average cost for each
distributed transaction by processing transactions in batch. Moreover, it also
reduces the effects of thread blocking caused by distributed transactions and
consequently improves the runtime performance. Further, dependency logging
exploits the same data structure that is used by DistDGCC to reduce the logging
overhead, as well as the logical dependency information to improve the recovery
parallelism. Extensive experiments are conducted to evaluate the performance of
our proposed technique against state-of-the-art techniques. Experimental
results show that DistDGCC is efficient and scalable, and dependency logging
supports fast recovery with marginal runtime overhead. Hence, the overall
system performance is significantly improved as a result.",
cross-border transaction costs,http://arxiv.org/abs/1603.00542v1,Repairing Conflicts among MVCC Transactions,"The optimistic variants of MVCC (Multi-Version Concurrency Control) avoid
blocking concurrent transactions at the cost of having a validation phase. Upon
failure in the validation phase, the transaction is usually aborted and
restarted from scratch. The ""abort and restart"" approach becomes a performance
bottleneck for the use cases with high contention objects or long running
transactions. In addition, restarting from scratch creates a negative feedback
loop in the system, because the system incurs additional overhead that may
create even further conflicts.
  In this paper, we propose a novel approach for conflict resolution in MVCC
for in-memory databases. This low overhead approach summarizes the transaction
programs in the form of a dependency graph. The dependency graph also contains
the constructs used in the validation phase of the MVCC algorithm. Then, in the
case of encountering conflicts among transactions, the conflict locations in
the program are quickly detected, and the conflicting transactions are
partially re-executed. This approach maximizes the reuse of the computations
done in the initial execution round, and increases the transaction processing
throughput.",
cross-border transaction costs,http://arxiv.org/abs/1712.07564v2,"Transaction Propagation on Permissionless Blockchains: Incentive and
  Routing Mechanisms","Existing permissionless blockchain solutions rely on peer-to-peer propagation
mechanisms, where nodes in a network transfer transaction they received to
their neighbors. Unfortunately, there is no explicit incentive for such
transaction propagation. Therefore, existing propagation mechanisms will not be
sustainable in a fully decentralized blockchain with rational nodes. In this
work, we formally define the problem of incentivizing nodes for transaction
propagation. We propose an incentive mechanism where each node involved in the
propagation of a transaction receives a share of the transaction fee. We also
show that our proposal is Sybil-proof. Furthermore, we combine the incentive
mechanism with smart routing to reduce the communication and storage costs at
the same time. The proposed routing mechanism reduces the redundant transaction
propagation from the size of the network to a factor of average shortest path
length. The routing mechanism is built upon a specific type of consensus
protocol where the round leader who creates the transaction block is known in
advance. Note that our routing mechanism is a generic one and can be adopted
independently from the incentive mechanism.",
cross-border transaction costs,http://arxiv.org/abs/physics/0603013v1,Stock mechanics: unification with economy,"Associating stock mechanics to real economy, in terms of volume, number of
transactions, and cost, i.e. money flow for shares, we obtained the fundamental
laws of stock mechanics.",
cross-border transaction costs,http://arxiv.org/abs/1103.1302v9,On the Cost of Concurrency in Transactional Memory,"The crux of software transactional memory (STM) is to combine an easy-to-use
programming interface with an efficient utilization of the concurrent-computing
abilities provided by modern machines. But does this combination come with an
inherent cost? We evaluate the cost of concurrency by measuring the amount of
expensive synchronization that must be employed in an STM implementation that
ensures positive concurrency, i.e., allows for concurrent transaction
processing in some executions. We focus on two popular progress conditions that
provide positive concurrency: progressiveness and permissiveness. We show that
in permissive STMs, providing a very high degree of concurrency, a transaction
performs a linear number of expensive synchronization patterns with respect to
its read-set size. In contrast, progressive STMs provide a very small degree of
concurrency but, as we demonstrate, can be implemented using at most one
expensive synchronization pattern per transaction. However, we show that even
in progressive STMs, a transaction has to ""protect"" (e.g., by using locks or
strong synchronization primitives) a linear amount of data with respect to its
write-set size. Our results suggest that looking for high degrees of
concurrency in STM implementations may bring a considerable synchronization
cost.",
cross-border transaction costs,http://arxiv.org/abs/1502.04908v2,Progressive Transactional Memory in Time and Space,"Transactional memory (TM) allows concurrent processes to organize sequences
of operations on shared \emph{data items} into atomic transactions. A
transaction may commit, in which case it appears to have executed sequentially
or it may \emph{abort}, in which case no data item is updated.
  The TM programming paradigm emerged as an alternative to conventional
fine-grained locking techniques, offering ease of programming and
compositionality. Though typically themselves implemented using locks, TMs hide
the inherent issues of lock-based synchronization behind a nice transactional
programming interface.
  In this paper, we explore inherent time and space complexity of lock-based
TMs, with a focus of the most popular class of \emph{progressive} lock-based
TMs. We derive that a progressive TM might enforce a read-only transaction to
perform a quadratic (in the number of the data items it reads) number of steps
and access a linear number of distinct memory locations, closing the question
of inherent cost of \emph{read validation} in TMs. We then show that the total
number of \emph{remote memory references} (RMRs) that take place in an
execution of a progressive TM in which $n$ concurrent processes perform
transactions on a single data item might reach $\Omega(n \log n)$, which
appears to be the first RMR complexity lower bound for transactional memory.",
cross-border transaction costs,http://arxiv.org/abs/1709.04284v1,"Accelerating Analytical Processing in MVCC using Fine-Granular
  High-Frequency Virtual Snapshotting","Efficient transactional management is a delicate task. As systems face
transactions of inherently different types, ranging from point updates to long
running analytical computations, it is hard to satisfy their individual
requirements with a single processing component. Unfortunately, most systems
nowadays rely on such a single component that implements its parallelism using
multi-version concurrency control (MVCC). While MVCC parallelizes short-running
OLTP transactions very well, it struggles in the presence of mixed workloads
containing long-running scan-centric OLAP queries, as scans have to work their
way through large amounts of versioned data. To overcome this problem, we
propose a system, which reintroduces the concept of heterogeneous transaction
processing: OLAP transactions are outsourced to run on separate (virtual)
snapshots while OLTP transactions run on the most recent representation of the
database. Inside both components, MVCC ensures a high degree of concurrency.
The biggest challenge of such a heterogeneous approach is to generate the
snapshots at a high frequency. Previous approaches heavily suffered from the
tremendous cost of snapshot creation. In our system, we overcome the
restrictions of the OS by introducing a custom system call vm_snapshot, that is
hand-tailored to our precise needs: it allows fine-granular snapshot creation
at very high frequencies, rendering the snapshot creation phase orders of
magnitudes faster than state-of-the-art approaches. Our experimental evaluation
on a heterogeneous workload based on TPC-H transactions and handcrafted OLTP
transactions shows that our system enables significantly higher analytical
transaction throughputs on mixed workloads than homogeneous approaches. In this
sense, we introduce a system that accelerates Analytical processing by
introducing custom Kernel functionalities: AnKerDB.",
cross-border transaction costs,http://arxiv.org/abs/1907.02669v1,On the Cost of Concurrency in Hybrid Transactional Memory,"State-of-the-art \emph{software transactional memory (STM)} implementations
achieve good performance by carefully avoiding the overhead of
\emph{incremental validation} (i.e., re-reading previously read data items to
avoid inconsistency) while still providing \emph{progressiveness} (allowing
transactional aborts only due to \emph{data conflicts}). Hardware transactional
memory (HTM) implementations promise even better performance, but offer no
progress guarantees. Thus, they must be combined with STMs, leading to
\emph{hybrid} TMs (HyTMs) in which hardware transactions must be
\emph{instrumented} (i.e., access metadata) to detect contention with software
transactions.
  We show that, unlike in progressive STMs, software transactions in
progressive HyTMs cannot avoid incremental validation. In fact, this result
holds even if hardware transactions can \emph{read} metadata
\emph{non-speculatively}. We then present \emph{opaque} HyTM algorithms
providing \emph{progressiveness for a subset of transactions} that are optimal
in terms of hardware instrumentation. We explore the concurrency vs. hardware
instrumentation vs. software validation trade-offs for these algorithms. Our
experiments with Intel and IBM POWER8 HTMs seem to suggest that (i) the
\emph{cost of concurrency} also exists in practice, (ii) it is important to
implement HyTMs that provide progressiveness for a maximal set of transactions
without incurring high hardware instrumentation overhead or using global
contending bottlenecks and (iii) there is no easy way to derive more efficient
HyTMs by taking advantage of non-speculative accesses within hardware.",
cross-border transaction costs,http://arxiv.org/abs/1405.5689v3,Inherent Limitations of Hybrid Transactional Memory,"Several Hybrid Transactional Memory (HyTM) schemes have recently been
proposed to complement the fast, but best-effort, nature of Hardware
Transactional Memory (HTM) with a slow, reliable software backup. However, the
fundamental limitations of building a HyTM with nontrivial concurrency between
hardware and software transactions are still not well understood.
  In this paper, we propose a general model for HyTM implementations, which
captures the ability of hardware transactions to buffer memory accesses, and
allows us to formally quantify and analyze the amount of overhead
(instrumentation) of a HyTM scheme. We prove the following: (1) it is
impossible to build a strictly serializable HyTM implementation that has both
uninstrumented reads and writes, even for weak progress guarantees, and (2)
under reasonable assumptions, in any opaque progressive HyTM, a hardware
transaction must incur instrumentation costs linear in the size of its data
set. We further provide two upper bound implementations whose instrumentation
costs are optimal with respect to their progress guarantees. In sum, this paper
captures for the first time an inherent trade-off between the degree of
concurrency a HyTM provides between hardware and software transactions, and the
amount of instrumentation overhead the implementation must incur.",
cross-border transaction costs,http://arxiv.org/abs/1604.02103v1,"Cooperative Planning of Renewable Generations for Interconnected
  Microgrids","We study the renewable energy generations in Hong Kong based on realistic
meteorological data, and find that different renewable sources exhibit diverse
time-varying and location-dependent profiles. To efficiently explore and
utilize the diverse renewable energy generations, we propose a theoretical
framework for the cooperative planning of renewable generations in a system of
interconnected microgrids. The cooperative framework considers the
self-interested behaviors of microgrids, and incorporates both their long-term
investment costs and short-term operational costs over the planning horizon.
Specifically, interconnected microgrids jointly decide where and how much to
deploy renewable energy generations, and how to split the associated investment
cost. We show that the cooperative framework minimizes the overall system cost.
We also design a fair cost sharing method based on Nash bargaining to
incentivize cooperative planning, such that all microgrids will benefit from
cooperative planning. Using realistic data obtained from the Hong Kong
observatory, we validate the cooperative planning framework, and demonstrate
that all microgrids benefit through the cooperation, and the overall system
cost is reduced by 35.9% compared to the noncooperative planning benchmark.","IEEE Transactions on Smart Grid, 7(5), Pages: 2486-2496, 2016"
cross-border transaction costs,http://arxiv.org/abs/1001.0393v2,Market Equilibrium with Transaction Costs,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",
merger and acquisition inflation,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition inflation,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition inflation,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition inflation,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition inflation,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition inflation,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition inflation,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition inflation,http://arxiv.org/abs/1506.06008v1,"4πβ (LS)-γ (HPGe) Digital Coincidence System Based on
  Synchronous High-Speed Multichannel Data Acquisition","A dedicated 4{\pi}{\beta} (LS)-{\gamma} (HPGe)digital coincidence system has
been developed in this work, which includes five acquisition channels. Three
analog-to-digital converter (ADC) acquisition channels with an acquisition
resolution of 8 bits and acquisition rate of 1GSPS (sample per second) are
utilized to collect the signals from three Photo multiplier tubes (PMTs) which
are adopted to detect {\beta} decay, and two acquisition channels with an
acquisition resolution of 16 bits and acquisition rate of 50MSPS are utilized
to collect the signals from high-purity germanium (HPGe) which are adopted to
detect {\gamma} decay. In order to increase the accuracy of the coincidence
system, all the five acquisition channels are synchronous within 500ps. The
data collected by the five acquisition channels will be transmitted to the host
PC through PCI bus and saved as a file. Off-line software is applied for the
4{\pi}{\beta} (LS)-{\gamma} (HPGe) coincidence and data analysis as needed in
practical application. With all the above preconditions, the flexibility of the
system is increased, and the structure and application of the system are
simplified. According to the test, the highest coincidence rate of the system
is 20K per second, which is sufficient for most applications. This paper mainly
introduces the design of the hardware, the synchronization method and the test
result of this system.",
merger and acquisition inflation,http://arxiv.org/abs/1010.5854v2,"An Alternative Approach to Data Acquisition Using Keyboard Emulation
  Technique","A number of data acquisition systems depend on human interface to access
computer for measuring, processing and analyzing data and to prepare it for
presentation and storage. Data acquisition software is installed on the
computer and all intended operations are performed manually. The data
acquisition software requires user intervention for operations like selection
of measurement setup, acquisition and storage of data to computer. The duty of
users becomes laborious if the data acquisition process lasts for a long
duration and requires continuous repetition of steps. An appropriate solution
to overcome such problem is to replace the physical operator with a virtual
user. This software generated simulated user sits at the data acquisition
process through out and automate all the intended steps of data acquisition.
This paper presents a new approach for data acquisition by using keyboard
emulation technique. A keyboard emulation software is developed which runs
beside the main data acquisition software and acts as a virtual user. All the
operations which require user interface are performed through fully automated
computer program. The developed software/system is executed in a real time
environment and the functionality of the software is verified. In the end,
potential application areas of the designed keyboard emulation software are
explored.",
merger and acquisition inflation,http://arxiv.org/abs/1802.09669v1,"A Multi-Disciplinary Review of Knowledge Acquisition Methods: From Human
  to Autonomous Eliciting Agents","This paper offers a multi-disciplinary review of knowledge acquisition
methods in human activity systems. The review captures the degree of
involvement of various types of agencies in the knowledge acquisition process,
and proposes a classification with three categories of methods: the human
agent, the human-inspired agent, and the autonomous machine agent methods. In
the first two categories, the acquisition of knowledge is seen as a cognitive
task analysis exercise, while in the third category knowledge acquisition is
treated as an autonomous knowledge-discovery endeavour. The motivation for this
classification stems from the continuous change over time of the structure,
meaning and purpose of human activity systems, which are seen as the factor
that fuelled researchers' and practitioners' efforts in knowledge acquisition
for more than a century.
  We show through this review that the KA field is increasingly active due to
the higher and higher pace of change in human activity, and conclude by
discussing the emergence of a fourth category of knowledge acquisition methods,
which are based on red-teaming and co-evolution.","Knowledge-Based Systems, Volume 105, Elsevier, 2016"
merger and acquisition inflation,http://arxiv.org/abs/1304.1116v1,"Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection","Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS.",
merger and acquisition inflation,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
merger and acquisition inflation,http://arxiv.org/abs/1910.08338v1,A Gigapixel Computational Light-Field Camera,"Light-field cameras allow the acquisition of both the spatial and angular
components of the light. This has a wide range of applications from image
refocusing to 3D reconstruction of a scene. The conventional way to perform
such acquisitions leads to a strong spatio-angular resolution limit. Here we
propose a computational version of the light-field camera. We perform a one
gigapixel photo-realistic diffraction limited light-field acquisition, that
would require the use of a one gigapixel sensor were the acquisition to be
performed with a conventional light-field camera. This result is mostly limited
by the total acquisition time, as our system could in principle allow
$\sim$Terapixel reconstructions to be achieved. The reported result presents
many potential advantages, such as the possibility to perform large depth of
field light-field acquisitions, realistic refocusing along a very wide range of
depths, very high dimensional super-resolved image acquisitions, and large
depth of field 3D reconstructions.",
merger and acquisition inflation,http://arxiv.org/abs/1610.05462v2,"Towards the Leveraging of Data Deduplication to Break the Disk
  Acquisition Speed Limit","Digital forensic evidence acquisition speed is traditionally limited by two
main factors: the read speed of the storage device being investigated, i.e.,
the read speed of the disk, memory, remote storage, mobile device, etc.), and
the write speed of the system used for storing the acquired data. Digital
forensic investigators can somewhat mitigate the latter issue through the use
of high-speed storage options, such as networked RAID storage, in the
controlled environment of the forensic laboratory. However, traditionally,
little can be done to improve the acquisition speed past its physical read
speed from the target device itself. The protracted time taken for data
acquisition wastes digital forensic experts' time, contributes to digital
forensic investigation backlogs worldwide, and delays pertinent information
from potentially influencing the direction of an investigation. In a remote
acquisition scenario, a third contributing factor can also become a detriment
to the overall acquisition time - typically the Internet upload speed of the
acquisition system. This paper explores an alternative to the traditional
evidence acquisition model through the leveraging of a forensic data
deduplication system. The advantages that a deduplicated approach can provide
over the current digital forensic evidence acquisition process are outlined and
some preliminary results of a prototype implementation are discussed.",
merger and acquisition inflation,http://arxiv.org/abs/1903.00269v1,"Covariance-Aided CSI Acquisition with Non-Orthogonal Pilots in Massive
  MIMO Systems","Massive multiple-input multiple-output (MIMO) systems use antenna arrays with
a large number of antenna elements to serve many different users
simultaneously. The large number of antennas in the system makes, however, the
channel state information (CSI) acquisition strategy design critical and
particularly challenging. Interestingly, in the context of massive MIMO
systems, channels exhibit a large degree of spatial correlation which can be
exploited to cope with the dimensionality problem in CSI acquisition. With the
final objective of analyzing the benefits of covariance-aided uplink multi-user
CSI acquisition in massive MIMO systems, here we compare the channel estimation
mean-square error (MSE) for (i) conventional CSI acquisition, which does not
assume any knowledge on the user spatial covariances and uses orthogonal pilot
sequences; and (ii) covariance-aided CSI acquisition, which exploits the
individual covariance matrices for channel estimation and possibly uses
non-orthogonal pilot sequences. We apply a large-system analysis to the latter
case, for which new asymptotic MSE expressions are established under various
assumptions on the distributions of the pilot sequences and on the covariance
matrices. We link these expressions to those describing the estimation MSE of
conventional CSI acquisition with orthogonal pilot sequences of some equivalent
length. This analysis provides insights about how much the CSI acquisition
process can be overloaded (in the sense of allowing estimating CSI with
sufficient accuracy for more users than the number resource elements allocated
for training) when a covariance-aided approach is adopted, hinting at
potentially significant gains in the spectral efficiency of CSI acquisition in
Massive MIMO.",
merger and acquisition inflation,http://arxiv.org/abs/1403.4508v1,Research on Study Mechanical Vibrations with Data Acquisition Systems,"The paper presents a new study method of mechanic vibrations with the help of
the data acquisition systems. The study of vibrations with the help of data
acquisition systems allows the solving of some engineering problems connected
to the measurement of some parameters which are difficult to measure having in
view the improvement of the technical performances of the industrial equipment
or devices",
merger and acquisition inflation,http://arxiv.org/abs/1304.2894v1,"A Data Acquisition and Monitoring System for the Detector Development
  Group at FZJ/ZEA-2","The central institute of electronics (ZEA-2) in the Forschungszentrum Juelich
(FZJ) has developed the novel readout electronics JUDIDT to cope with high-rate
data acquisition of the KWS-1 and KWS-2 detectors in the experimental Hall at
the Forschungsreacktor Muenchen FRM-II in Garching, Muenchen. This electronics
has been then modified and used also for the data-acquisition of a prototype
for an ANGER Camera proposed for the planned European Spallation Source. To
commission the electronics, software for the data acquisition and the data
monitoring has been developed. In this report the software is described.",
merger and acquisition inflation,http://arxiv.org/abs/1301.3893v1,A Knowledge Acquisition Tool for Bayesian-Network Troubleshooters,"This paper describes a domain-specific knowledge acquisition tool for
intelligent automated troubleshooters based on Bayesian networks. No Bayesian
network knowledge is required to use the tool, and troubleshooting information
can be specified as natural and intuitive as possible. Probabilities can be
specified in the direction that is most natural to the domain expert. Thus, the
knowledge acquisition efficiently removes the traditional knowledge acquisition
bottleneck of Bayesian networks.",
merger and acquisition inflation,http://arxiv.org/abs/1605.02559v1,"Robust imaging of hippocampal inner structure at 7T: in vivo acquisition
  protocol and methodological choices","OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in
vivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high
resolution imaging, such as can be achieved with 7T MRI. An acquisition
protocol was designed for imaging hippocampal inner structure at 7T. It relies
on a compromise between anatomical details visibility and robustness to motion.
In order to reduce acquisition time and motion artifacts, the full slab
covering the hippocampus was split into separate slabs with lower acquisition
time. A robust registration approach was implemented to combine the acquired
slabs within a final 3D-consistent high-resolution slab covering the whole
hippocampus. Evaluation was performed on 50 subjects overall, made of three
groups of subjects acquired using three acquisition settings; it focused on
three issues: visibility of hippocampal inner structure, robustness to motion
artifacts and registration procedure performance.RESULTS:Overall, T2-weighted
acquisitions with interleaved slabs proved robust. Multi-slab registration
yielded high quality datasets in 96 % of the subjects, thus compatible with
further analyses of hippocampal inner structure.CONCLUSION:Multi-slab
acquisition and registration setting is efficient for reducing acquisition time
and consequently motion artifacts for ultra-high resolution imaging of the
inner structure of the hippocampus.","Magnetic Resonance Materials in Physics, Biology and Medicine,
  Springer Verlag, 2016"
merger and acquisition inflation,http://arxiv.org/abs/physics/0111077v1,An Overview of the LIGO Control and Data Acquisition System,"The LIGO Control and Data system (CDS) features a tightly coupled and highly
integrated control and data acquisition system. Control of the interferometers
requires many Multiple Input Multiple Output (MIMO) control loops closed both
locally and across the 4-kilometer interferometer arm lengths. In addition to
providing the closed loop control, the control systems front end processors act
as Data Collection Units (DCU) for the data acquisition system. Data collected
by these front ends and the data acquisition system must be collected and time
stamped to an accuracy of 1 microsecond and made available to on-line analysis
tools such as the Global Diagnostics System (GDS)[1]. Data is also sent to the
LIGO Data Analysis System (LDAS)[2] for long-term storage and off-line
analysis. Data rates exceed 5 Mbytes per second per interferometer continuous.
Connection between the various front end processors and the data acquisition
system is achieved using fiber optic reflective memory networks. Both controls
and data acquisition systems use VME hardware and VxWorks operating systems.
This paper will present an overview of the LIGO CDS and discuss key aspects of
its design.",eConf C011127 (2001) TUBI001
merger and acquisition inflation,http://arxiv.org/abs/1308.4458v1,Coded Acquisition of High Frame Rate Video,"High frame video (HFV) is an important investigational tool in sciences,
engineering and military. In ultra-high speed imaging, the obtainable temporal,
spatial and spectral resolutions are limited by the sustainable throughput of
in-camera mass memory, the lower bound of exposure time, and illumination
conditions. In order to break these bottlenecks, we propose a new coded video
acquisition framework that employs K > 2 conventional cameras, each of which
makes random measurements of the 3D video signal in both temporal and spatial
domains. For each of the K cameras, this multi-camera strategy greatly relaxes
the stringent requirements in memory speed, shutter speed, and illumination
strength. The recovery of HFV from these random measurements is posed and
solved as a large scale l1 minimization problem by exploiting joint temporal
and spatial sparsities of the 3D signal. Three coded video acquisition
techniques of varied trade offs between performance and hardware complexity are
developed: frame-wise coded acquisition, pixel-wise coded acquisition, and
column-row-wise coded acquisition. The performances of these techniques are
analyzed in relation to the sparsity of the underlying video signal.
Simulations of these new HFV capture techniques are carried out and
experimental results are reported.",
merger and acquisition inflation,http://arxiv.org/abs/1806.10663v1,"The Design of Data Acquisition System for EAST Technical Diagnostic
  System","EAST (Experimental Advanced Superconducting Tokamak) Technical Diagnostic
System (TDS) is used to monitor the outlet temperature of all superconducting
coils, in case of temperature anomaly, it will trigger safety interlock system
to meet EAST device safety. The data acquisition system of TDS is in charge of
continuous data acquisition of the nitrogen and helium temperature signals, TDS
security alarm and long-term data storage. It supports continuous data
acquisition and pulse data acquisition. The data acquisition of the nitrogen
temperature signals is based on the PXI technology while obtaining the helium
temperature signals from Lake Shore model 224 temperature monitors with VISA
standard. After data conversion, all the data will be stored in MySQL and
MDSPlus for long-term storage. It should output TDS fault signal and status
signal to trigger the safety interlock system to take actions after threshold
evaluation of key temperature signals. It publishes part of real-time TDS data
to the cryogenic system and provides an information inquiry service to the TDS
administrator. The system has been used in 2018 EAST campaign.",
merger and acquisition inflation,http://arxiv.org/abs/1806.00812v1,A Framework for Speechreading Acquisition Tools,"At least 360 million people worldwide have disabling hearing loss that
frequently causes difficulties in day-to-day conversations. Hearing aids often
fail to offer enough benefits and have low adoption rates. However, people with
hearing loss find that speechreading can improve their understanding during
conversation. Speechreading (often called lipreading) refers to using visual
information about the movements of a speaker's lips, teeth, and tongue to help
understand what they are saying. Speechreading is commonly used by people with
all severities of hearing loss to understand speech, and people with typical
hearing also speechread (albeit subconsciously) to help them understand others.
However, speechreading is a skill that takes considerable practice to acquire.
Publicly-funded speechreading classes are sometimes provided, and have been
shown to improve speechreading acquisition. However, classes are only provided
in a handful of countries around the world and students can only practice
effectively when attending class. Existing tools have been designed to help
improve speechreading acquisition, but are often not effective because they
have not been designed within the context of contemporary speechreading lessons
or practice. To address this, in this thesis I present a novel speechreading
acquisition framework that can be used to design Speechreading Acquisition
Tools (SATs) - a new type of technology to improve speechreading acquisition.",
merger and acquisition inflation,http://arxiv.org/abs/1709.05964v1,"Why Pay More When You Can Pay Less: A Joint Learning Framework for
  Active Feature Acquisition and Classification","We consider the problem of active feature acquisition, where we sequentially
select the subset of features in order to achieve the maximum prediction
performance in the most cost-effective way. In this work, we formulate this
active feature acquisition problem as a reinforcement learning problem, and
provide a novel framework for jointly learning both the RL agent and the
classifier (environment). We also introduce a more systematic way of encoding
subsets of features that can properly handle innate challenge with missing
entries in active feature acquisition problems, that uses the orderless
LSTM-based set encoding mechanism that readily fits in the joint learning
framework. We evaluate our model on a carefully designed synthetic dataset for
the active feature acquisition as well as several real datasets such as
electric health record (EHR) datasets, on which it outperforms all baselines in
terms of prediction performance as well feature acquisition cost.",
merger and acquisition GDP,http://arxiv.org/abs/1710.07382v1,Partially Coherent Ptychography by Gradient Decomposition of the Probe,"Coherent ptychographic imaging experiments often discard over 99.9 % of the
flux from a light source to define the coherence of an illumination. Even when
coherent flux is sufficient, the stability required during an exposure is
another important limiting factor. Partial coherence analysis can considerably
reduce these limitations. A partially coherent illumination can often be
written as the superposition of a single coherent illumination convolved with a
separable translational kernel. In this paper we propose the Gradient
Decomposition of the Probe (GDP), a model that exploits translational kernel
separability, coupling the variances of the kernel with the transverse
coherence. We describe an efficient first-order splitting algorithm GDP-ADMM to
solve the proposed nonlinear optimization problem. Numerical experiments
demonstrate the effectiveness of the proposed method with Gaussian and binary
kernel functions in fly-scan measurements. Remarkably, GDP-ADMM produces
satisfactory results even when the ratio between kernel width and beam size is
more than one, or when the distance between successive acquisitions is twice as
large as the beam width.","Acta Crystallographica Section A: Foundations and Advances 74 (3),
  157-169 (2018)"
merger and acquisition GDP,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition GDP,http://arxiv.org/abs/1609.00876v1,Statistical Dynamics of Regional Populations and Economies,"A practical statistical analysis on the regional populations and GDPs of
China is conducted. The result shows that the distribution of the populations
and that of the GDPs obeys the shifted power law, respectively. To understand
these characteristics, a generalized Langevin equation describing variation of
population is proposed based on the correlation between population and GDP as
well as the random fluctuations of the related factors. The equation is
transformed into the Fokker-Plank equation, and the solution demonstrates a
transform of population distribution from the normal Gaussian distribution to a
shifted power law. It also suggests a critical point of time at which the
transform occurs. The shifted power law distribution in the supercritical
situation is qualitatively in accordance with the practical result. The
distribution of the GDPs is derived based on the Cobb-Douglas production
function, and presents a change from a shifted power law to the Gaussian
distribution. This result indicates that the regional GDP distribution of our
society will be the Gaussian distribution in the future. The analysis on the
growth trend of economy suggests it will become a reality. These theoretical
attempts may draw a historical picture of our world in the aspects of
population and economy.",
merger and acquisition GDP,http://arxiv.org/abs/physics/0607139v2,"Clusters or networks of economies? A macroeconomy study through GDP
  fluctuation correlations","We follow up on the study of correlations between GDP's of rich countries. We
analyze web-downloaded data on GDP that we use as individual wealth signatures
of the country economical state. We calculate the yearly fluctuations of the
GDP. We look for forward and backward correlations between such fluctuations.
The system is represented by an evolving network, nodes being the GDP
fluctuations (or countries) at different times.
  In order to extract structures from the network, we focus on filtering the
time delayed correlations by removing the least correlated links. This
percolation idea-based method reveals the emergence of connections, that are
visualized by a branching representation. Note that the network is made of
weighted and directed links when taking into account a delay time. Such a
measure of collective habits does not fit the usual expectations defined by
politicians or economists.",Physica A 382 (2007) 16-21
merger and acquisition GDP,http://arxiv.org/abs/1308.5572v1,"Insight into the Properties of the UK Power Consumption Using a Linear
  Regression and Wavelet Transform Approach","In this paper, the relationship between the Gross Domestic Product (GDP), air
temperature variations and power consumption is evaluated using the linear
regression and Wavelet Coherence (WTC) approach on a 1971-2011 time series for
the United Kingdom (UK). The results based on the linear regression approach
indicate that some 66% variability of the UK electricity demand can be
explained by the quarterly GDP variations, while only 11% of the quarterly
changes of the UK electricity demand are caused by seasonal air temperature
variations. WTC however, can detect the period of time when GDP and air
temperature significantly correlate with electricity demand and the results of
the wavelet correlation at different time scales indicate that a significant
correlation is to be found on a long-term basis for GDP and on an annual basis
for seasonal air-temperature variations. This approach provides an insight into
the properties of the impact of the main factors on power consumption on the
basis of which the power system development or operation planning and
forecasting the power consumption can be improved.","Elektrotehniski vestnik/Electrotechnical review 79(5), 278-283,
  2012"
merger and acquisition GDP,http://arxiv.org/abs/physics/0701030v1,Interplay between topology and dynamics in the World Trade Web,"We present an empirical analysis of the network formed by the trade
relationships between all world countries, or World Trade Web (WTW). Each
(directed) link is weighted by the amount of wealth flowing between two
countries, and each country is characterized by the value of its Gross Domestic
Product (GDP). By analysing a set of year-by-year data covering the time
interval 1950-2000, we show that the dynamics of all GDP values and the
evolution of the WTW (trade flow and topology) are tightly coupled. The
probability that two countries are connected depends on their GDP values,
supporting recent theoretical models relating network topology to the presence
of a `hidden' variable (or fitness). On the other hand, the topology is shown
to determine the GDP values due to the exchange between countries. This leads
us to a new framework where the fitness value is a dynamical variable
determining, and at the same time depending on, network topology in a
continuous feedback.","Eur. Phys. J. B 57,159-164 (2007)"
merger and acquisition GDP,http://arxiv.org/abs/1809.06665v1,"Compressed Sensing Parallel MRI with Adaptive Shrinkage TV
  Regularization","Compressed sensing (CS) methods in magnetic resonance imaging (MRI) offer
rapid acquisition and improved image quality but require iterative
reconstruction schemes with regularization to enforce sparsity. Regardless of
the difficulty in obtaining a fast numerical solution, the total variation (TV)
regularization is a preferred choice due to its edge-preserving and structure
recovery capabilities. While many approaches have been proposed to overcome the
non-differentiability of the TV cost term, an iterative shrinkage based
formulation allows recovering an image through recursive application of linear
filtering and soft thresholding. However, providing an optimal setting for the
regularization parameter is critical due to its direct impact on the rate of
convergence as well as steady state error. In this paper, a regularizer
adaptively varying in the derivative space is proposed, that follows the
generalized discrepancy principle (GDP). The implementation proceeds by
adaptively reducing the discrepancy level expressed as the absolute difference
between TV norms of the consistency error and the sparse approximation error. A
criterion based on the absolute difference between TV norms of consistency and
sparse approximation errors is used to update the threshold. Application of the
adaptive shrinkage TV regularizer to CS recovery of parallel MRI (pMRI) and
temporal gradient adaptation in dynamic MRI are shown to result in improved
image quality with accelerated convergence. In addition, the adaptive TV-based
iterative shrinkage (ATVIS) provides a significant speed advantage over the
fast iterative shrinkage-thresholding algorithm (FISTA).",
merger and acquisition GDP,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition GDP,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition GDP,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition GDP,http://arxiv.org/abs/1006.5269v1,Allometric Scaling of Countries,"As huge complex systems consisting of geographic regions, natural resources,
people and economic entities, countries follow the allometric scaling law which
is ubiquitous in ecological, urban systems. We systematically investigated the
allometric scaling relationships between a large number of macroscopic
properties and geographic (area), demographic (population) and economic (GDP,
gross domestic production) sizes of countries respectively. We found that most
of the economic, trade, energy consumption, communication related properties
have significant super-linear (the exponent is larger than 1) or nearly linear
allometric scaling relations with GDP. Meanwhile, the geographic (arable area,
natural resources, etc.), demographic(labor force, military age population,
etc.) and transportation-related properties (road length, airports) have
significant and sub-linear (the exponent is smaller than 1) allometric scaling
relations with area. Several differences of power law relations with respect to
population between countries and cities were pointed out. Firstly, population
increases sub-linearly with area in countries. Secondly, GDP increases linearly
in countries but not super-linearly as in cities. Finally, electricity or oil
consumptions per capita increases with population faster than cities.","Physica A: Statistical Mechanics and its Applications. Volume 389,
  Issue 21, 1 November 2010, Pages 4887-4896"
merger and acquisition GDP,http://arxiv.org/abs/physics/0607180v1,"How Do Output Growth Rate Distributions Look Like? Some Time-Series
  Evidence on OECD Countries","This paper investigates the statistical properties of within-country GDP and
industrial production (IP) growth rate distributions. Many empirical
contributions have recently pointed out that cross-section growth rates of
firms, industries and countries all follow Laplace distributions. In this work,
we test whether also within-country, time-series GDP and IP growth rates can be
approximated by tent-shaped distributions. We fit output growth rates with the
exponential-power (Subbotin) family of densities, which includes as particular
cases both the Gaussian and the Laplace distributions. We find that, for a
large number of OECD countries including the U.S., both GDP and IP growth rates
are Laplace distributed. Moreover, we show that fat-tailed distributions
robustly emerge even after controlling for outliers, autocorrelation and
heteroscedasticity.",
merger and acquisition GDP,http://arxiv.org/abs/0802.4170v1,"Cluster Expansion Method for Evolving Weighted Networks Having
  Vector-like Nodes","The Cluster Variation Method known in statistical mechanics and condensed
matter is revived for weighted bipartite networks. The decomposition of a
Hamiltonian through a finite number of components, whence serving to define
variable clusters, is recalled. As an illustration the network built from data
representing correlations between (4) macro-economic features, i.e. the so
called $vector$ $components$, of 15 EU countries, as (function) nodes, is
discussed. We show that statistical physics principles, like the maximum
entropy criterion points to clusters, here in a (4) variable phase space: Gross
Domestic Product (GDP), Final Consumption Expenditure (FCE), Gross Capital
Formation (GCF) and Net Exports (NEX). It is observed that the $maximum$
entropy corresponds to a cluster which does $not$ explicitly include the GDP
but only the other (3) ''axes'', i.e. consumption, investment and trade
components. On the other hand, the $minimal$ entropy clustering scheme is
obtained from a coupling necessarily including GDP and FCE. The results confirm
intuitive economic theory and practice expectations at least as regards
geographical connexions. The technique can of course be applied to many other
cases in the physics of socio-economy networks.",Acta Phys. Pol. A 114 (2008) 491-499
merger and acquisition GDP,http://arxiv.org/abs/physics/0607098v1,"Cluster structure of EU-15 countries derived from the correlation matrix
  analysis of macroeconomic index fluctuations","The statistical distances between countries, calculated for various moving
average time windows, are mapped into the ultrametric subdominant space as in
classical Minimal Spanning Tree methods. The Moving Average Minimal Length Path
(MAMLP) algorithm allows a decoupling of fluctuations with respect to the mass
center of the system from the movement of the mass center itself. A Hamiltonian
representation given by a factor graph is used and plays the role of cost
function. The present analysis pertains to 11 macroeconomic (ME) indicators,
namely the GDP (x1), Final Consumption Expenditure (x2), Gross Capital
Formation (x3), Net Exports (x4), Consumer Price Index (y1), Rates of Interest
of the Central Banks (y2), Labour Force (z1), Unemployment (z2), GDP/hour
worked (z3), GDP/capita (w1) and Gini coefficient (w2). The target group of
countries is composed of 15 EU countries, data taken between 1995 and 2004. By
two different methods (the Bipartite Factor Graph Analysis and the Correlation
Matrix Eigensystem Analysis) it is found that the strongly correlated countries
with respect to the macroeconomic indicators fluctuations can be partitioned
into stable clusters.",Eur. Phys. J B 57 (2007) 139-146
merger and acquisition GDP,http://arxiv.org/abs/1603.02734v1,"Codebook Design for Millimeter-Wave Channel Estimation with Hybrid
  Precoding Structure","In this paper, we study hierarchical codebook design for channel estimation
in millimeter-wave (mmWave) communications with a hybrid precoding structure.
Due to the limited saturation power of mmWave power amplifier (PA), we take the
per-antenna power constraint (PAPC) into consideration. We first propose a
metric, i.e., generalized detection probability (GDP), to evaluate the quality
of \emph{an arbitrary codeword}. This metric not only enables an optimization
approach for mmWave codebook design, but also can be used to compare the
performance of two different codewords/codebooks. To the best of our knowledge,
GDP is the first metric particularly for mmWave codebook design for channel
estimation. We then propose an approach to design a hierarchical codebook
exploiting BeaM Widening with Multi-RF-chain Sub-array technique (BMW-MS). To
obtain crucial parameters of BMW-MS, we provide two solutions, namely a
low-complexity search (LCS) solution to optimize the GDP metric and a
closed-form (CF) solution to pursue a flat beam pattern. Performance
comparisons show that BMW-MS/LCS and BMW-MS/CF achieve very close performances,
and they outperform the existing alternatives under the PAPC.",
merger and acquisition GDP,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition GDP,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition GDP,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition GDP,http://arxiv.org/abs/0909.4786v1,"Worldwide Use and Impact of the NASA Astrophysics Data System Digital
  Library","By combining data from the text, citation, and reference databases with data
from the ADS readership logs we have been able to create Second Order
Bibliometric Operators, a customizable class of collaborative filters which
permits substantially improved accuracy in literature queries.
  Using the ADS usage logs along with membership statistics from the
International Astronomical Union and data on the population and gross domestic
product (GDP) we develop an accurate model for world-wide basic research where
the number of scientists in a country is proportional to the GDP of that
country, and the amount of basic research done by a country is proportional to
the number of scientists in that country times that country's per capita GDP.
  We introduce the concept of utility time to measure the impact of the
ADS/URANIA and the electronic astronomical library on astronomical research. We
find that in 2002 it amounted to the equivalent of 736 FTE researchers, or $250
Million, or the astronomical research done in France.
  Subject headings: digital libraries; bibliometrics; sociology of science;
information retrieval","The Journal of the American Society for Information Science and
  Technology, Vol. 56, p. 36. (2005)"
merger and acquisition GDP,http://arxiv.org/abs/1505.05321v1,Regional Development Classification Model using Decision Tree Approach,"Regional development classification is one way to look at differences in
levels of development outcomes. Some frequently used methods are the shift
share, Gain index, the Iindex Williamson and Klassen typology. The development
of science in the field of data mining, offers a new way for regional
development data classification. This study discusses how the decision tree is
used to classify the level of development based on indicators of regional gross
domestic product (GDP). GDP Data Central Java and Banten used in this study.
Before the data is entered into the decision tree forming algorithm, both the
provincial GDP data are classified using Klassen typology. Three decision tree
algorithms, namely J48, NBTRee and REPTree tested in this study using
cross-validation evaluation, then selected one of the best performing
algorithms. The results show that the J48 has a better accuracy rate which is
equal to 85.18% compared to the algorithm NBTRee and REPTree. Testing the model
is done to the six districts / municipalities in the province of Banten, and
shows that there are two districts / cities are still at the development of the
status quadrant relatively underdeveloped regions, namely Kota Tangerang and
Kabupaten Tangerang. As for the Central Java Province, Kendal, Magelang,
Pemalang, Rembang, Semarang and Wonosobo are an area with a quadrant of
development also on the status of the region is relatively underdeveloped.
Classification model that has been developed is able to classify the level of
development fast and easy to enter data directly into the decision tree is
formed. This study can be used as an alternative decision support for policy
makers in order to determine the future direction of development.","International Journal of Computer Applications Volume 114, No. 8,
  March 2015"
merger and acquisition GDP,http://arxiv.org/abs/1902.09872v1,Economic geography and the scaling of urban and regional income in India,"We undertake an exploration of the economic income (Gross Domestic Product,
GDP) of Indian districts and cities based on scaling analyses of the dependence
of these quantities on associated population size. Scaling analysis provides a
straightforward method for the identification of network effects in
socioeconomic organization, which are the tell-tale of cities and urbanization.
For districts, a sub-state regional administrative division in India, we find
almost linear scaling of GDP with population, a result quite different from
urban functional units in other national contexts. Using deviations from
scaling, we explore the behavior of these regional units to find strong
distinct geographic patterns of economic behavior. We characterize these
patterns in detail and connect them to the literature on regional economic
development for a diverse subcontinental nation such as India. Given the
paucity of economic data for Urban Agglomerations in India, we use a set of
assumptions to create a new dataset of GDP based on districts, for large
cities. This reveals superlinear scaling of income with city size, as expected
from theory, while displaying similar underlying patterns of economic geography
observed for district economic performance. This analysis of the economic
performance of Indian cities is severely limited by the absence of
higher-fidelity, direct city level economic data. We discuss the need for
standardized and consistent estimates of the size and change in urban economies
in India, and point to a number of proxies that can be explored to develop such
indicators.",
merger and acquisition GDP,http://arxiv.org/abs/1608.00275v1,"Metastable Features of Economic Networks and Responses to Exogenous
  Shocks","It has been proved that network structure plays an important role in
addressing a collective behaviour. In this paper we consider a network of firms
and corporations and study its metastable features in an Ising based model. In
our model, we observe that if in a recession the government imposes a demand
shock to stimulate the network, metastable features shape its response.
Actually we find that there is a minimum bound where demand shocks with a size
below it are unable to trigger the market out from recession. We then
investigate the impact of network characteristics on this minimum bound. We
surprisingly observe that in a Watts-Strogatz network though the minimum bound
depends on the average of the degrees, when translated into the economics
language, such a bound is independent of the average degrees. This bound is
about $0.44 \Delta$GDP, where $\Delta$GDP is the gap of GDP between recession
and expansion. We examine our suggestions for the cases of the United States
and the European Union in the recent recession, and compare them with the
imposed stimulations. While stimulation in the US has been above our threshold,
in the EU it has been far below our threshold. Beside providing a minimum bound
for a successful stimulation, our study on the metastable features suggests
that in the time of crisis there is a ""golden time passage"" in which the
minimum bound for successful stimulation can be much lower. So, our study
strongly suggests stimulations to be started within this time passage.","PloS one 11 (10), e0160363 (2016)"
merger and acquisition GDP,http://arxiv.org/abs/1711.10883v1,"An Analytical Framework for Understanding the Intensity of Religious
  Fundamentalism","This paper examines the process of emergence of religious fundamentalism
through development parameters. Therefore this research work reflects an
analytical discussion on how the level of religious fundamentalism can be
explained by the economic, political administrative and legal parameters such
as GDP, Employment to Population ratio, Government Effectiveness, Voice &
Accountability, Rule of Law (World Justice Project Report) and Rule of law
(Governance Indicators).",
merger and acquisition GDP,http://arxiv.org/abs/0710.5447v1,"Clusters in weighted macroeconomic networks : the EU case. Introducing
  the overlapping index of GDP/capita fluctuation correlations","GDP/capita correlations are investigated in various time windows (TW), for
the time interval 1990-2005. The target group of countries is the set of 25 EU
members, 15 till 2004 plus the 10 countries which joined EU later on. The
TW-means of the statistical correlation coefficients are taken as the weights
(links) of a fully connected network having the countries as nodes. Thereafter
we define and introduce the overlapping index of weighted network nodes. A
cluster structure of EU countries is derived from the statistically relevant
eigenvalues and eigenvectors of the adjacency matrix. This may be considered to
yield some information about the structure, stability and evolution of the EU
country clusters in a macroeconomic sense.",Eur. Phys. J. B 63 (2008) 533-539
merger and acquisition GDP,http://arxiv.org/abs/1902.05218v1,"Regional economic status inference from information flow and talent
  mobility","Novel data has been leveraged to estimate socioeconomic status in a timely
manner, however, direct comparison on the use of social relations and talent
movements remains rare. In this letter, we estimate the regional economic
status based on the structural features of the two networks. One is the online
information flow network built on the following relations on social media, and
the other is the offline talent mobility network built on the anonymized resume
data of job seekers with higher education. We find that while the structural
features of both networks are relevant to economic status, the talent mobility
network in a relatively smaller size exhibits a stronger predictive power for
the gross domestic product (GDP). In particular, a composite index of
structural features can explain up to about 84% of the variance in GDP. The
result suggests future socioeconomic studies to pay more attention to the
cost-effective talent mobility data.","EPL (Europhysics Letters), 125(6) (2019) 68002"
merger and acquisition GDP,http://arxiv.org/abs/physics/0101078v1,A Unifying Hypothesis for the Conformational Change of Tubulin,"Microtubule dynamic instability arises from the hydrolysis of GTP bound to
the beta-monomer of the tubulin dimer. The conformational change induced by
hydrolysis is unknown, but microtubules disassemble into protofilaments of
GDP-bound tubulin that curve away from the microtubule axis. This paper
presents the unfolding of a portion of the tubulin molecule into the
microtubule interior as a plausible, unifying explanation for diverse
structural and kinetic features of microtubules. This is the first specific
structural hypothesis for the hydrolysis induced conformational change of
tubulin that simultaneously explains weakening of lateral bonds, bending about
longitudinal bonds, changes in protofilament supertwist associated with GTP
hydrolysis, structural features of GDP-tubulin double rings, faster disassembly
at higher temperatures and slower disassembly in the presence of glycerol and
deuterium oxide. The hypothesis suggests further theoretical investigations and
direct experimental tests.",
merger and acquisition GDP,http://arxiv.org/abs/1112.4708v1,"Transformation Networks: How Innovation and the Availability of
  Technology can Increase Economic Performance","A transformation network describes how one set of resources can be
transformed into another via technological processes. Transformation networks
in economics are useful because they can highlight areas for future
innovations, both in terms of new products, new production techniques, or
better efficiency. They also make it easy to detect areas where an economy
might be fragile. In this paper, we use computational simulations to
investigate how the density of a transformation network affects the economic
performance, as measured by the gross domestic product (GDP), of an artificial
economy. Our results show that on average, the GDP of our economy increases as
the density of the transformation network increases. We also find that while
the average performance increases, the maximum possible performance decreases
and the minimum possible performance increases.",
merger and acquisition GDP,http://arxiv.org/abs/0911.1044v1,"Macro-level Indicators of the Relations between Research Funding and
  Research Output","In response to the call for a science of science policy, we discuss the
contribution of indicators at the macro-level of nations from a scientometric
perspective. In addition to global trends such as the rise of China, one can
relate percentages of world share of publications to government expenditure in
academic research. The marginal costs of improving one's share are increasing
over time. Countries differ considerably in terms of the efficiency of turning
(financial) input into bibliometrically measurable output. Both funding schemes
and disciplinary portfolios differ among countries. A price per paper can
nevertheless be estimated. The percentages of GDP spent on academic research in
different nations are significantly correlated to historical contingencies such
as the percentage of researchers in the population. The institutional dynamics
make strategic objectives such as the Lisbon objective of the EU--that is,
spending 3% of GDP for R&D in 2010--unrealistic.","Loet Leydesdorff & Caroline Wagner, Macro-level Indicators of the
  Relations between Research Funding and Research Output; Journal of
  Informetrics 3(4) (2009), 353-362"
merger and acquisition GDP,http://arxiv.org/abs/1906.01997v3,MEDEAS-World model calibration for the study of the energy transition,"MEDEAS (Modelling the Energy Development under Environmental And
Socioeconomic constraint) World is a new global-aggregated
energy-economy-environmental model, which runs from 1995 to 2050. In this work,
we tested the MEDEAS world model to reproduce the IPCC (International Panel on
Climate Change) GHG (Green House Gases) emission pathways consistent with 2
{\deg}C Global Warming. We achieved parameter optimizations of the MEDEAS model
related to different scenarios until 2050. We chose to provide a sensitivity
analysis on the parameters that directly influence the emission curves focusing
on the annual growth of the RES (Renewable Energy Sources), GDP (Gross Domestic
Product) and annual population growth. From such an analysis, it has been
possible to infer the large impact of GDP on the emission scenarios.",
merger and acquisition GDP,http://arxiv.org/abs/1506.06008v1,"4πβ (LS)-γ (HPGe) Digital Coincidence System Based on
  Synchronous High-Speed Multichannel Data Acquisition","A dedicated 4{\pi}{\beta} (LS)-{\gamma} (HPGe)digital coincidence system has
been developed in this work, which includes five acquisition channels. Three
analog-to-digital converter (ADC) acquisition channels with an acquisition
resolution of 8 bits and acquisition rate of 1GSPS (sample per second) are
utilized to collect the signals from three Photo multiplier tubes (PMTs) which
are adopted to detect {\beta} decay, and two acquisition channels with an
acquisition resolution of 16 bits and acquisition rate of 50MSPS are utilized
to collect the signals from high-purity germanium (HPGe) which are adopted to
detect {\gamma} decay. In order to increase the accuracy of the coincidence
system, all the five acquisition channels are synchronous within 500ps. The
data collected by the five acquisition channels will be transmitted to the host
PC through PCI bus and saved as a file. Off-line software is applied for the
4{\pi}{\beta} (LS)-{\gamma} (HPGe) coincidence and data analysis as needed in
practical application. With all the above preconditions, the flexibility of the
system is increased, and the structure and application of the system are
simplified. According to the test, the highest coincidence rate of the system
is 20K per second, which is sufficient for most applications. This paper mainly
introduces the design of the hardware, the synchronization method and the test
result of this system.",
merger and acquisition GDP,http://arxiv.org/abs/1010.5854v2,"An Alternative Approach to Data Acquisition Using Keyboard Emulation
  Technique","A number of data acquisition systems depend on human interface to access
computer for measuring, processing and analyzing data and to prepare it for
presentation and storage. Data acquisition software is installed on the
computer and all intended operations are performed manually. The data
acquisition software requires user intervention for operations like selection
of measurement setup, acquisition and storage of data to computer. The duty of
users becomes laborious if the data acquisition process lasts for a long
duration and requires continuous repetition of steps. An appropriate solution
to overcome such problem is to replace the physical operator with a virtual
user. This software generated simulated user sits at the data acquisition
process through out and automate all the intended steps of data acquisition.
This paper presents a new approach for data acquisition by using keyboard
emulation technique. A keyboard emulation software is developed which runs
beside the main data acquisition software and acts as a virtual user. All the
operations which require user interface are performed through fully automated
computer program. The developed software/system is executed in a real time
environment and the functionality of the software is verified. In the end,
potential application areas of the designed keyboard emulation software are
explored.",
merger and acquisition CPI,http://arxiv.org/abs/1206.3282v1,Improving the Accuracy and Efficiency of MAP Inference for Markov Logic,"In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in
two different MAP inference methods: the current method of choice for MAP
inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming.",
merger and acquisition CPI,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition CPI,http://arxiv.org/abs/1006.4622v2,"A High-Resolution Human Contact Network for Infectious Disease
  Transmission","The most frequent infectious diseases in humans - and those with the highest
potential for rapid pandemic spread - are usually transmitted via droplets
during close proximity interactions (CPIs). Despite the importance of this
transmission route, very little is known about the dynamic patterns of CPIs.
Using wireless sensor network technology, we obtained high-resolution data of
CPIs during a typical day at an American high school, permitting the
reconstruction of the social network relevant for infectious disease
transmission. At a 94% coverage, we collected 762,868 CPIs at a maximal
distance of 3 meters among 788 individuals. The data revealed a high density
network with typical small world properties and a relatively homogenous
distribution of both interaction time and interaction partners among subjects.
Computer simulations of the spread of an influenza-like disease on the weighted
contact graph are in good agreement with absentee data during the most recent
influenza season. Analysis of targeted immunization strategies suggested that
contact network data are required to design strategies that are significantly
more effective than random immunization. Immunization strategies based on
contact network data were most effective at high vaccination coverage.",
merger and acquisition CPI,http://arxiv.org/abs/1405.2878v1,Approximate Policy Iteration Schemes: A Comparison,"We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on several approximate
variations of the Policy Iteration algorithm: Approximate Policy Iteration,
Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search
by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),
and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all
algorithms, we describe performance bounds, and make a comparison by paying a
particular attention to the concentrability constants involved, the number of
iterations and the memory required. Our analysis highlights the following
points: 1) The performance guarantee of CPI can be arbitrarily better than that
of API/API($\alpha$), but this comes at the cost of a relative---exponential in
$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$
enjoys the best of both worlds: its performance guarantee is similar to that of
CPI, but within a number of iterations similar to that of API. 3) Contrary to
API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$
is proportional to their number of iterations, which may be problematic when
the discount factor $\gamma$ is close to 1 or the approximation error
$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make
an overall trade-off between memory and performance. Simulations with these
schemes confirm our analysis.",
merger and acquisition CPI,http://arxiv.org/abs/1705.08775v2,"A Control Performance Index for Multicopters Under Off-nominal
  Conditions","In order to prevent loss of control (LOC) accidents,the real-time control
performance monitoring problem is studied for multicopters. Different from the
existing work, this paper does not try to monitor the performance of the
controllers directly. In turn, the disturbances of multicopters under
off-nominal conditions are estimated to affect a proposed index to tell the
user whether the multicopter will be LOC or not. Firstly, a new degree of
controllability (DoC) will be proposed for multicopters subject to control
constrains and off-nominal conditions. Then a control performance index (CPI)
is defined based on the new DoC to reflect the control performance for
multicopters. Besides, the proposed CPI is applied to a new switching control
framework to guide the control decision of multicopter under off-nominal
conditions. Finally, simulation and experimental results show the effectiveness
of the CPI and the proposed switching control framework.",
merger and acquisition CPI,http://arxiv.org/abs/1906.09784v1,Deep Conservative Policy Iteration,"Conservative Policy Iteration (CPI) is a founding algorithm of Approximate
Dynamic Programming (ADP). Its core principle is to stabilize greediness
through stochastic mixtures of consecutive policies. It comes with strong
theoretical guarantees, and inspired approaches in deep Reinforcement Learning
(RL). However, CPI itself has rarely been implemented, never with neural
networks, and only experimented on toy problems. In this paper, we show how CPI
can be practically combined with deep RL with discrete actions. We also
introduce adaptive mixture rates inspired by the theory. We experiment
thoroughly the resulting algorithm on the simple Cartpole problem, and validate
the proposed method on a representative subset of Atari games. Overall, this
work suggests that revisiting classic ADP may lead to improved and more stable
deep RL algorithms.",
merger and acquisition CPI,http://arxiv.org/abs/1710.05944v1,Neuro Fuzzy Modelling for Prediction of Consumer Price Index,"Economic indicators such as Consumer Price Index (CPI) have frequently used
in predicting future economic wealth for financial policy makers of respective
country. Most central banks, on guidelines of research studies, have recently
adopted an inflation targeting monetary policy regime, which accounts for high
requirement for effective prediction model of consumer price index. However,
prediction accuracy by numerous studies is still low, which raises a need for
improvement. This manuscript presents findings of study that use neuro fuzzy
technique to design a machine-learning model that train and test data to
predict a univariate time series CPI. The study establishes a matrix of monthly
CPI data from secondary data source of Tanzania National Bureau of Statistics
from January 2000 to December 2015 as case study and thereafter conducted
simulation experiments on MATLAB whereby ninety five percent (95%) of data used
to train the model and five percent (5%) for testing. Furthermore, the study
use root mean square error (RMSE) and mean absolute percentage error (MAPE) as
error metrics for model evaluation. The results show that the neuro fuzzy model
have an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2,
2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to
existing research studies.","International Journal of Artificial Intelligence and Applications
  (IJAIA), Vol.8, No.5, September 2017"
merger and acquisition CPI,http://arxiv.org/abs/0708.3387v1,"The Impact of Noise Correlation and Channel Phase Information on the
  Data-Rate of the Single-Symbol ML Decodable Distributed STBCs","Very recently, we proposed the row-monomial distributed orthogonal space-time
block codes (DOSTBCs) and showed that the row-monomial DOSTBCs achieved
approximately twice higher bandwidth efficiency than the repetitionbased
cooperative strategy [1]. However, we imposed two limitations on the
row-monomial DOSTBCs. The first one was that the associated matrices of the
codes must be row-monomial. The other was the assumption that the relays did
not have any channel state information (CSI) of the channels from the source to
the relays, although this CSI could be readily obtained at the relays without
any additional pilot signals or any feedback overhead. In this paper, we first
remove the row-monomial limitation; but keep the CSI limitation. In this case,
we derive an upper bound of the data-rate of the DOSTBC and it is larger than
that of the row-monomial DOSTBCs in [1]. Secondly, we abandon the CSI
limitation; but keep the row-monomial limitation. Specifically, we propose the
row-monomial DOSTBCs with channel phase information (DOSTBCs-CPI) and derive an
upper bound of the data-rate of those codes. The rowmonomial DOSTBCs-CPI have
higher data-rate than the DOSTBCs and the row-monomial DOSTBCs. Furthermore, we
find the actual row-monomial DOSTBCs-CPI which achieve the upper bound of the
data-rate.",
merger and acquisition CPI,http://arxiv.org/abs/1306.0539v1,"On the Performance Bounds of some Policy Search Dynamic Programming
  Algorithms","We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on Policy Search algorithms,
that compute an approximately optimal policy by following the standard Policy
Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,
2002; Lazaric et al., 2010). We describe existing and a few new performance
bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et
al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)
(Kakade and Langford, 2002). By paying a particular attention to the
concentrability constants involved in such guarantees, we notably argue that
the guarantee of CPI is much better than that of DPI, but this comes at the
cost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of time
complexity. We then describe an algorithm, Non-Stationary Direct Policy
Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search
by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon
situation or 2) a simplified version of the Non-Stationary PI with growing
period of Scherrer and Lesner (2012). We provide an analysis of this algorithm,
that shows in particular that it enjoys the best of both worlds: its
performance guarantee is similar to that of CPI, but within a time complexity
similar to that of DPI.",
merger and acquisition CPI,http://arxiv.org/abs/1405.1750v1,Numerical simulation of turbulent duct flows with constant power input,"The numerical simulation of a flow through a duct requires an externally
specified forcing that makes the fluid flow against viscous friction. To this
aim, it is customary to enforce a constant value for either the flow rate (CFR)
or the pressure gradient (CPG). When comparing a laminar duct flow before and
after a geometrical modification that induces a change of the viscous drag,
both approaches (CFR and CPG) lead to a change of the power input across the
comparison. Similarly, when carrying out the (DNS and LES) numerical simulation
of unsteady turbulent flows, the power input is not constant over time.
Carrying out a simulation at constant power input (CPI) is thus a further
physically sound option, that becomes particularly appealing in the context of
flow control, where a comparison between control-on and control-off conditions
has to be made.
  We describe how to carry out a CPI simulation, and start with defining a new
power-related Reynolds number, whose velocity scale is the bulk flow that can
be attained with a given pumping power in the laminar regime. Under the CPI
condition, we derive a relation that is equivalent to the
Fukagata--Iwamoto--Kasagi relation valid for CFR (and to its extension valid
for CPG), that presents the additional advantage of natively including the
required control power. The implementation of the CPI approach is then
exemplified in the standard case of a plane turbulent channel flow, and then
further applied to a flow control case, where the spanwise-oscillating wall is
used for skin friction drag reduction. For this low-Reynolds number flow, using
90% of the available power for the pumping system and the remaining 10% for the
control system is found to be the optimum share that yields the largest
increase of the flow rate above the reference case, where 100% of the power
goes to the pump.",
merger and acquisition CPI,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition CPI,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition CPI,http://arxiv.org/abs/1907.07776v1,CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers,"Memory controller scheduling is crucial in multicore processors, where DRAM
bandwidth is shared. Since increased number of requests from multiple cores of
processors becomes a source of bottleneck, scheduling the requests efficiently
is necessary to utilize all the computing power these processors offer.
However, current multicore processors are using traditional memory controllers,
which are designed for single-core processors. They are unable to adapt to
changing characteristics of memory workloads that run simultaneously on
multiple cores. Existing schedulers may disrupt locality and bank parallelism
among data requests coming from different cores. Hence, novel memory
controllers that consider and adapt to the memory access characteristics, and
share memory resources efficiently and fairly are necessary. We introduce
Core-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses
Reinforcement Learning (RL) to alter its scheduling strategy dynamically at
runtime. Our scheduler utilizes locality among data requests from multiple
cores and exploits parallelism in accessing multiple banks of DRAM. CADS is
also able to share the DRAM while guaranteeing fairness to all cores accessing
memory. Using CADS policy, we achieve 20% better cycles per instruction (CPI)
in running memory intensive and compute intensive PARSEC parallel benchmarks
simultaneously, and 16% better CPI with SPEC 2006 benchmarks.",
merger and acquisition CPI,http://arxiv.org/abs/1709.00310v3,"Detection via simultaneous trajectory estimation and long time
  integration","In this work, we consider the detection of manoeuvring small objects with
radars. Such objects induce low signal to noise ratio (SNR) reflections in the
received signal. We consider both co-located and separated transmitter/receiver
pairs, i.e., mono-static and bi-static configurations, respectively, as well as
multi-static settings involving both types. We propose a detection approach
which is capable of coherently integrating these reflections within a coherent
processing interval (CPI) in all these configurations and continuing
integration for an arbitrarily long time across consecutive CPIs. We estimate
the complex value of the reflection coefficients for integration while
simultaneously estimating the object trajectory. Compounded with this is the
estimation of the unknown time reference shift of the separated transmitters
necessary for coherent processing. Detection is made by using the resulting
integration value in a Neyman-Pearson test against a constant false alarm rate
threshold. We demonstrate the efficacy of our approach in a simulation example
with a very low SNR object which cannot be detected with conventional
techniques.",
merger and acquisition CPI,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition CPI,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition CPI,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition CPI,http://arxiv.org/abs/0807.4219v1,Statistical properties of world investment networks,"We have performed a detailed investigation on the world investment networks
constructed from the Coordinated Portfolio Investment Survey (CPIS) data of the
International Monetary Fund, ranging from 2001 to 2006. The distributions of
degrees and node strengthes are scale-free. The weight distributions can be
well modeled by the Weibull distribution. The maximum flow spanning trees of
the world investment networks possess two universal allometric scaling
relations, independent of time and the investment type. The topological scaling
exponent is $1.17\pm0.02$ and the flow scaling exponent is $1.03\pm0.01$.","Physica A 388 (12), 2450-2460 (2009)"
merger and acquisition CPI,http://arxiv.org/abs/1507.02456v2,Towards Log-Linear Logics with Concrete Domains,"We present $\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an
extension of the log-linear description logics $\mathcal{EL}^{++}$-LL with
concrete domains, nominals, and instances. We use Markov logic networks (MLNs)
in order to find the most probable, classified and coherent $\mathcal{EL}^{++}$
ontology from an $\mathcal{MEL}^{++}$ knowledge base. In particular, we develop
a novel way to deal with concrete domains (also known as datatypes) by
extending MLN's cutting plane inference (CPI) algorithm.",
merger and acquisition CPI,http://arxiv.org/abs/1901.06103v1,"Exploring Semi-supervised Variational Autoencoders for Biomedical
  Relation Extraction","The biomedical literature provides a rich source of knowledge such as
protein-protein interactions (PPIs), drug-drug interactions (DDIs) and
chemical-protein interactions (CPIs). Biomedical relation extraction aims to
automatically extract biomedical relations from biomedical text for various
biomedical research. State-of-the-art methods for biomedical relation
extraction are primarily based on supervised machine learning and therefore
depend on (sufficient) labeled data. However, creating large sets of training
data is prohibitively expensive and labor-intensive, especially so in
biomedicine as domain knowledge is required. In contrast, there is a large
amount of unlabeled biomedical text available in PubMed. Hence, computational
methods capable of employing unlabeled data to reduce the burden of manual
annotation are of particular interest in biomedical relation extraction. We
present a novel semi-supervised approach based on variational autoencoder (VAE)
for biomedical relation extraction. Our model consists of the following three
parts, a classifier, an encoder and a decoder. The classifier is implemented
using multi-layer convolutional neural networks (CNNs), and the encoder and
decoder are implemented using both bidirectional long short-term memory
networks (Bi-LSTMs) and CNNs, respectively. The semi-supervised mechanism
allows our model to learn features from both the labeled and unlabeled data. We
evaluate our method on multiple public PPI, DDI and CPI corpora. Experimental
results show that our method effectively exploits the unlabeled data to improve
the performance and reduce the dependence on labeled data. To our best
knowledge, this is the first semi-supervised VAE-based method for (biomedical)
relation extraction. Our results suggest that exploiting such unlabeled data
can be greatly beneficial to improved performance in various biomedical
relation extraction.",
merger and acquisition CPI,http://arxiv.org/abs/physics/0008035v2,2.856-GHz Modulation of Conventional Triode Electron Gun,"For the generation of picosecond (< 100 ps) electron beam pulses, we studied
the RF modulation of a conventional triode electron gun. The feasibility study
for this scheme has been experimentally investigated by modulating a triode gun
of the Y-824 cathode-grid (KG) structure provided by the CPI Eimac, with
2.856-GHz pulsed RF generated by a solid-state amplifier (SSA). In this paper,
we present the methods and results of this investigation.",eConf C000821 (2000) MOB18
merger and acquisition CPI,http://arxiv.org/abs/1504.04974v1,Understanding Big Data Analytic Workloads on Modern Processors,"Big data analytics applications play a significant role in data centers, and
hence it has become increasingly important to understand their behaviors in
order to further improve the performance of data center computer systems, in
which characterizing representative workloads is a key practical problem. In
this paper, after investigating three most impor- tant application domains in
terms of page views and daily visitors, we chose 11 repre- sentative data
analytics workloads and characterized their micro-architectural behaviors by
using hardware performance counters, so as to understand the impacts and
implications of data analytics workloads on the systems equipped with modern
superscalar out-of-order processors. Our study reveals that big data analytics
applications themselves share many inherent characteristics, which place them
in a different class from traditional workloads and scale-out services. To
further understand the characteristics of big data analytics work- loads we
performed a correlation analysis of CPI (cycles per instruction) with other
micro- architecture level characteristics and an investigation of the big data
software stack impacts on application behaviors. Our correlation analysis
showed that even though big data ana- lytics workloads own notable pipeline
front end stalls, the main factors affecting the CPI performance are long
latency data accesses rather than the front end stalls. Our software stack
investigation found that the typical big data software stack significantly
contributes to the front end stalls and incurs bigger working set. Finally we
gave several recommen- dations for architects, programmers and big data system
designers with the knowledge acquired from this paper.",
merger and acquisition CPI,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition CPI,http://arxiv.org/abs/1506.06008v1,"4πβ (LS)-γ (HPGe) Digital Coincidence System Based on
  Synchronous High-Speed Multichannel Data Acquisition","A dedicated 4{\pi}{\beta} (LS)-{\gamma} (HPGe)digital coincidence system has
been developed in this work, which includes five acquisition channels. Three
analog-to-digital converter (ADC) acquisition channels with an acquisition
resolution of 8 bits and acquisition rate of 1GSPS (sample per second) are
utilized to collect the signals from three Photo multiplier tubes (PMTs) which
are adopted to detect {\beta} decay, and two acquisition channels with an
acquisition resolution of 16 bits and acquisition rate of 50MSPS are utilized
to collect the signals from high-purity germanium (HPGe) which are adopted to
detect {\gamma} decay. In order to increase the accuracy of the coincidence
system, all the five acquisition channels are synchronous within 500ps. The
data collected by the five acquisition channels will be transmitted to the host
PC through PCI bus and saved as a file. Off-line software is applied for the
4{\pi}{\beta} (LS)-{\gamma} (HPGe) coincidence and data analysis as needed in
practical application. With all the above preconditions, the flexibility of the
system is increased, and the structure and application of the system are
simplified. According to the test, the highest coincidence rate of the system
is 20K per second, which is sufficient for most applications. This paper mainly
introduces the design of the hardware, the synchronization method and the test
result of this system.",
merger and acquisition CPI,http://arxiv.org/abs/1010.5854v2,"An Alternative Approach to Data Acquisition Using Keyboard Emulation
  Technique","A number of data acquisition systems depend on human interface to access
computer for measuring, processing and analyzing data and to prepare it for
presentation and storage. Data acquisition software is installed on the
computer and all intended operations are performed manually. The data
acquisition software requires user intervention for operations like selection
of measurement setup, acquisition and storage of data to computer. The duty of
users becomes laborious if the data acquisition process lasts for a long
duration and requires continuous repetition of steps. An appropriate solution
to overcome such problem is to replace the physical operator with a virtual
user. This software generated simulated user sits at the data acquisition
process through out and automate all the intended steps of data acquisition.
This paper presents a new approach for data acquisition by using keyboard
emulation technique. A keyboard emulation software is developed which runs
beside the main data acquisition software and acts as a virtual user. All the
operations which require user interface are performed through fully automated
computer program. The developed software/system is executed in a real time
environment and the functionality of the software is verified. In the end,
potential application areas of the designed keyboard emulation software are
explored.",
merger and acquisition CPI,http://arxiv.org/abs/1802.09669v1,"A Multi-Disciplinary Review of Knowledge Acquisition Methods: From Human
  to Autonomous Eliciting Agents","This paper offers a multi-disciplinary review of knowledge acquisition
methods in human activity systems. The review captures the degree of
involvement of various types of agencies in the knowledge acquisition process,
and proposes a classification with three categories of methods: the human
agent, the human-inspired agent, and the autonomous machine agent methods. In
the first two categories, the acquisition of knowledge is seen as a cognitive
task analysis exercise, while in the third category knowledge acquisition is
treated as an autonomous knowledge-discovery endeavour. The motivation for this
classification stems from the continuous change over time of the structure,
meaning and purpose of human activity systems, which are seen as the factor
that fuelled researchers' and practitioners' efforts in knowledge acquisition
for more than a century.
  We show through this review that the KA field is increasingly active due to
the higher and higher pace of change in human activity, and conclude by
discussing the emergence of a fourth category of knowledge acquisition methods,
which are based on red-teaming and co-evolution.","Knowledge-Based Systems, Volume 105, Elsevier, 2016"
merger and acquisition CPI,http://arxiv.org/abs/1304.1116v1,"Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection","Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS.",
merger and acquisition CPI,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
merger and acquisition CPI,http://arxiv.org/abs/1910.08338v1,A Gigapixel Computational Light-Field Camera,"Light-field cameras allow the acquisition of both the spatial and angular
components of the light. This has a wide range of applications from image
refocusing to 3D reconstruction of a scene. The conventional way to perform
such acquisitions leads to a strong spatio-angular resolution limit. Here we
propose a computational version of the light-field camera. We perform a one
gigapixel photo-realistic diffraction limited light-field acquisition, that
would require the use of a one gigapixel sensor were the acquisition to be
performed with a conventional light-field camera. This result is mostly limited
by the total acquisition time, as our system could in principle allow
$\sim$Terapixel reconstructions to be achieved. The reported result presents
many potential advantages, such as the possibility to perform large depth of
field light-field acquisitions, realistic refocusing along a very wide range of
depths, very high dimensional super-resolved image acquisitions, and large
depth of field 3D reconstructions.",
merger and acquisition CPI,http://arxiv.org/abs/1610.05462v2,"Towards the Leveraging of Data Deduplication to Break the Disk
  Acquisition Speed Limit","Digital forensic evidence acquisition speed is traditionally limited by two
main factors: the read speed of the storage device being investigated, i.e.,
the read speed of the disk, memory, remote storage, mobile device, etc.), and
the write speed of the system used for storing the acquired data. Digital
forensic investigators can somewhat mitigate the latter issue through the use
of high-speed storage options, such as networked RAID storage, in the
controlled environment of the forensic laboratory. However, traditionally,
little can be done to improve the acquisition speed past its physical read
speed from the target device itself. The protracted time taken for data
acquisition wastes digital forensic experts' time, contributes to digital
forensic investigation backlogs worldwide, and delays pertinent information
from potentially influencing the direction of an investigation. In a remote
acquisition scenario, a third contributing factor can also become a detriment
to the overall acquisition time - typically the Internet upload speed of the
acquisition system. This paper explores an alternative to the traditional
evidence acquisition model through the leveraging of a forensic data
deduplication system. The advantages that a deduplicated approach can provide
over the current digital forensic evidence acquisition process are outlined and
some preliminary results of a prototype implementation are discussed.",
merger and acquisition CPI,http://arxiv.org/abs/1903.00269v1,"Covariance-Aided CSI Acquisition with Non-Orthogonal Pilots in Massive
  MIMO Systems","Massive multiple-input multiple-output (MIMO) systems use antenna arrays with
a large number of antenna elements to serve many different users
simultaneously. The large number of antennas in the system makes, however, the
channel state information (CSI) acquisition strategy design critical and
particularly challenging. Interestingly, in the context of massive MIMO
systems, channels exhibit a large degree of spatial correlation which can be
exploited to cope with the dimensionality problem in CSI acquisition. With the
final objective of analyzing the benefits of covariance-aided uplink multi-user
CSI acquisition in massive MIMO systems, here we compare the channel estimation
mean-square error (MSE) for (i) conventional CSI acquisition, which does not
assume any knowledge on the user spatial covariances and uses orthogonal pilot
sequences; and (ii) covariance-aided CSI acquisition, which exploits the
individual covariance matrices for channel estimation and possibly uses
non-orthogonal pilot sequences. We apply a large-system analysis to the latter
case, for which new asymptotic MSE expressions are established under various
assumptions on the distributions of the pilot sequences and on the covariance
matrices. We link these expressions to those describing the estimation MSE of
conventional CSI acquisition with orthogonal pilot sequences of some equivalent
length. This analysis provides insights about how much the CSI acquisition
process can be overloaded (in the sense of allowing estimating CSI with
sufficient accuracy for more users than the number resource elements allocated
for training) when a covariance-aided approach is adopted, hinting at
potentially significant gains in the spectral efficiency of CSI acquisition in
Massive MIMO.",
merger and acquisition CPI,http://arxiv.org/abs/1403.4508v1,Research on Study Mechanical Vibrations with Data Acquisition Systems,"The paper presents a new study method of mechanic vibrations with the help of
the data acquisition systems. The study of vibrations with the help of data
acquisition systems allows the solving of some engineering problems connected
to the measurement of some parameters which are difficult to measure having in
view the improvement of the technical performances of the industrial equipment
or devices",
merger and acquisition CPI,http://arxiv.org/abs/1309.7119v3,"Stock price direction prediction by directly using prices data: an
  empirical study on the KOSPI and HSI","The prediction of a stock market direction may serve as an early
recommendation system for short-term investors and as an early financial
distress warning system for long-term shareholders. Many stock prediction
studies focus on using macroeconomic indicators, such as CPI and GDP, to train
the prediction model. However, daily data of the macroeconomic indicators are
almost impossible to obtain. Thus, those methods are difficult to be employed
in practice. In this paper, we propose a method that directly uses prices data
to predict market index direction and stock price direction. An extensive
empirical study of the proposed method is presented on the Korean Composite
Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual
constituents included in the indices. The experimental results show notably
high hit ratios in predicting the movements of the individual constituents in
the KOSPI and HIS.",
merger and acquisition CPI,http://arxiv.org/abs/1602.07367v1,"Classical realization of dispersion-canceled, artifact-free, and
  background-free optical coherence tomography","Quantum-optical coherence tomography (Q-OCT) provides a dispersion-canceled
axial-imaging method, but its practical use is limited by the weakness of the
light source and by artifacts in the images. A recent study using chirped-pulse
interferometry (CPI) has demonstrated dispersion-canceled and artifact-free OCT
with a classical system; however, unwanted background signals still remain
after removing the artifacts. Here, we propose a classical optical method that
realizes dispersion-canceled, artifact-free, and background-free OCT. We employ
a time-reversed system for Q-OCT with transform-limited input laser pulses to
achieve dispersion-canceled OCT with a classical system. We have also
introduced a subtraction method to remove artifacts and background signals.
With these methods, we experimentally demonstrated dispersion-canceled,
artifact-free, and background-free axial imaging of a coverglass and
cross-sectional imaging of the surface of a coin.","Optics Express Vol. 24, Issue 8, pp. 8280-8289 (2016)"
merger and acquisition CPI,http://arxiv.org/abs/1212.2044v2,Macro-Economic Time Series Modeling and Interaction Networks,"Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.","Applications of Evolutionary Computation, LNCS 6625 (Springer
  Berlin Heidelberg), pp. 101-110 (2011)"
merger and acquisition CPI,http://arxiv.org/abs/1909.02769v1,"Adaptive Trust Region Policy Optimization: Global Convergence and Faster
  Rates for Regularized MDPs","Trust region policy optimization (TRPO) is a popular and empirically
successful policy search algorithm in Reinforcement Learning (RL) in which a
surrogate problem, that restricts consecutive policies to be `close' to one
another, is iteratively solved. Nevertheless, TRPO has been considered a
heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show
that the adaptive scaling mechanism used in TRPO is in fact the natural ""RL
version"" of traditional trust-region methods from convex analysis. We first
analyze TRPO in the planning setting, in which we have access to the model and
the entire state space. Then, we consider sample-based TRPO and establish
$\tilde O(1/\sqrt{N})$ convergence rate to the global optimum. Importantly, the
adaptive scaling mechanism allows us to analyze TRPO in {\em regularized MDPs}
for which we prove fast rates of $\tilde O(1/N)$, much like results in convex
optimization. This is the first result in RL of better rates when regularizing
the instantaneous cost or reward.",
merger and acquisition CPI,http://arxiv.org/abs/1607.02818v1,Equation-free analysis of a dynamically evolving multigraph,"In order to illustrate the adaptation of traditional continuum numerical
techniques to the study of complex network systems, we use the equation-free
framework to analyze a dynamically evolving multigraph. This approach is based
on coupling short intervals of direct dynamic network simulation with
appropriately-defined lifting and restriction operators, mapping the detailed
network description to suitable macroscopic (coarse-grained) variables and
back. This enables the acceleration of direct simulations through Coarse
Projective Integration (CPI), as well as the identification of coarse
stationary states via a Newton-GMRES method. We also demonstrate the use of
data-mining, both linear (principal component analysis, PCA) and nonlinear
(diffusion maps, DMAPS) to determine good macroscopic variables (observables)
through which one can coarse-grain the model. These results suggest methods for
decreasing simulation times of dynamic real-world systems such as
epidemiological network models. Additionally, the data-mining techniques could
be applied to a diverse class of problems to search for a succint,
low-dimensional description of the system in a small number of variables.",
merger and acquisition CPI,http://arxiv.org/abs/1903.00191v1,"MIPS-Core Application Specific Instruction-Set Processor for IDEA
  Cryptography - Comparison between Single-Cycle and Multi-Cycle Architectures","A single-cycle processor completes the execution of an instruction in only
one clock cycle. However, its clock period is usually rather long. On the
contrary, although clock frequency is higher in a multi-cycle processor, it
takes several clock cycles to finish an instruction. Therefore, their runtime
efficiencies depend on which program is executed. This paper presents a new
processor for International Data Encryption Algorithm (IDEA) cryptography. The
new design is an Application Specific Instruction-set Processor (ASIP) in which
both general-purpose and special instructions are supported. It is a
single-cycle MIPS-core architecture, whose average Clocks Per Instruction (CPI)
is 1. Furthermore, a comparison is provided in this paper to show the
differences between the proposed single-cycle processor and another comparable
multi-cycle crypto processor. FPGA implementation results show that both
architectures have almost the same encoding/decoding throughput. However, the
previous processor consumes nearly twice as many resources as the new one does.",
merger and acquisition CPI,http://arxiv.org/abs/1304.2894v1,"A Data Acquisition and Monitoring System for the Detector Development
  Group at FZJ/ZEA-2","The central institute of electronics (ZEA-2) in the Forschungszentrum Juelich
(FZJ) has developed the novel readout electronics JUDIDT to cope with high-rate
data acquisition of the KWS-1 and KWS-2 detectors in the experimental Hall at
the Forschungsreacktor Muenchen FRM-II in Garching, Muenchen. This electronics
has been then modified and used also for the data-acquisition of a prototype
for an ANGER Camera proposed for the planned European Spallation Source. To
commission the electronics, software for the data acquisition and the data
monitoring has been developed. In this report the software is described.",
merger and acquisition CPI,http://arxiv.org/abs/1301.3893v1,A Knowledge Acquisition Tool for Bayesian-Network Troubleshooters,"This paper describes a domain-specific knowledge acquisition tool for
intelligent automated troubleshooters based on Bayesian networks. No Bayesian
network knowledge is required to use the tool, and troubleshooting information
can be specified as natural and intuitive as possible. Probabilities can be
specified in the direction that is most natural to the domain expert. Thus, the
knowledge acquisition efficiently removes the traditional knowledge acquisition
bottleneck of Bayesian networks.",
merger and acquisition CPI,http://arxiv.org/abs/1605.02559v1,"Robust imaging of hippocampal inner structure at 7T: in vivo acquisition
  protocol and methodological choices","OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in
vivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high
resolution imaging, such as can be achieved with 7T MRI. An acquisition
protocol was designed for imaging hippocampal inner structure at 7T. It relies
on a compromise between anatomical details visibility and robustness to motion.
In order to reduce acquisition time and motion artifacts, the full slab
covering the hippocampus was split into separate slabs with lower acquisition
time. A robust registration approach was implemented to combine the acquired
slabs within a final 3D-consistent high-resolution slab covering the whole
hippocampus. Evaluation was performed on 50 subjects overall, made of three
groups of subjects acquired using three acquisition settings; it focused on
three issues: visibility of hippocampal inner structure, robustness to motion
artifacts and registration procedure performance.RESULTS:Overall, T2-weighted
acquisitions with interleaved slabs proved robust. Multi-slab registration
yielded high quality datasets in 96 % of the subjects, thus compatible with
further analyses of hippocampal inner structure.CONCLUSION:Multi-slab
acquisition and registration setting is efficient for reducing acquisition time
and consequently motion artifacts for ultra-high resolution imaging of the
inner structure of the hippocampus.","Magnetic Resonance Materials in Physics, Biology and Medicine,
  Springer Verlag, 2016"
merger and acquisition CPI,http://arxiv.org/abs/physics/0111077v1,An Overview of the LIGO Control and Data Acquisition System,"The LIGO Control and Data system (CDS) features a tightly coupled and highly
integrated control and data acquisition system. Control of the interferometers
requires many Multiple Input Multiple Output (MIMO) control loops closed both
locally and across the 4-kilometer interferometer arm lengths. In addition to
providing the closed loop control, the control systems front end processors act
as Data Collection Units (DCU) for the data acquisition system. Data collected
by these front ends and the data acquisition system must be collected and time
stamped to an accuracy of 1 microsecond and made available to on-line analysis
tools such as the Global Diagnostics System (GDS)[1]. Data is also sent to the
LIGO Data Analysis System (LDAS)[2] for long-term storage and off-line
analysis. Data rates exceed 5 Mbytes per second per interferometer continuous.
Connection between the various front end processors and the data acquisition
system is achieved using fiber optic reflective memory networks. Both controls
and data acquisition systems use VME hardware and VxWorks operating systems.
This paper will present an overview of the LIGO CDS and discuss key aspects of
its design.",eConf C011127 (2001) TUBI001
merger and acquisition wages,http://arxiv.org/abs/1909.12338v1,Hardware Design and Analysis of the ACE and WAGE Ciphers,"This paper presents the hardware design and analysis of ACE and WAGE, two
candidate ciphers for the NIST Lightweight Cryptography standardization. Both
ciphers use sLiSCP's unified sponge duplex mode. ACE has an internal state of
320 bits, uses three 64 bit Simeck boxes, and implements both authenticated
encryption and hashing. WAGE is based on the Welch-Gong stream cipher and
provides authenticated encryption. WAGE has 259 bits of state, two 7 bit
Welch-Gong permutations, and four lightweight 7 bit S-boxes. ACE and WAGE have
the same external interface and follow the same I/O protocol to transition
between phases. The paper illustrates how a hardware perspective influenced key
aspects of the ACE and WAGE algorithms. The paper reports area, power, and
energy results for both serial and parallel (unrolled) implementations using
four different ASIC libraries: two 65 nm libraries, a 90 nm library, and a 130
nm library. ACE implementations range from a throughput of 0.5 bits-per-clock
cycle (bpc) and an area of 4210 GE (averaged across the four ASIC libraries) up
to 4 bpc and 7260 GE. WAGE results range from 0.57 bpc with 2920 GE to 4.57 bpc
with 11080 GE.",
merger and acquisition wages,http://arxiv.org/abs/1609.09067v1,Using Big Data to Decode Private Sector Wage Growth,"The U.S. labor market is dynamic and complex, and understanding wage data
across different segments of the workforce is critical to providing
policymakers and business leaders with actionable insights. There is no labor
index that assesses the labor market performance at such a detailed level as
the ADP Research Institute's Workforce Vitality Report (WVR). Drawing on the
actual, aggregated and anonymous payroll data of 24 million Americans paid by
ADP, the WVR looks at key dynamics and market indicators including wage growth,
hours worked and turnover rate. Unlike other data sets, the WVR calculates wage
growth of individual workers on a quarter-to-quarter basis, avoiding the
deviations caused by various workplace occurrences, like when new workers are
hired and older ones retire. In this paper, Dr. Ahu Yildirmaz, head of the ADP
Research Institute, drills down into wage growth by industry, age, gender and
income level, as well as for both job holders and job switchers. Using WVR
data, Ahu walks through those factors contributing to overall shifts in wage
growth, the future of the labor market and what this data means for today's
U.S. workforce.",
merger and acquisition wages,http://arxiv.org/abs/1712.05796v2,A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk,"A growing number of people are working as part of on-line crowd work, which
has been characterized by its low wages; yet, we know little about wage
distribution and causes of low/high earnings. We recorded 2,676 workers
performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis
revealed that workers earned a median hourly wage of only ~\$2/h, and only 4%
earned more than \$7.25/h. The average requester pays more than \$11/h,
although lower-paying requesters post much more work. Our wage calculations are
influenced by how unpaid work is included in our wage calculations, e.g., time
spent searching for tasks, working on tasks that are rejected, and working on
tasks that are ultimately not submitted. We further explore the characteristics
of tasks and working patterns that yield higher hourly wages. Our analysis
informs future platform design and worker tools to create a more positive
future for crowd work.",
merger and acquisition wages,http://arxiv.org/abs/1001.0627v1,The Labor Economics of Paid Crowdsourcing,"Crowdsourcing is a form of ""peer production"" in which work traditionally
performed by an employee is outsourced to an ""undefined, generally large group
of people in the form of an open call."" We present a model of workers supplying
labor to paid crowdsourcing projects. We also introduce a novel method for
estimating a worker's reservation wage--the smallest wage a worker is willing
to accept for a task and the key parameter in our labor supply model. It shows
that the reservation wages of a sample of workers from Amazon's Mechanical Turk
(AMT) are approximately log normally distributed, with a median wage of
$1.38/hour. At the median wage, the point elasticity of extensive labor supply
is 0.43. We discuss how to use our calibrated model to make predictions in
applied work. Two experimental tests of the model show that many workers
respond rationally to offered incentives. However, a non-trivial fraction of
subjects appear to set earnings targets. These ""target earners"" consider not
just the offered wage--which is what the rational model predicts--but also
their proximity to earnings goals. Interestingly, a number of workers clearly
prefer earning total amounts evenly divisible by 5, presumably because these
amounts make good targets.",
merger and acquisition wages,http://arxiv.org/abs/physics/0506229v1,"The Age-Competency Model to the Study of the Age-Wage Profiles for
  Workers","In this article, I present a new approach and a novel model to the study of
the life cycle of wages. The key idea is that wage can be thought as
remuneration paid for the competency. It is assumed with the approach that
there are three mechanisms acting at micro level and resulting in the change of
workers' competencies during their lives. These are an endogenous growth of
workers' initial competencies; a rate of investments in schooling in the life
cycle of wages; and an effect of relative losses in workers' competencies. The
developed model is to shed light on the processes resulting in the age-wage
profiles seen in mass. The model obeys a nonlinear integro-differential
equation. The found analytic solution of the equation has the form of Fisk PDF
of a special type. The solution and its features are discussed. The regression
technique is used to check the model upon reliability. The model provides
better fitting to the data (Elo and Salonen, 2004) than minceraninan earnings
function (Mincer, 1974) does.",
merger and acquisition wages,http://arxiv.org/abs/1706.10097v3,Design Activism for Minimum Wage Crowd Work,"Entry-level crowd work is often reported to pay less than minimum wage. While
this may be appropriate or even necessary, due to various legal, economic, and
pragmatic factors, some Requesters and workers continue to question this status
quo. To promote further discussion on the issue, we survey Requesters and
workers whether they would support restricting tasks to require minimum wage
pay. As a form of design activism, we confronted workers with this dilemma
directly by posting a dummy Mechanical Turk task which told them that they
could not work on it because it paid less than their local minimum wage, and we
invited their feedback. Strikingly, for those workers expressing an opinion,
two-thirds of Indians favored the policy while two-thirds of Americans opposed
it. Though a majority of Requesters supported minimum wage pay, only 20\% would
enforce it. To further empower Requesters, and to ensure that effort or
ignorance are not barriers to change, we provide a simple public API to make it
easy to find a worker's local minimum wage by his/her IP address.",
merger and acquisition wages,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition wages,http://arxiv.org/abs/1903.07032v1,TurkScanner: Predicting the Hourly Wage of Microtasks,"Workers in crowd markets struggle to earn a living. One reason for this is
that it is difficult for workers to accurately gauge the hourly wages of
microtasks, and they consequently end up performing labor with little pay. In
general, workers are provided with little information about tasks, and are left
to rely on noisy signals, such as textual description of the task or rating of
the requester. This study explores various computational methods for predicting
the working times (and thus hourly wages) required for tasks based on data
collected from other workers completing crowd work. We provide the following
contributions. (i) A data collection method for gathering real-world training
data on crowd-work tasks and the times required for workers to complete them;
(ii) TurkScanner: a machine learning approach that predicts the necessary
working time to complete a task (and can thus implicitly provide the expected
hourly wage). We collected 9,155 data records using a web browser extension
installed by 84 Amazon Mechanical Turk workers, and explored the challenge of
accurately recording working times both automatically and by asking workers.
TurkScanner was created using ~150 derived features, and was able to predict
the hourly wages of 69.6% of all the tested microtasks within a 75% error.
Directions for future research include observing the effects of tools on
people's working practices, adapting this approach to a requester tool for
better price setting, and predicting other elements of work (e.g., the
acceptance likelihood and worker task preferences.)",
merger and acquisition wages,http://arxiv.org/abs/1705.09529v1,"Fully Automatic Segmentation and Objective Assessment of Atrial Scars
  for Longstanding Persistent Atrial Fibrillation Patients Using Late
  Gadolinium-Enhanced MRI","Purpose: Atrial fibrillation (AF) is the most common cardiac arrhythmia and
is correlated with increased morbidity and mortality. It is associated with
atrial fibrosis, which may be assessed non-invasively using late
gadolinium-enhanced (LGE) magnetic resonance imaging (MRI) where scar tissue is
visualised as a region of signal enhancement. In this study, we proposed a
novel fully automatic pipeline to achieve an accurate and objective atrial
scarring segmentation and assessment of LGE MRI scans for the AF patients.
Methods: Our fully automatic pipeline uniquely combined: (1) a multi-atlas
based whole heart segmentation (MA-WHS) to determine the cardiac anatomy from
an MRI Roadmap acquisition which is then mapped to LGE MRI, and (2) a
super-pixel and supervised learning based approach to delineate the
distribution and extent of atrial scarring in LGE MRI. Results: Both our MA-WHS
and atrial scarring segmentation showed accurate delineations of cardiac
anatomy (mean Dice = 89%) and atrial scarring (mean Dice =79%) respectively
compared to the established ground truth from manual segmentation. Compared
with previously studied methods with manual interventions, our innovative
pipeline demonstrated comparable results, but was computed fully automatically.
Conclusion: The proposed segmentation methods allow LGE MRI to be used as an
objective assessment tool for localisation, visualisation and quantification of
atrial scarring.",
merger and acquisition wages,http://arxiv.org/abs/1905.12535v1,Ride-share matching algorithms generate income inequality,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",
merger and acquisition wages,http://arxiv.org/abs/1810.07781v2,"Responsible team players wanted: an analysis of soft skill requirements
  in job advertisements","During the past decades the importance of soft skills for labour market
outcomes has grown substantially. This carries implications for labour market
inequality, since previous research shows that soft skills are not valued
equally across race and gender. This work explores the role of soft skills in
job advertisements by drawing on methods from computational science as well as
on theoretical and empirical insights from economics, sociology and psychology.
We present a semi-automatic approach based on crowdsourcing and text mining for
extracting a list of soft skills. We find that soft skills are a crucial
component of job ads, especially of low-paid jobs and jobs in female-dominated
professions. Our work shows that soft skills can serve as partial predictors of
the gender composition in job categories and that not all soft skills receive
equal wage returns at the labour market. Especially ""female"" skills are
frequently associated with wage penalties. Our results expand the growing
literature on the association of soft skills on wage inequality and highlight
their importance for occupational gender segregation at labour markets.",
merger and acquisition wages,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition wages,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition wages,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition wages,http://arxiv.org/abs/1705.07643v5,Near-Feasible Stable Matchings with Budget Constraints,"We consider the matching with contracts framework of Hatfield and Milgrom
when one side (a firm or hospital) can make monetary transfers (offer wages) to
the other (a worker or doctor). In a standard model, monetary transfers are not
restricted. However, we assume that each hospital has a fixed budget; that is,
the total amount of wages allocated by each hospital to the doctors is
constrained. With this constraint, stable matchings may fail to exist and
checking for the existence is hard. To deal with the nonexistence, we focus on
near-feasible matchings that can exceed each hospital budget by a certain
amount, and We introduce a new concept of compatibility. We show that the
compatibility condition is a sufficient condition for the existence of a
near-feasible stable matching in the matching with contracts framework. Under a
slight restriction on hospitals' preferences, we provide mechanisms that
efficiently return a near-feasible stable matching with respect to the actual
amount of wages allocated by each hospital. By sacrificing strategy-proofness,
the best possible bound of budget excess is achieved.",
merger and acquisition wages,http://arxiv.org/abs/1601.05664v1,Defining urban agglomerations to detect agglomeration economies,"Agglomeration economies are a persistent subject of debate among economists
and urban planners. Their definition turns on whether or not larger cities and
regions are more efficient and more productive than smaller ones. We complement
existing discussion on agglomeration economies and the urban wage premium here
by providing a sensitivity analysis of estimated coefficients to different
delineations of urban agglomeration as well as to different definitions of the
economic measure that summarises the urban premium. This quantity can consist
of total wages measured at the place of work, or of income registered at the
place of residence. The chosen option influences the scaling behaviour of city
size as well as the spatial distribution of the phenomenon at the city level.
Spatial discrepancies between the distribution of jobs and the distribution of
households at different economic levels makes city definitions crucial to the
estimation of economic relations which vary with city size. We argue this point
by regressing measures of income and wage over about five thousands different
definitions of cities in France, based on our algorithmic aggregation of
administrative spatial units at regular cutoffs which reflect density,
population thresholds and commuting flows. We also go beyond aggregated
observations of wages and income by searching for evidence of larger
inequalities and economic segregation in the largest cities. This paper
therefore considers the spatial and economic complexity of cities with respect
to discussion about how we measure agglomeration economies. It provides a basis
for reflection on alternative ways to model the processes which lead to
observed variations, and this can provide insights for more comprehensive
regional planning.","Cottineau, C., Finance, O., Hatna, E., Arcaute, E., & Batty, M.
  (2018). Defining urban clusters to detect agglomeration economies.
  Environment and Planning B: Urban Analytics and City Science,
  2399808318755146"
merger and acquisition wages,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition wages,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition wages,http://arxiv.org/abs/1802.04680v1,Training and Inference with Integers in Deep Neural Networks,"Researches on deep neural networks with discrete parameters and their
deployment in embedded systems have been active and promising topics. Although
previous works have successfully reduced precision in inference, transferring
both training and inference processes to low-bitwidth integers has not been
demonstrated simultaneously. In this work, we develop a new method termed as
""WAGE"" to discretize both training and inference, where weights (W),
activations (A), gradients (G) and errors (E) among layers are shifted and
linearly constrained to low-bitwidth integers. To perform pure discrete
dataflow for fixed-point devices, we further replace batch normalization by a
constant scaling layer and simplify other components that are arduous for
integer implementation. Improved accuracies can be obtained on multiple
datasets, which indicates that WAGE somehow acts as a type of regularization.
Empirically, we demonstrate the potential to deploy training in hardware
systems such as integer-based deep learning accelerators and neuromorphic chips
with comparable accuracy and higher energy efficiency, which is crucial to
future AI applications in variable scenarios with transfer and continual
learning demands.",
merger and acquisition wages,http://arxiv.org/abs/1510.05189v2,Causal Falling Rule Lists,"A causal falling rule list (CFRL) is a sequence of if-then rules that
specifies heterogeneous treatment effects, where (i) the order of rules
determines the treatment effect subgroup a subject belongs to, and (ii) the
treatment effect decreases monotonically down the list. A given CFRL
parameterizes a hierarchical bayesian regression model in which the treatment
effects are incorporated as parameters, and assumed constant within
model-specific subgroups. We formulate the search for the CFRL best supported
by the data as a Bayesian model selection problem, where we perform a search
over the space of CFRL models, and approximate the evidence for a given CFRL
model using standard variational techniques. We apply CFRL to a census wage
dataset to identify subgroups of differing wage inequalities between men and
women.",
merger and acquisition wages,http://arxiv.org/abs/1404.6103v1,"An Approximate ""Law of One Price"" in Random Assignment Games","Assignment games represent a tractable yet versatile model of two-sided
markets with transfers. We study the likely properties of the core of randomly
generated assignment games. If the joint productivities of every firm and
worker are i.i.d bounded random variables, then with high probability all
workers are paid roughly equal wages, and all firms make similar profits. This
implies that core allocations vary significantly in balanced markets, but that
there is core convergence in even slightly unbalanced markets. For the
benchmark case of uniform distribution, we provide a tight bound for the
workers' share of the surplus under the firm-optimal core allocation. We
present simulation results suggesting that the phenomena analyzed appear even
in medium-sized markets. Finally, we briefly discuss the effects of unbounded
distributions and the ways in which they may affect wage dispersion.",
merger and acquisition wages,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition wages,http://arxiv.org/abs/1708.01876v1,The Countries' Relation Formation Problem: I and II,"This paper integrates the studies of various countries' behaviors, e.g.,
waging wars and entering into military alliances, into a general framework of
\emph{countries' relation formation}, which consists of two components, i.e., a
static game and a dynamical system. Aside from being a stand-alone framework
itself, this paper can also be seen as a necessary extension of a recently
developed \emph{countries' power allocation game} in \cite{allocation}. We
establish certain theoretical results, such as pure strategy Nash equilibrium
existence in the static game, and propose several applications of interest made
possible by combining both frameworks of countries' power allocation and
relation formation.",
merger and acquisition wages,http://arxiv.org/abs/1506.06008v1,"4πβ (LS)-γ (HPGe) Digital Coincidence System Based on
  Synchronous High-Speed Multichannel Data Acquisition","A dedicated 4{\pi}{\beta} (LS)-{\gamma} (HPGe)digital coincidence system has
been developed in this work, which includes five acquisition channels. Three
analog-to-digital converter (ADC) acquisition channels with an acquisition
resolution of 8 bits and acquisition rate of 1GSPS (sample per second) are
utilized to collect the signals from three Photo multiplier tubes (PMTs) which
are adopted to detect {\beta} decay, and two acquisition channels with an
acquisition resolution of 16 bits and acquisition rate of 50MSPS are utilized
to collect the signals from high-purity germanium (HPGe) which are adopted to
detect {\gamma} decay. In order to increase the accuracy of the coincidence
system, all the five acquisition channels are synchronous within 500ps. The
data collected by the five acquisition channels will be transmitted to the host
PC through PCI bus and saved as a file. Off-line software is applied for the
4{\pi}{\beta} (LS)-{\gamma} (HPGe) coincidence and data analysis as needed in
practical application. With all the above preconditions, the flexibility of the
system is increased, and the structure and application of the system are
simplified. According to the test, the highest coincidence rate of the system
is 20K per second, which is sufficient for most applications. This paper mainly
introduces the design of the hardware, the synchronization method and the test
result of this system.",
merger and acquisition wages,http://arxiv.org/abs/1010.5854v2,"An Alternative Approach to Data Acquisition Using Keyboard Emulation
  Technique","A number of data acquisition systems depend on human interface to access
computer for measuring, processing and analyzing data and to prepare it for
presentation and storage. Data acquisition software is installed on the
computer and all intended operations are performed manually. The data
acquisition software requires user intervention for operations like selection
of measurement setup, acquisition and storage of data to computer. The duty of
users becomes laborious if the data acquisition process lasts for a long
duration and requires continuous repetition of steps. An appropriate solution
to overcome such problem is to replace the physical operator with a virtual
user. This software generated simulated user sits at the data acquisition
process through out and automate all the intended steps of data acquisition.
This paper presents a new approach for data acquisition by using keyboard
emulation technique. A keyboard emulation software is developed which runs
beside the main data acquisition software and acts as a virtual user. All the
operations which require user interface are performed through fully automated
computer program. The developed software/system is executed in a real time
environment and the functionality of the software is verified. In the end,
potential application areas of the designed keyboard emulation software are
explored.",
merger and acquisition wages,http://arxiv.org/abs/physics/0505173v1,Empirical study and model of personal income,"Personal income distributions in Japan are analyzed empirically and a simple
stochastic model of the income process is proposed. Based on empirical facts,
we propose a minimal two-factor model. Our model of personal income consists of
an asset accumulation process and a wage process. We show that these simple
processes can successfully reproduce the empirical distribution of income. In
particular, the model can reproduce the particular transition of the
distribution shape from the middle part to the tail part. This model also
allows us to derive the tail exponent of the distribution analytically.",
merger and acquisition wages,http://arxiv.org/abs/physics/0510248v1,A Harris-Todaro Agent-Based Model to Rural-Urban Migration,"The Harris-Todaro model of the rural-urban migration process is revisited
under an agent-based approach. The migration of the workers is interpreted as a
process of social learning by imitation, formalized by a computational model.
By simulating this model, we observe a transitional dynamics with continuous
growth of the urban fraction of overall population toward an equilibrium. Such
an equilibrium is characterized by stabilization of rural-urban expected wages
differential (generalized Harris-Todaro equilibrium condition), urban
concentration and urban unemployment. These classic results obtained originally
by Harris and Todaro are emergent properties of our model.",
merger and acquisition wages,http://arxiv.org/abs/1803.06563v1,Viewpoint: Artificial Intelligence and Labour,"The welfare of modern societies has been intrinsically linked to wage labour.
With some exceptions, the modern human has to sell her labour-power to be able
reproduce biologically and socially. Thus, a lingering fear of technological
unemployment features predominately as a theme among Artificial Intelligence
researchers. In this short paper we show that, if past trends are anything to
go by, this fear is irrational. On the contrary, we argue that the main problem
humanity will be facing is the normalisation of extremely long working hours.",
merger and acquisition wages,http://arxiv.org/abs/1602.01578v2,Modeling the relation between income and commuting distance,"We discuss the distribution of commuting distances and its relation to
income. Using data from Denmark, the UK, and the US, we show that the commuting
distance is (i) broadly distributed with a slow decaying tail that can be
fitted by a power law with exponent $\gamma \approx 3$ and (ii) an average
growing slowly as a power law with an exponent less than one that depends on
the country considered. The classical theory for job search is based on the
idea that workers evaluate the wage of potential jobs as they arrive
sequentially through time, and extending this model with space, we obtain
predictions that are strongly contradicted by our empirical findings. We
propose an alternative model that is based on the idea that workers evaluate
potential jobs based on a quality aspect and that workers search for jobs
sequentially across space. We also assume that the density of potential jobs
depends on the skills of the worker and decreases with the wage. The predicted
distribution of commuting distances decays as $1/r^{3}$ and is independent of
the distribution of the quality of jobs. We find our alternative model to be in
agreement with our data. This type of approach opens new perspectives for the
modeling of mobility.",J. R. Soc. Interface 13:20160306 (2016)
merger and acquisition wages,http://arxiv.org/abs/1711.07359v2,Approximately Stable Matchings with Budget Constraints,"This paper considers two-sided matching with budget constraints where one
side (firm or hospital) can make monetary transfers (offer wages) to the other
(worker or doctor). In a standard model, while multiple doctors can be matched
to a single hospital, a hospital has a maximum quota: the number of doctors
assigned to a hospital cannot exceed a certain limit. In our model, a hospital
instead has a fixed budget: the total amount of wages allocated by each
hospital to doctors is constrained. With budget constraints, stable matchings
may fail to exist and checking for the existence is hard. To deal with the
nonexistence of stable matchings, we extend the ""matching with contracts"" model
of Hatfield and Milgrom, so that it handles approximately stable matchings
where each of the hospitals' utilities after deviation can increase by factor
up to a certain amount. We then propose two novel mechanisms that efficiently
return such a stable matching that exactly satisfies the budget constraints. In
particular, by sacrificing strategy-proofness, our first mechanism achieves the
best possible bound. Furthermore, we find a special case such that a simple
mechanism is strategy-proof for doctors, keeping the best possible bound of the
general case.",
merger and acquisition wages,http://arxiv.org/abs/1802.09669v1,"A Multi-Disciplinary Review of Knowledge Acquisition Methods: From Human
  to Autonomous Eliciting Agents","This paper offers a multi-disciplinary review of knowledge acquisition
methods in human activity systems. The review captures the degree of
involvement of various types of agencies in the knowledge acquisition process,
and proposes a classification with three categories of methods: the human
agent, the human-inspired agent, and the autonomous machine agent methods. In
the first two categories, the acquisition of knowledge is seen as a cognitive
task analysis exercise, while in the third category knowledge acquisition is
treated as an autonomous knowledge-discovery endeavour. The motivation for this
classification stems from the continuous change over time of the structure,
meaning and purpose of human activity systems, which are seen as the factor
that fuelled researchers' and practitioners' efforts in knowledge acquisition
for more than a century.
  We show through this review that the KA field is increasingly active due to
the higher and higher pace of change in human activity, and conclude by
discussing the emergence of a fourth category of knowledge acquisition methods,
which are based on red-teaming and co-evolution.","Knowledge-Based Systems, Volume 105, Elsevier, 2016"
merger and acquisition wages,http://arxiv.org/abs/1304.1116v1,"Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection","Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS.",
merger and acquisition wages,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
merger and acquisition wages,http://arxiv.org/abs/1910.08338v1,A Gigapixel Computational Light-Field Camera,"Light-field cameras allow the acquisition of both the spatial and angular
components of the light. This has a wide range of applications from image
refocusing to 3D reconstruction of a scene. The conventional way to perform
such acquisitions leads to a strong spatio-angular resolution limit. Here we
propose a computational version of the light-field camera. We perform a one
gigapixel photo-realistic diffraction limited light-field acquisition, that
would require the use of a one gigapixel sensor were the acquisition to be
performed with a conventional light-field camera. This result is mostly limited
by the total acquisition time, as our system could in principle allow
$\sim$Terapixel reconstructions to be achieved. The reported result presents
many potential advantages, such as the possibility to perform large depth of
field light-field acquisitions, realistic refocusing along a very wide range of
depths, very high dimensional super-resolved image acquisitions, and large
depth of field 3D reconstructions.",
merger and acquisition wages,http://arxiv.org/abs/1610.05462v2,"Towards the Leveraging of Data Deduplication to Break the Disk
  Acquisition Speed Limit","Digital forensic evidence acquisition speed is traditionally limited by two
main factors: the read speed of the storage device being investigated, i.e.,
the read speed of the disk, memory, remote storage, mobile device, etc.), and
the write speed of the system used for storing the acquired data. Digital
forensic investigators can somewhat mitigate the latter issue through the use
of high-speed storage options, such as networked RAID storage, in the
controlled environment of the forensic laboratory. However, traditionally,
little can be done to improve the acquisition speed past its physical read
speed from the target device itself. The protracted time taken for data
acquisition wastes digital forensic experts' time, contributes to digital
forensic investigation backlogs worldwide, and delays pertinent information
from potentially influencing the direction of an investigation. In a remote
acquisition scenario, a third contributing factor can also become a detriment
to the overall acquisition time - typically the Internet upload speed of the
acquisition system. This paper explores an alternative to the traditional
evidence acquisition model through the leveraging of a forensic data
deduplication system. The advantages that a deduplicated approach can provide
over the current digital forensic evidence acquisition process are outlined and
some preliminary results of a prototype implementation are discussed.",
merger and acquisition wages,http://arxiv.org/abs/1903.00269v1,"Covariance-Aided CSI Acquisition with Non-Orthogonal Pilots in Massive
  MIMO Systems","Massive multiple-input multiple-output (MIMO) systems use antenna arrays with
a large number of antenna elements to serve many different users
simultaneously. The large number of antennas in the system makes, however, the
channel state information (CSI) acquisition strategy design critical and
particularly challenging. Interestingly, in the context of massive MIMO
systems, channels exhibit a large degree of spatial correlation which can be
exploited to cope with the dimensionality problem in CSI acquisition. With the
final objective of analyzing the benefits of covariance-aided uplink multi-user
CSI acquisition in massive MIMO systems, here we compare the channel estimation
mean-square error (MSE) for (i) conventional CSI acquisition, which does not
assume any knowledge on the user spatial covariances and uses orthogonal pilot
sequences; and (ii) covariance-aided CSI acquisition, which exploits the
individual covariance matrices for channel estimation and possibly uses
non-orthogonal pilot sequences. We apply a large-system analysis to the latter
case, for which new asymptotic MSE expressions are established under various
assumptions on the distributions of the pilot sequences and on the covariance
matrices. We link these expressions to those describing the estimation MSE of
conventional CSI acquisition with orthogonal pilot sequences of some equivalent
length. This analysis provides insights about how much the CSI acquisition
process can be overloaded (in the sense of allowing estimating CSI with
sufficient accuracy for more users than the number resource elements allocated
for training) when a covariance-aided approach is adopted, hinting at
potentially significant gains in the spectral efficiency of CSI acquisition in
Massive MIMO.",
merger and acquisition wages,http://arxiv.org/abs/1403.4508v1,Research on Study Mechanical Vibrations with Data Acquisition Systems,"The paper presents a new study method of mechanic vibrations with the help of
the data acquisition systems. The study of vibrations with the help of data
acquisition systems allows the solving of some engineering problems connected
to the measurement of some parameters which are difficult to measure having in
view the improvement of the technical performances of the industrial equipment
or devices",
merger and acquisition market capitalisation,http://arxiv.org/abs/1803.03088v2,"Classification of cryptocurrency coins and tokens by the dynamics of
  their market capitalisations","We empirically verify that the market capitalisations of coins and tokens in
the cryptocurrency universe follow power-law distributions with significantly
different values, with the tail exponent falling between 0.5 and 0.7 for coins,
and between 1.0 and 1.3 for tokens. We provide a rationale for this, based on a
simple proportional growth with birth & death model previously employed to
describe the size distribution of firms, cities, webpages, etc. We empirically
validate the model and its main predictions, in terms of proportional growth
(Gibrat's law) of the coins and tokens. Estimating the main parameters of the
model, the theoretical predictions for the power-law exponents of coin and
token distributions are in remarkable agreement with the empirical estimations,
given the simplicity of the model. Our results clearly characterize coins as
being ""entrenched incumbents"" and tokens as an ""explosive immature ecosystem"",
largely due to massive and exuberant Initial Coin Offering activity in the
token space. The theory predicts that the exponent for tokens should converge
to 1 in the future, reflecting a more reasonable rate of new entrants
associated with genuine technological innovations.","Royal Society Open Science 5 (9), 2018"
merger and acquisition market capitalisation,http://arxiv.org/abs/1902.04517v2,"Wikipedia and Digital Currencies: Interplay Between Collective Attention
  and Market Performance","The production and consumption of information about Bitcoin and other
digital-, or 'crypto'-, currencies have grown together with their market
capitalisation. However, a systematic investigation of the relationship between
online attention and market dynamics, across multiple digital currencies, is
still lacking. Here, we quantify the interplay between the attention towards
digital currencies in Wikipedia and their market performance. We consider the
entire edit history of currency-related pages, and their view history from July
2015. First, we quantify the evolution of the cryptocurrency presence in
Wikipedia by analysing the editorial activity and the network of co-edited
pages. We find that a small community of tightly connected editors is
responsible for most of the production of information about cryptocurrencies in
Wikipedia. Then, we show that a simple trading strategy informed by Wikipedia
views performs better, in terms of returns on investment, than classic baseline
strategies for most of the covered period. Our results contribute to the recent
literature on the interplay between online information and investment markets,
and we anticipate it will be of interest for researchers as well as investors.",
merger and acquisition market capitalisation,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition market capitalisation,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1809.09805v4,"Towards Safer Smart Contracts: A Survey of Languages and Verification
  Methods","With a market capitalisation of over USD 205 billion in just under ten years,
public distributed ledgers have experienced significant adoption. Apart from
novel consensus mechanisms, their success is also accountable to smart
contracts. These programs allow distrusting parties to enter agreements that
are executed autonomously. However, implementation issues in smart contracts
caused severe losses to the users of such contracts. Significant efforts are
taken to improve their security by introducing new programming languages and
advance verification methods. We provide a survey of those efforts in two
parts. First, we introduce several smart contract languages focussing on
security features. To that end, we present an overview concerning paradigm,
type, instruction set, semantics, and metering. Second, we examine verification
tools and methods for smart contract and distributed ledgers. Accordingly, we
introduce their verification approach, level of automation, coverage, and
supported languages. Last, we present future research directions including
formal semantics, verified compilers, and automated verification.",
merger and acquisition market capitalisation,http://arxiv.org/abs/cs/0108014v1,"What's Fit To Print: The Effect Of Ownership Concentration On Product
  Variety In Daily Newspaper Markets","This paper examines the effect of ownership concentration on product
position, product variety and readership in markets for daily newspapers. US
antitrust policy presumes that mergers reduce the amount and diversity of
content available to consumers. However, the effects of consolidation in
differentiated product markets cannot be determined solely from theory. Because
multi-product firms internalize business stealing, mergers may encourage firms
to reposition products, leading to more, not less, variety. Using data on
reporter assignments from 1993-1999, results show that differentiation and
variety increase with concentration. Moreover, there is evidence that
additional variety increases readership, suggesting that concentration benefits
consumers.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1708.04866v5,Economic Factors of Vulnerability Trade and Exploitation,"Cybercrime markets support the development and diffusion of new attack
technologies, vulnerability exploits, and malware. Whereas the revenue streams
of cyber attackers have been studied multiple times in the literature, no
quantitative account currently exists on the economics of attack acquisition
and deployment. Yet, this understanding is critical to characterize the
production of (traded) exploits, the economy that drives it, and its effects on
the overall attack scenario. In this paper we provide an empirical
investigation of the economics of vulnerability exploitation, and the effects
of market factors on likelihood of exploit. Our data is collected
first-handedly from a prominent Russian cybercrime market where the trading of
the most active attack tools reported by the security industry happens. Our
findings reveal that exploits in the underground are priced similarly or above
vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle
of exploits is slower than currently often assumed. On the other hand,
cybercriminals are becoming faster at introducing selected vulnerabilities, and
the market is in clear expansion both in terms of players, traded exploits, and
exploit pricing. We then evaluate the effects of these market variables on
likelihood of attack realization, and find strong evidence of the correlation
between market activity and exploit deployment. We discuss implications on
vulnerability metrics, economics, and exploit measurement.","In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
  Communications Security (CCS '17). ACM, New York, NY, USA, 1483-1499"
merger and acquisition market capitalisation,http://arxiv.org/abs/1906.05998v2,"Non-zero-sum Stackelberg Budget Allocation Game for Computational
  Advertising","Computational advertising has been studied to design efficient marketing
strategies that maximize the number of acquired customers. In an increased
competitive market, however, a market leader (a leader) requires the
acquisition of new customers as well as the retention of her loyal customers
because there often exists a competitor (a follower) who tries to attract
customers away from the market leader. In this paper, we formalize a new model
called the Stackelberg budget allocation game with a bipartite influence model
by extending a budget allocation problem over a bipartite graph to a
Stackelberg game. To find a strong Stackelberg equilibrium, a standard solution
concept of the Stackelberg game, we propose two algorithms: an approximation
algorithm with provable guarantees and an efficient heuristic algorithm. In
addition, for a special case where customers are disjoint, we propose an exact
algorithm based on linear programming. Our experiments using real-world
datasets demonstrate that our algorithms outperform a baseline algorithm even
when the follower is a powerful competitor.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1603.07682v3,Descending Price Optimally Coordinates Search,"Investigating potential purchases is often a substantial investment under
uncertainty. Standard market designs, such as simultaneous or English auctions,
compound this with uncertainty about the price a bidder will have to pay in
order to win. As a result they tend to confuse the process of search both by
leading to wasteful information acquisition on goods that have already found a
good purchaser and by discouraging needed investigations of objects,
potentially eliminating all gains from trade. In contrast, we show that the
Dutch auction preserves all of its properties from a standard setting without
information costs because it guarantees, at the time of information
acquisition, a price at which the good can be purchased. Calibrations to
start-up acquisition and timber auctions suggest that in practice the social
losses through poor search coordination in standard formats are an order of
magnitude or two larger than the (negligible) inefficiencies arising from
ex-ante bidder asymmetries.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1602.01154v3,The value of Side Information in Secondary Spectrum Markets,"In a secondary spectrum market primaries set prices for their unused channels
to the secondaries. The payoff of a primary depends on the availability of
unused channels of its competitors. We consider a model were a primary can
acquire its competitor's channel state information (C-CSI) at a cost. We
formulate a game between two primaries where each primary decides whether to
acquire C-CSI or not and then selects its price based on that. We first
characterize the Nash Equilibrium (NE) of this game for a symmetric model where
the C-CSI is perfect. We show that the payoff of a primary is independent of
the C-CSI acquisition cost. We then generalize our analysis to allow for
imperfect estimation and cases where the two primaries have different C-CSI
costs or different channel availabilities. Our results show interestingly that
the payoff of a primary increases when there is estimation error. We also show
that surprisingly, the expected payoff of a primary may decrease when the C-CSI
acquisition cost decreases when primaries have different availabilities.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition market capitalisation,http://arxiv.org/abs/1911.04435v1,"A path-based many-to-many assignment game to model Mobility-as-a-Service
  market networks","As Mobility as a Service (MaaS) systems become increasingly popular, travel
is changing from unimodal trips to personalized services offered by a market of
mobility operators. Traditional traffic assignment models ignore the
interaction of different operators. However, a key characteristic of MaaS
markets is that urban trip decisions depend on both user route decisions as
well as operator service and pricing decisions. We adopt a new paradigm for
traffic assignment in a MaaS network of multiple operators using the concept of
stable matching to allocate costs and determine prices offered by operators
corresponding to user route choices and operator service choices without
resorting to nonconvex bilevel programming formulations. Unlike our prior work,
the proposed model allows travelers to make multimodal, multi-operator trips,
resulting in stable cost allocations between competing network operators to
provide MaaS for users. Algorithms are proposed to generate stability
conditions for the stable outcome pricing model. Extensive computational
experiments demonstrate the use of the model, and effectiveness of the proposed
algorithm, to handling pricing responses of MaaS operators in technological and
capacity changes, government acquisition, consolidation, and firm entry, using
the classic Sioux Falls network.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1604.04859v2,Online Truthful Mechanisms for Multi-sided Markets,"The study of mechanisms for multi-sided markets has received an increasingly
growing attention from the research community, and is motivated by the numerous
examples of such markets on the web and in electronic commerce. Many of these
examples represent dynamic and uncertain environments, and thus, require, in
fact, online mechanisms. Unfortunately, as far as we know, no previously
published online mechanism for a multi-sided market (or even for a double-sided
market) has managed to (approximately) maximize the gain from trade, while
guaranteeing desirable economic properties such as incentivizing truthfulness,
voluntary participation and avoiding budget deficit. In this work we present
the first online mechanism for a multi-sided market which has the above
properties. Our mechanism is designed for a market setting suggested by
[Feldman and Gonen (2016)]; which is motivated by the foreseeable future form
of online advertising.
  The online nature of our setting motivated us to define a stronger notion of
individual rationality, called ""continuous individual rationality"", capturing
the natural requirement that a player should never lose either by participating
in the mechanism or by not leaving prematurely. Satisfying the requirements of
continuous individual rationality, together with the other economic properties
our mechanism guarantees, requires the mechanism to use a novel pricing scheme
where users may be paid ongoing increments during the mechanism's execution up
to a pre-known maximum value. As users rarely ever get paid in reality, this
pricing scheme is new to mechanism design. Nevertheless, the principle it is
based on can be observed in many common real life scenarios such as executive
compensation payments and company acquisition deals. We believe both our new
dynamic pricing scheme concept and our strengthened notion of individual
rationality are of independent interest.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1402.1438v1,Deployment of an Innovative Resource Choice Method for Process Planning,"Designers, process planners and manufacturers naturally consider different
concepts for a same object. The stiffness of production means and the design
specification requirements mark out process planners as responsible of the
coherent integration of all constraints. First, this paper details an
innovative solution of resource choice, applied for aircraft manufacturing
parts. In a second part, key concepts are instanced for the considered
industrial domain. Finally, a digital mock up validates the solution viability
and demonstrates the possibility of an in-process knowledge capitalisation and
use. Formalising the link between Design and Manufacturing allows to hope
enhancements of simultaneous Product / Process developments.","CIRP Journal of Manufacturing Systems 35, 5 (2006) 487-506"
merger and acquisition market capitalisation,http://arxiv.org/abs/cs/0412038v1,"Tycoon: an Implementation of a Distributed, Market-based Resource
  Allocation System","Distributed clusters like the Grid and PlanetLab enable the same statistical
multiplexing efficiency gains for computing as the Internet provides for
networking. One major challenge is allocating resources in an economically
efficient and low-latency way. A common solution is proportional share, where
users each get resources in proportion to their pre-defined weight. However,
this does not allow users to differentiate the value of their jobs. This leads
to economic inefficiency. In contrast, systems that require reservations impose
a high latency (typically minutes to hours) to acquire resources.
  We present Tycoon, a market based distributed resource allocation system
based on proportional share. The key advantages of Tycoon are that it allows
users to differentiate the value of their jobs, its resource acquisition
latency is limited only by communication delays, and it imposes no manual
bidding overhead on users. We present experimental results using a prototype
implementation of our design.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1505.03305v1,Transformation of marketing in the e-commerce,"The article is about transformation of the theory and practice of marketing
in the conditions of e-commerce and network economy. The author considers
Internet-marketing as an independent kind of marketing in the virtual
communicative environment. The basic thesis of the article: the virtual
environment defines marketing transformation, changing methods, priorities and
structure at practice and then theories of marketing.",Practical marketing. 2013. No 1 (191). P. 4-16
merger and acquisition market capitalisation,http://arxiv.org/abs/1310.0548v1,Designing Markets for Daily Deals,"Daily deals platforms such as Amazon Local, Google Offers, GroupOn, and
LivingSocial have provided a new channel for merchants to directly market to
consumers. In order to maximize consumer acquisition and retention, these
platforms would like to offer deals that give good value to users. Currently,
selecting such deals is done manually; however, the large number of submarkets
and localities necessitates an automatic approach to selecting good deals and
determining merchant payments.
  We approach this challenge as a market design problem. We postulate that
merchants already have a good idea of the attractiveness of their deal to
consumers as well as the amount they are willing to pay to offer their deal.
The goal is to design an auction that maximizes a combination of the revenue of
the auctioneer (platform), welfare of the bidders (merchants), and the positive
externality on a third party (the consumer), despite the asymmetry of
information about this consumer benefit. We design auctions that truthfully
elicit this information from the merchants and maximize the social welfare
objective, and we characterize the consumer welfare functions for which this
objective is truthfully implementable. We generalize this characterization to a
very broad mechanism-design setting and give examples of other applications.",
merger and acquisition market capitalisation,http://arxiv.org/abs/physics/0509090v2,Effects of the globalization in the Korean financial markets,"We study the effect of globalization on the Korean market, one of the
emerging markets. Some characteristics of the Korean market are different from
those of the mature market according to the latest market data, and this is due
to the influence of foreign markets or investors. We concentrate on the market
network structures over the past two decades with knowledge of the history of
the market, and determine the globalization effect and market integration as a
function of time.","J. Korean Phys. Soc. 48, pp.S135-S138 (2006)."
merger and acquisition market capitalisation,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition market capitalisation,http://arxiv.org/abs/physics/0512210v1,"Micro-economic Analysis of the Physical Constrained Markets: Game Theory
  Application to Competitive Electricity Markets","Competition has been introduced in the electricity markets with the goal of
reducing prices and improving efficiency. The basic idea which stays behind
this choice is that, in competitive markets, a greater quantity of the good is
exchanged at a lower and a lower price, leading to higher market efficiency.
Electricity markets are pretty different from other commodities mainly due to
the physical constraints related to the network structure that may impact the
market performance. The network structure of the system on which the economic
transactions need to be undertaken poses strict physical and operational
constraints. Strategic interactions among producers that game the market with
the objective of maximizing their producer surplus must be taken into account
when modeling competitive electricity markets. The physical constraints,
specific of the electricity markets, provide additional opportunity of gaming
to the market players. Game theory provides a tool to model such a context.
This paper discussed the application of game theory to physical constrained
electricity markets with the goal of providing tools for assessing the market
performance and pinpointing the critical network constraints that may impact
the market efficiency. The basic models of game theory specifically designed to
represent the electricity markets will be presented. IEEE30 bus test system of
the constrained electricity market will be discussed to show the network
impacts on the market performances in presence of strategic bidding behavior of
the producers.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1505.03303v1,Dropshipping - alternative infrastructure of sales and promotion,"An article about the transformation of the theory and practice of marketing
in terms of e-commerce and network economy. The author considers Internet
Marketing as an independent marketing communication in a virtual environment.
The main thesis of the article: virtual environment determines the
transformation of marketing, changing methods, priorities and structure not
only practice, but also the theory of marketing.",Marketing in Russia and Abroad. 2012. No 1 (87). P. 90-104
merger and acquisition market capitalisation,http://arxiv.org/abs/1704.03524v1,Forensic Analysis of TomTom Navigation Application,"In the forensic field of digital technology, there has been a great deal of
investigation into the decoding of navigation systems of the brand TomTom. As
TomTom is the market leader in navigation systems, a large number of these
devices are investigated. These devices can hold an abundance of significant
location information. Currently, it is possible with the use of multiple
methods to make physical copies of mobile devices running Android. The next
great forensic problem is all the various programs that can be installed on
these devices. There is now an application available from the company TomTom in
the Google Play Store. This application mimics a navigation system on your
Android mobile device. Indeed, the TomTom application on Android can hold a
great deal of information. In this paper, we present a process of forensic
acquisition and analysis of the TomTom Android application. We focus on the
following questions: Is there a possibility to find previously driven routes or
GPS coordinates with timestamps in the memory of the mobile device? To
investigate what is stored in these files, driving tests were performed. During
these driving tests a copy was made of the most important file using a
self-written program. The significant files were found and the data in these
files was decoded. We show and analyse our results with Samsung mobile devices.
We compare also these results with forensic acquisition from TomTom GPS
devices.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1911.04794v2,"Emerging Natural User Interfaces in Mobile Computing: A Bottoms-Up
  Survey","Mobile and wearable interfaces and interaction paradigms are highly
constrained by the available screen real estate, and the computational and
power resources. Although there exist many ways of displaying information to
mobile users, inputting data to a mobile device is, usually, limited to a
conventional touch based interaction, that distracts users from their ongoing
activities. Furthermore, emerging applications, like augmented, mixed and
virtual reality (AR/MR/VR), require new types of input methods in order to
interact with complex virtual worlds, challenging the traditional techniques of
Human-Computer Interaction (HCI). Leveraging of Natural User Interfaces (NUIs),
as a paradigm of using natural intuitive actions to interact with computing
systems, is one of many ways to meet these challenges in mobile computing and
its modern applications. Brain-Machine Interfaces that enable thought-only
hands-free interaction, Myoelectric input methods that track body gestures and
gaze-tracking input interfaces - are the examples of NUIs applicable to mobile
and wearable interactions. The wide adoption of wearable devices and the
penetration of mobile technologies, alongside with the growing market of
AR/MR/VR, motivates the exploration and implementation of new interaction
paradigms. The concurrent development of bio-signal acquisition techniques and
accompanying ecosystems offers a useful toolbox to address open challenges. In
this survey, we present state-of-the-art bio-signal acquisition methods,
summarize and evaluate recent developments in the area of NUIs and outline
potential application in mobile scenarios. The survey will provide a bottoms-up
overview starting from (i) underlying biological aspects and signal acquisition
techniques, (ii) portable NUI hardware solutions, (iii) NUI-enabled
applications, as well as (iv) research challenges and open problems.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1206.5252v1,A Utility Framework for Bounded-Loss Market Makers,"We introduce a class of utility-based market makers that always accept orders
at their risk-neutral prices. We derive necessary and sufficient conditions for
such market makers to have bounded loss. We prove that hyperbolic absolute risk
aversion utility market makers are equivalent to weighted pseudospherical
scoring rule market makers. In particular, Hanson's logarithmic scoring rule
market maker corresponds to a negative exponential utility market maker in our
framework. We describe a third equivalent formulation based on maintaining a
cost function that seems most natural for implementation purposes, and we
illustrate how to translate among the three equivalent formulations. We examine
the tradeoff between the market's liquidity and the market maker's worst-case
loss. For a fixed bound on worst-case loss, some market makers exhibit greater
liquidity near uniform prices and some exhibit greater liquidity near extreme
prices, but no market maker can exhibit uniformly greater liquidity in all
regimes. For a fixed minimum liquidity level, we give the lower bound of market
maker's worst-case loss under some regularity conditions.",
merger and acquisition market capitalisation,http://arxiv.org/abs/physics/0411026v1,What is fair to ask Society to fund ?,"Our world is strongly driven by technological developments that continuously
create new markets and shape individual taste and choice. The scale of
dissemination of the most recent technological breakthroughs and the capability
of creating them is what divides wealthy societies from poor ones. The
increasing sophistication of these developments make it harder and harder for
developing countries to actively contribute to these breakthroughs as well as
to acquire the right to benefit from them through the exchange of products
obtained through low-tech agriculture, industry and services.
  Thus, as new technological developments rely often on discoveries in
fundamental science and technology that have their origins, in some instances,
many decades ago, it is only through fostering the acquisition of the skills
necessary for innovation in basic science and technology that economical and
social progress can be insured on a long term basis. In this perspective,
funding of research is an investment for the future and clearly, just buying
technology is no short cut to create sustainable wealth.
  Furthermore, Society should fund fundamental research for cultural and
training reasons. Indeed, as a representative of a country or of a culture in a
given historical period, activities assertive of cultural and national
identities should be funded. These involve necessarily scientific developments
too. Moreover, experience shows that the best training for innovation is
acquired from individuals and institutions that have an active interest in
basic science and on its application to technology.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1003.0034v1,A New Understanding of Prediction Markets Via No-Regret Learning,"We explore the striking mathematical connections that exist between market
scoring rules, cost function based prediction markets, and no-regret learning.
We show that any cost function based prediction market can be interpreted as an
algorithm for the commonly studied problem of learning from expert advice by
equating trades made in the market with losses observed by the learning
algorithm. If the loss of the market organizer is bounded, this bound can be
used to derive an O(sqrt(T)) regret bound for the corresponding learning
algorithm. We then show that the class of markets with convex cost functions
exactly corresponds to the class of Follow the Regularized Leader learning
algorithms, with the choice of a cost function in the market corresponding to
the choice of a regularizer in the learning problem. Finally, we show an
equivalence between market scoring rules and prediction markets with convex
cost functions. This implies that market scoring rules can also be interpreted
naturally as Follow the Regularized Leader algorithms, and may be of
independent interest. These connections provide new insight into how it is that
commonly studied markets, such as the Logarithmic Market Scoring Rule, can
aggregate opinions into accurate estimates of the likelihood of future events.",
merger and acquisition market capitalisation,http://arxiv.org/abs/cs/0109041v2,Open Access beyond cable: The case of Interactive TV,"In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented ""walled gardens"" offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone",
merger and acquisition market capitalisation,http://arxiv.org/abs/1905.08909v2,Equilibrium Characterization for Data Acquisition Games,"We study a game between two firms in which each provide a service based on
machine learning. The firms are presented with the opportunity to purchase a
new corpus of data, which will allow them to potentially improve the quality of
their products. The firms can decide whether or not they want to buy the data,
as well as which learning model to build with that data. We demonstrate a
reduction from this potentially complicated action space to a one-shot,
two-action game in which each firm only decides whether or not to buy the data.
The game admits several regimes which depend on the relative strength of the
two firms at the outset and the price at which the data is being offered. We
analyze the game's Nash equilibria in all parameter regimes and demonstrate
that, in expectation, the outcome of the game is that the initially stronger
firm's market position weakens whereas the initially weaker firm's market
position becomes stronger. Finally, we consider the perspective of the users of
the service and demonstrate that the expected outcome at equilibrium is not the
one which maximizes the welfare of the consumers.",
merger and acquisition market capitalisation,http://arxiv.org/abs/cs/0109048v1,"Competition and Commons: The Post-Telecom Act Public Interest, in and
  after the AOLTW Merger","In asserting a competitive market environment as a justification for
regulatory forbearance, the Telecommunications Act of 1996 finally articulated
a clear standard for the FCC's public interest standard, one of the most
protean concepts in communications. This seeming clarity has not, however,
inhibited intense political conflict over the term. This paper examines public
and regulatory debate over the AOL Time Warner merger as an example of the way
in which the linkage between competitions and commons policy becomes relevant
to communications policy, particularly in relation to mass media, and discusses
interpretations of the public interest in the current FCC. The paper proposes
that the Telecom Act's goal of fostering economic competition among information
service providers, and the democratic ideal of nurturing public relationships
and behaviors can be linked. Competition policy that creates the opportunity
for untrammeled interactivity also provides a sine qua non to nurture the
social phenomenon of the commons. The linked concepts of competition and
commons could also provide useful ways to interpret the public interest in
policy arenas as spectrum allocation and intellectual property.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1406.7584v1,"Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of
  Anchoring ""Bias""","In an online prediction market, forecasters who could not see the current
state of the market until they made their own separate estimates moved their
estimates closer to the market forecast when the current state of the market
became known. Their first edits to the market forecast were very similar to the
first edits of forecasters who could always see the current state of the
market, and forecasters in both conditions had similar accuracy. These results
suggest that our more elaborate forecast elicitation method might not improve
forecasts and that any anchoring on the state of the market does not constitute
an error in judgment.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1507.05762v1,"Horn Clauses as an Intermediate Representation for Program Analysis and
  Transformation","Many recent analyses for conventional imperative programs begin by
transforming programs into logic programs, capitalising on existing LP analyses
and simple LP semantics. We propose using logic programs as an intermediate
program representation throughout the compilation process. With restrictions
ensuring determinism and single-modedness, a logic program can easily be
transformed to machine language or other low-level language, while maintaining
the simple semantics that makes it suitable as a language for program analysis
and transformation. We present a simple LP language that enforces determinism
and single-modedness, and show that it makes a convenient program
representation for analysis and transformation.",Theory and Practice of Logic Programming 15 (2015) 526-542
merger and acquisition market capitalisation,http://arxiv.org/abs/1606.07138v3,"Airbnb in tourist cities: comparing spatial patterns of hotels and
  peer-to-peer accommodation","In recent years, what has become known as collaborative consumption has
undergone rapid expansion through peer-to-peer (P2P) platforms. In the field of
tourism, a particularly notable example is that of Airbnb. This article
analyses the spatial patterns of Airbnb in Barcelona and compares them with
hotels and sightseeing spots. New sources of data, such as Airbnb listings and
geolocated photographs are used. Analysis of bivariate spatial autocorrelation
reveals a close spatial relationship between Airbnb and hotels, with a marked
centre-periphery pattern, although Airbnb predominates around the main hotel
axis and hotels predominate in some peripheral areas of the city. Another
interesting finding is that Airbnb capitalises more on the advantages of
proximity to the main tourist attractions of the city than does the hotel
sector. Finally, it was possible to detect those parts of the city that have
seen the greatest increase in pressure from tourism related to Airbnb's recent
expansion.","Tourism Management 62:278-291, May 2017"
merger and acquisition market capitalisation,http://arxiv.org/abs/1905.00288v1,Beyond Mobile Apps: A Survey of Technologies for Mental Well-being,"Mental health problems are on the rise globally and strain national health
systems worldwide. Mental disorders are closely associated with fear of stigma,
structural barriers such as financial burden, and lack of available services
and resources which often prohibit the delivery of frequent clinical advice and
monitoring. Technologies for mental well-being exhibit a range of attractive
properties which facilitate the delivery of state of the art clinical
monitoring. This review article provides an overview of traditional techniques
followed by their technological alternatives, sensing devices, behaviour
changing tools, and feedback interfaces. The challenges presented by these
technologies are then discussed with data collection, privacy and battery life
being some of the key issues which need to be carefully considered for the
successful deployment of mental health tool-kits. Finally, the opportunities
this growing research area presents are discussed including the use of portable
tangible interfaces combining sensing and feedback technologies. Capitalising
on the captured data these ubiquitous devices offer, state of the art machine
learning algorithms can lead to the develop",
merger and acquisition market capitalisation,http://arxiv.org/abs/1506.06008v1,"4πβ (LS)-γ (HPGe) Digital Coincidence System Based on
  Synchronous High-Speed Multichannel Data Acquisition","A dedicated 4{\pi}{\beta} (LS)-{\gamma} (HPGe)digital coincidence system has
been developed in this work, which includes five acquisition channels. Three
analog-to-digital converter (ADC) acquisition channels with an acquisition
resolution of 8 bits and acquisition rate of 1GSPS (sample per second) are
utilized to collect the signals from three Photo multiplier tubes (PMTs) which
are adopted to detect {\beta} decay, and two acquisition channels with an
acquisition resolution of 16 bits and acquisition rate of 50MSPS are utilized
to collect the signals from high-purity germanium (HPGe) which are adopted to
detect {\gamma} decay. In order to increase the accuracy of the coincidence
system, all the five acquisition channels are synchronous within 500ps. The
data collected by the five acquisition channels will be transmitted to the host
PC through PCI bus and saved as a file. Off-line software is applied for the
4{\pi}{\beta} (LS)-{\gamma} (HPGe) coincidence and data analysis as needed in
practical application. With all the above preconditions, the flexibility of the
system is increased, and the structure and application of the system are
simplified. According to the test, the highest coincidence rate of the system
is 20K per second, which is sufficient for most applications. This paper mainly
introduces the design of the hardware, the synchronization method and the test
result of this system.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1601.03101v1,Magneto-Plasmonic Nanoantennas: Basics and Applications (Review),"Plasmonic nanoantennas is a hot and rapidly expanding research field. Here we
overview basic operating principles and applications of novel magneto-plasmonic
nanoantennas, which are made of ferromagnetic metals and driven not only by
light, but also by external magnetic fields. We demonstrate that
magneto-plasmonic nanoantennas enhance the magneto-optical effects, which
introduces additional degrees of freedom in the control of light at the
nano-scale. This property is used in conceptually new devices such as
magneto-plasmonic rulers, ultra-sensitive biosensors, one-way subwavelength
waveguides and extraordinary optical transmission structures, as well as in
novel biomedical imaging modalities. We also point out that in certain cases
'non-optical' ferromagnetic nanostructures may operate as magneto-plasmonic
nanoantennas. This undesigned extra functionality capitalises on established
optical characterisation techniques of magnetic nanomaterials and it may be
useful for the integration of nanophotonics and nanomagnetism on a single chip.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1407.0576v1,Novelty Search in Competitive Coevolution,"One of the main motivations for the use of competitive coevolution systems is
their ability to capitalise on arms races between competing species to evolve
increasingly sophisticated solutions. Such arms races can, however, be hard to
sustain, and it has been shown that the competing species often converge
prematurely to certain classes of behaviours. In this paper, we investigate if
and how novelty search, an evolutionary technique driven by behavioural
novelty, can overcome convergence in coevolution. We propose three methods for
applying novelty search to coevolutionary systems with two species: (i) score
both populations according to behavioural novelty; (ii) score one population
according to novelty, and the other according to fitness; and (iii) score both
populations with a combination of novelty and fitness. We evaluate the methods
in a predator-prey pursuit task. Our results show that novelty-based approaches
can evolve a significantly more diverse set of solutions, when compared to
traditional fitness-based coevolution.","Parallel Problem Solving from Nature (PPSN). vol. 8672 LNCS. pp.
  233-242. Springer (2014)"
merger and acquisition market capitalisation,http://arxiv.org/abs/1010.5854v2,"An Alternative Approach to Data Acquisition Using Keyboard Emulation
  Technique","A number of data acquisition systems depend on human interface to access
computer for measuring, processing and analyzing data and to prepare it for
presentation and storage. Data acquisition software is installed on the
computer and all intended operations are performed manually. The data
acquisition software requires user intervention for operations like selection
of measurement setup, acquisition and storage of data to computer. The duty of
users becomes laborious if the data acquisition process lasts for a long
duration and requires continuous repetition of steps. An appropriate solution
to overcome such problem is to replace the physical operator with a virtual
user. This software generated simulated user sits at the data acquisition
process through out and automate all the intended steps of data acquisition.
This paper presents a new approach for data acquisition by using keyboard
emulation technique. A keyboard emulation software is developed which runs
beside the main data acquisition software and acts as a virtual user. All the
operations which require user interface are performed through fully automated
computer program. The developed software/system is executed in a real time
environment and the functionality of the software is verified. In the end,
potential application areas of the designed keyboard emulation software are
explored.",
merger and acquisition market capitalisation,http://arxiv.org/abs/1201.1425v1,"Interconnection of Communities of Practice: A Web Platform for Knowledge
  Management","Our works aim at developing a Web platform to connect various Communities of
Practice (CoPs) and to capitalise on all their knowledge. This platform
addresses CoPs interested in a same general activity, for example tutoring. For
that purpose, we propose a general model of Interconnection of Communities of
Practice (ICP), based on the concept of Constellation of Practice (CCP)
developed by Wenger (1998). The model of ICP was implemented and has been used
to develop the TE-Cap 2 platform which has, as its field of application,
educational tutoring activities. In particular, we propose an indexation and
search tool for the ICP knowledge base. The TE-Cap 2 platform has been used in
real conditions. We present the main results of this descriptive investigation
to validate this work.","Firth International Conference on Knowledge Management and
  Information Sharing (KMIS 2009), Madeira : Portugal (2009)"
merger and acquisition bond yields,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition bond yields,http://arxiv.org/abs/1805.03308v1,"Investor Reaction to Financial Disclosures Across Topics: An Application
  of Latent Dirichlet Allocation","This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.",
merger and acquisition bond yields,http://arxiv.org/abs/1605.02559v1,"Robust imaging of hippocampal inner structure at 7T: in vivo acquisition
  protocol and methodological choices","OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in
vivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high
resolution imaging, such as can be achieved with 7T MRI. An acquisition
protocol was designed for imaging hippocampal inner structure at 7T. It relies
on a compromise between anatomical details visibility and robustness to motion.
In order to reduce acquisition time and motion artifacts, the full slab
covering the hippocampus was split into separate slabs with lower acquisition
time. A robust registration approach was implemented to combine the acquired
slabs within a final 3D-consistent high-resolution slab covering the whole
hippocampus. Evaluation was performed on 50 subjects overall, made of three
groups of subjects acquired using three acquisition settings; it focused on
three issues: visibility of hippocampal inner structure, robustness to motion
artifacts and registration procedure performance.RESULTS:Overall, T2-weighted
acquisitions with interleaved slabs proved robust. Multi-slab registration
yielded high quality datasets in 96 % of the subjects, thus compatible with
further analyses of hippocampal inner structure.CONCLUSION:Multi-slab
acquisition and registration setting is efficient for reducing acquisition time
and consequently motion artifacts for ultra-high resolution imaging of the
inner structure of the hippocampus.","Magnetic Resonance Materials in Physics, Biology and Medicine,
  Springer Verlag, 2016"
merger and acquisition bond yields,http://arxiv.org/abs/1405.6682v1,Optimality Theory as a Framework for Lexical Acquisition,"This paper re-investigates a lexical acquisition system initially developed
for French.We show that, interestingly, the architecture of the system
reproduces and implements the main components of Optimality Theory. However, we
formulate the hypothesis that some of its limitations are mainly due to a poor
representation of the constraints used. Finally, we show how a better
representation of the constraints used would yield better results.","15th International Conference on Intelligent Text Processing and
  Computational Linguistics, Nepal (2014)"
merger and acquisition bond yields,http://arxiv.org/abs/1906.09135v1,"Maximum entropy approaches for the study of triadic motifs in the
  Mergers & Acquisitions network","In the past years statistical physics has been successfully applied for
complex networks modelling. In particular, it has been shown that the maximum
entropy principle can be exploited in order to construct graph ensembles for
real-world networks which maximize the randomness of the graph structure
keeping fixed some topological constraint. Such ensembles can be used as null
models to detect statistically significant structural patterns and to
reconstruct the network structure in cases of incomplete information. Recently,
these randomizing methods have been used for the study of self-organizing
systems in economics and finance, such as interbank and world trade networks,
in order to detect topological changes and, possibly, early-warning signals for
the economical crisis. In this work we consider the configuration models with
different constraints for the network of mergers and acquisitions (M&As),
Comparing triadic and dyadic motifs, for both the binary and weighted M&A
network, with the randomized counterparts can shed light on its organization at
higher order level.",
merger and acquisition bond yields,http://arxiv.org/abs/1210.2934v1,"Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies","The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.","International Journal of Network Security & Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96"
merger and acquisition bond yields,http://arxiv.org/abs/1111.7176v1,Acquisition system for the CLIC Module,"The status of R&D activities for CLIC module acquisition are discussed [1].
LAPP is involved in the design of the local CLIC module acquisition crate,
described in the document Study of the CLIC Module Front-End Acquisition and
Evaluation Electronics [2]. This acquisition system is a project based on a
local crate, assigned to the CLIC module, including several mother boards.
These motherboards are foreseen to hold mezzanines dedicated to the different
subsystems. This system has to work in radiation environment. LAPP is involved
in the development of Drive Beam stripline position monitors read-out,
described in the document Drive Beam Stripline BPM Electronics and Acquisition
[3]. LAPP also develops a generic acquisition mezzanine that allows to perform
all-around acquisition and components tests for drive beam stripline BPM
read-out.",
merger and acquisition bond yields,http://arxiv.org/abs/1512.07712v1,"Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from
  Non-linear Measurements","This work addresses the problem of estimating proton density and T1 maps from
two partially sampled K-space scans such that the total acquisition time
remains approximately the same as a single scan. Existing multi parametric non
linear curve fitting techniques require a large number (8 or more) of echoes to
estimate the maps resulting in prolonged (clinically infeasible) acquisition
times. Our simulation results show that our method yields very accurate and
robust results from only two partially sampled scans (total scan time being the
same as a single echo MRI). We model PD and T1 maps to be sparse in some
transform domain. The PD map is recovered via standard Compressed Sensing based
recovery technique. Estimating the T1 map requires solving an analysis prior
sparse recovery problem from non linear measurements, since the relationship
between T1 values and intensity values or K space samples is not linear. For
the first time in this work, we propose an algorithm for analysis prior sparse
recovery for non linear measurements. We have compared our approach with the
only existing technique based on matrix factorization from non linear
measurements; our method yields considerably superior results.",
merger and acquisition bond yields,http://arxiv.org/abs/1602.02423v1,Catch bond mechanism in Dynein motor driven collective transport,"Recent experiments have demonstrated that dynein motor exhibits catch bonding
behaviour, in which the unbinding rate of a single dynein decreases with
increasing force, for a certain range of force. Motivated by these experiments,
we propose a model for catch bonding in dynein using a threshold force bond
deformation (TFBD) model wherein catch bonding sets in beyond a critical
applied load force. We study the effect of catch bonding on unidirectional
transport properties of cellular cargo carried by multiple dynein motors within
the framework of this model. We find catch bonding can result in dramatic
changes in the transport properties, which are in sharp contrast to kinesin
driven unidirectional transport, where catch bonding is absent. We predict
that, under certain conditions, the average velocity of the cellular cargo can
actually increase as applied load is increased. We characterize the transport
properties in terms of a velocity profile phase plot in the parameter space of
the catch bond strength and the stall force of the motor. This phase plot
yields predictions that may be experimentally accessed by suitable
modifications of motor transport and binding properties. Our work necessitates
a reexamination of existing theories of collective bidirectional transport of
cellular cargo where the catch bond effect of dynein described in this paper is
expected to play a crucial role.",
merger and acquisition bond yields,http://arxiv.org/abs/1808.02951v1,"Constructing a Non-additive Non-interacting Kinetic Energy Functional
  Approximation for Covalent Bonds from Exact Conditions","We present a non-decomposable approximation for the non-additive
non-interacting kinetic energy (NAKE) for covalent bonds based on the exact
behavior of the von Weizs\""{a}cker (vW) functional in regions dominated by one
orbital. This covalent approximation (CA) seamlessly combines the vW and the
Thomas-Fermi (TF) functional with a switching function of the fragment
densities constructed to satisfy exact constraints. It also makes use of
ensembles and fractionally-occupied spin-orbitals to yield highly accurate NAKE
for stretched bonds while outperforming other standard NAKE approximations near
equilibrium bond lengths. We tested the CA within Partition-Density Functional
Theory (P-DFT) and demonstrated its potential to enable fast and accurate P-DFT
calculations.",
merger and acquisition bond yields,http://arxiv.org/abs/cs/0406004v1,Application of Business Intelligence In Banks (Pakistan),"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).",
merger and acquisition bond yields,http://arxiv.org/abs/1703.04899v2,"Spatio-Temporal Patterns of the International Merger and Acquisition
  Network","This paper analyses the world web of mergers and acquisitions (M&As) using a
complex network approach. We use data of M&As to build a temporal sequence of
binary and weighted-directed networks for the period 1995-2010 and 224
countries (nodes) connected according to their M&As flows (links). We study
different geographical and temporal aspects of the international M&A network
(IMAN), building sequences of filtered sub-networks whose links belong to
specific intervals of distance or time. Given that M&As and trade are
complementary ways of reaching foreign markets, we perform our analysis using
statistics employed for the study of the international trade network (ITN),
highlighting the similarities and differences between the ITN and the IMAN. In
contrast to the ITN, the IMAN is a low density network characterized by a
persistent giant component with many external nodes and low reciprocity.
Clustering patterns are very heterogeneous and dynamic. High-income economies
are the main acquirers and are characterized by high connectivity, implying
that most countries are targets of a few acquirers. Like in the ITN,
geographical distance strongly impacts the structure of the IMAN: link-weights
and node degrees have a non-linear relation with distance, and an assortative
pattern is present at short distances.",
merger and acquisition bond yields,http://arxiv.org/abs/1903.06805v1,"Performance of ePix10K, a high dynamic range, gain auto-ranging pixel
  detector for FELs","ePix10K is a hybrid pixel detector developed at SLAC for demanding
free-electron laser (FEL) applications, providing an ultrahigh dynamic range
(245 eV to 88 MeV) through gain auto-ranging. It has three gain modes (high,
medium and low) and two auto-ranging modes (high-to-low and medium-to-low). The
first ePix10K cameras are built around modules consisting of a sensor flip-chip
bonded to 4 ASICs, resulting in 352x384 pixels of 100 $\mu$m x 100 $\mu$m each.
We present results from extensive testing of three ePix10K cameras with FEL
beams at LCLS, resulting in a measured noise floor of 245 eV rms, or 67 e$^-$
equivalent noise charge (ENC), and a range of 11000 photons at 8 keV. We
demonstrate the linearity of the response in various gain combinations: fixed
high, fixed medium, fixed low, auto-ranging high to low, and auto-ranging
medium-to-low, while maintaining a low noise (well within the counting
statistics), a very low cross-talk, perfect saturation response at fluxes up to
900 times the maximum range, and acquisition rates of up to 480 Hz. Finally, we
present examples of high dynamic range x-ray imaging spanning more than 4
orders of magnitude dynamic range (from a single photon to 11000
photons/pixel/pulse at 8 keV). Achieving this high performance with only one
auto-ranging switch leads to relatively simple calibration and reconstruction
procedures. The low noise levels allow usage with long integration times at
non-FEL sources. ePix10K cameras leverage the advantages of hybrid pixel
detectors with high production yield and good availability, minimize
development complexity through sharing the hardware, software and DAQ
development with all other versions of ePix cameras, while providing an upgrade
path to 5 kHz, 25 kHz and 100 kHz in three steps over the next few years,
matching the LCLS-II requirements.","=AIP Conf. Proc. 2054, 060062 (2019)"
merger and acquisition bond yields,http://arxiv.org/abs/1911.01148v1,"Validation and noise robustness assessment of microscopic anisotropy
  estimation with clinically feasible double diffusion encoding MRI","Purpose: Double diffusion encoding (DDE) MRI enables the estimation of
microscopic diffusion anisotropy, yielding valuable information on tissue
microstructure. A recent study proposed that the acquisition of rotationally
invariant DDE metrics, typically obtained using a spherical ""5-design"", could
be greatly simplified by assuming Gaussian diffusion, facilitating reduced
acquisition times that are more compatible with clinical settings. Here, we aim
to validate the new minimal acquisition scheme against the standard DDE
5-design, and to quantify the proposed method's noise robustness to facilitate
future clinical use. Methods: DDE MRI experiments were performed on both ex
vivo and in vivo rat brains at 9.4 T using the 5-design and the proposed
minimal design and taking into account the difference in the number of
acquisitions. The ensuing microscopic fractional anisotropy ({\mu}FA) maps were
compared over a range of b-values up to 5000 s/mm2. Noise robustness was
studied using analytical calculations and numerical simulations. Results: The
minimal protocol quantified {\mu}FA at an accuracy comparable to the estimates
obtained via the more theoretically robust DDE 5-design. {\mu}FA's sensitivity
to noise was found to strongly depend on compartment anisotropy and tensor
magnitude in a non-linear fashion. When {\mu}FA < 0.75 or when mean diffusivity
is particularly low, very high signal to noise ratio (SNR) is required for
precise quantification of {\mu}FA. Conclusion: Our work supports using DDE for
quantifying microscopic diffusion anisotropy in clinical settings but raises
hitherto overlooked precision issues when measuring {\mu}FA with DDE and
typical clinical SNR.","Magn Reson Med. 2019, 00: 1-13"
merger and acquisition bond yields,http://arxiv.org/abs/1407.1998v1,"Wave-function and density functional theory studies of dihydrogen
  complexes","We performed a benchmark study on a series of dihydrogen bond complexes and
constructed a set of reference bond distances and interaction energies. The
test set was employed to assess the performance of several wave-function
correlated and density functional theory methods. We found that second-order
correlation methods describe relatively well the dihydrogen complexes. However,
for high accuracy inclusion of triple contributions is important. On the other
hand, none of the considered density functional methods can simultaneously
yield accurate bond lengths and interaction energies. However, we found that
improved results can be obtained by the inclusion of non-local exchange
contributions.","J. Chem. Theory Comput. 10, 3151 (2014)"
merger and acquisition bond yields,http://arxiv.org/abs/0911.5568v1,"Acquisition d'informations lexicales à partir de corpus Cédric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition.","Troisi\`eme colloque international de l'Association Fran\c{c}aise
  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)"
merger and acquisition bond yields,http://arxiv.org/abs/physics/0507098v1,Ab initio yield curve dynamics,"We derive an equation of motion for interest-rate yield curves by applying a
minimum Fisher information variational approach to the implied probability
density. By construction, solutions to the equation of motion recover observed
bond prices. More significantly, the form of the resulting equation explains
the success of the Nelson Siegel approach to fitting static yield curves and
the empirically observed modal structure of yield curves. A practical numerical
implementation of this equation of motion is found by using the Karhunen-Loeve
expansion and Galerkin's method to formulate a reduced-order model of yield
curve dynamics.",
merger and acquisition bond yields,http://arxiv.org/abs/1607.05249v6,"Performance of a Nonempirical Density Functional on Molecules and
  Hydrogen-Bonded Complexes","Recently, Tao and Mo (TM) derived a meta-generalized gradient approximation
functional based on a model exchange-correlation hole. In this work, the
performance of this functional is assessed on standard test sets, using the
6-311++G(3df,3pd) basis set. These test sets include 223 G3/99 enthalpies of
formation, 99 atomization energies, 76 barrier heights, 58 electron affinities,
8 proton affinities, 96 bond lengths, 82 harmonic vibrational frequencies, 10
hydrogen-bonded molecular complexes, and 22 atomic excitation energies. Our
calculations show that the TM functional can achieve high accuracy for most
properties considered, relative to the LSDA, PBE, and TPSS functionals. In
particular, it yields the best accuracy for proton affinities, harmonic
vibrational frequencies, hydrogen-bonded dissociation energies and bond
lengths, and atomic excitation energies.",
merger and acquisition bond yields,http://arxiv.org/abs/1311.2467v2,Funding the Search for Extraterrestrial Intelligence with a Lottery Bond,"I propose the establishment of a SETI Lottery Bond to provide a continued
source of funding for the search for extraterrestrial intelligence (SETI). The
SETI Lottery Bond is a fixed rate perpetual bond with a lottery at maturity,
where maturity occurs only upon discovery and confirmation of extraterrestrial
intelligent life. Investors in the SETI Lottery Bond purchase shares that yield
a fixed rate of interest that continues indefinitely until SETI succeeds---at
which point a random subset of shares will be awarded a prize from a lottery
pool. SETI Lottery Bond shares also are transferable, so that investors can
benefact their shares to kin or trade them in secondary markets. The total
capital raised this way will provide a fund to be managed by a financial
institution, with annual payments from this fund to support SETI research, pay
investor interest, and contribute to the lottery fund. Such a plan could
generate several to tens of millions of dollars for SETI research each year,
which would help to revitalize and expand facilities such as the Allen
Telescope Array. The SETI Lottery Bond is a savings product that only can be
offered by a financial institution with authorization to engage in banking and
gaming activities. I therefore suggest that one or more banks offer a
lottery-linked savings product in support of SETI research, with the added
benefit of promoting personal savings and intergenerational wealth building
among individuals.",
merger and acquisition bond yields,http://arxiv.org/abs/0903.3502v1,"QM/MM Simulation of the Hydrogen Bond Dynamics of an Adenine:Uracil Base
  Pair in Solution. Geometric Correlations and Infrared Spectrum","Hybrid QM(DFT)/MM molecular dynamics simulations have been carried out for
the Watson-Crick base pair of 9-ethyl-8-phenyladenine and 1-cyclohexyluracil in
deuterochloroform solution at room temperature.
  Trajectories are analyzed putting special attention to the geometric
correlations of the $\NHN$ and $\NHO$ hydrogen bonds in the base pair. Further,
based on empirical correlations between the hydrogen bond bond length and the
fundamental NH stretching frequency its fluctuations are obtained along the
trajectory. Using the time dependent frequencies the infrared lineshape is
determined assuming the validity of a second order cumulant expansion. The
deviations for the fundamental transition frequencies are calculated to amount
to less than 2% as compared with experiment. The width of the spectrum for the
$\NHN$ bond is in reasonable agreement with experiment while that for the
$\NHO$ case is underestimated by the present model. Comparing the performance
of different pseudopotentials it is found that the Troullier-Martins
pseudopotential with a 70 Ry cut-off yields the best agreement.",
merger and acquisition transaction costs,http://arxiv.org/abs/1811.01249v1,Dynamic Feature Acquisition Using Denoising Autoencoders,"In real-world scenarios, different features have different acquisition costs
at test-time which necessitates cost-aware methods to optimize the cost and
performance trade-off. This paper introduces a novel and scalable approach for
cost-aware feature acquisition at test-time. The method incrementally asks for
features based on the available context that are known feature values. The
proposed method is based on sensitivity analysis in neural networks and density
estimation using denoising autoencoders with binary representation layers. In
the proposed architecture, a denoising autoencoder is used to handle unknown
features (i.e., features that are yet to be acquired), and the sensitivity of
predictions with respect to each unknown feature is used as a context-dependent
measure of informativeness. We evaluated the proposed method on eight different
real-world datasets as well as one synthesized dataset and compared its
performance with several other approaches in the literature. According to the
results, the suggested method is capable of efficiently acquiring features at
test-time in a cost- and context-aware fashion.","IEEE Transactions on Neural Networks and Learning Systems, 2018"
merger and acquisition transaction costs,http://arxiv.org/abs/1401.6455v1,"Motivating Smartphone Collaboration in Data Acquisition and Distributed
  Computing","This paper analyzes and compares different incentive mechanisms for a master
to motivate the collaboration of smartphone users on both data acquisition and
distributed computing applications. To collect massive sensitive data from
users, we propose a reward-based collaboration mechanism, where the master
announces a total reward to be shared among collaborators, and the
collaboration is successful if there are enough users wanting to collaborate.
We show that if the master knows the users' collaboration costs, then he can
choose to involve only users with the lowest costs. However, without knowing
users' private information, then he needs to offer a larger total reward to
attract enough collaborators. Users will benefit from knowing their costs
before the data acquisition. Perhaps surprisingly, the master may benefit as
the variance of users' cost distribution increases.
  To utilize smartphones' computation resources to solve complex computing
problems, we study how the master can design an optimal contract by specifying
different task-reward combinations for different user types. Under complete
information, we show that the master involves a user type as long as the
master's preference characteristic outweighs that type's unit cost. All
collaborators achieve a zero payoff in this case. If the master does not know
users' private cost information, however, he will conservatively target at a
smaller group of users with small costs, and has to give most benefits to the
collaborators.",
merger and acquisition transaction costs,http://arxiv.org/abs/1209.3804v2,Compressive Link Acquisition in Multiuser Communications,"An important receiver operation is to detect the presence specific preamble
signals with unknown delays in the presence of scattering, Doppler effects and
carrier offsets. This task, referred to as ""link acquisition"", is typically a
sequential search over the transmitted signal space. Recently, many authors
have suggested applying sparse recovery algorithms in the context of similar
estimation or detection problems. These works typically focus on the benefits
of sparse recovery, but not generally on the cost brought by compressive
sensing. Thus, our goal is to examine the trade-off in complexity and
performance that is possible when using sparse recovery. To do so, we propose a
sequential sparsity-aware compressive sampling (C-SA) acquisition scheme, where
a compressive multi-channel sampling (CMS) front-end is followed by a sparsity
regularized likelihood ratio test (SR-LRT) module.
  The proposed C-SA acquisition scheme borrows insights from the models studied
in the context of sub-Nyquist sampling, where a minimal amount of samples is
captured to reconstruct signals with Finite Rate of Innovation (FRI). In
particular, we propose an A/D conversion front-end that maximizes a well-known
probability divergence measure, the average Kullback-Leibler distance, of all
the hypotheses of the SR-LRT performed on the samples. We compare the proposed
acquisition scheme vis-\`{a}-vis conventional alternatives with relatively low
computational cost, such as the Matched Filter (MF), in terms of performance
and complexity.",
merger and acquisition transaction costs,http://arxiv.org/abs/1802.06656v1,"Cost-efficient QoS-Aware Data Acquisition Point Placement for Advanced
  Metering Infrastructure","In an advanced metering infrastructure (AMI), data acquisition points (DAPs)
are responsible for collecting traffic from several smart meters and automated
devices and transmitting them to the utility control center. Although the
problem of optimized data collector placement has already been addressed for
wireless broadband and sensor networks, DAP placement is quite a new research
area for AMIs. In this paper, we investigate the minimum required number of
DAPs and their optimized locations on top of the existing utility poles in a
distribution grid such that smart grid quality of service requirements can best
be provided. In order to solve the problem for large-scale AMIs, we devise a
novel heuristic algorithm using a greedy approach for identifying potential
pole locations for DAP placement and the Dijkstra shortest path algorithm for
constructing reliable routes. We employ the characteristics of medium access
schemes from the IEEE 802.15.4g smart utility network (SUN) standard, and
consider mission-critical and non-critical smart grid traffic. The performance
and time-complexity of our algorithm are compared with those obtained by the
IBM CPLEX software for small scenarios. Finally, we apply our devised DAP
placement algorithm to examples of realistic smart grid AMI topologies.","IEEE Transactions on Communications (Early Access), Volume 66,
  Issue 12, 2018"
merger and acquisition transaction costs,http://arxiv.org/abs/1903.00269v1,"Covariance-Aided CSI Acquisition with Non-Orthogonal Pilots in Massive
  MIMO Systems","Massive multiple-input multiple-output (MIMO) systems use antenna arrays with
a large number of antenna elements to serve many different users
simultaneously. The large number of antennas in the system makes, however, the
channel state information (CSI) acquisition strategy design critical and
particularly challenging. Interestingly, in the context of massive MIMO
systems, channels exhibit a large degree of spatial correlation which can be
exploited to cope with the dimensionality problem in CSI acquisition. With the
final objective of analyzing the benefits of covariance-aided uplink multi-user
CSI acquisition in massive MIMO systems, here we compare the channel estimation
mean-square error (MSE) for (i) conventional CSI acquisition, which does not
assume any knowledge on the user spatial covariances and uses orthogonal pilot
sequences; and (ii) covariance-aided CSI acquisition, which exploits the
individual covariance matrices for channel estimation and possibly uses
non-orthogonal pilot sequences. We apply a large-system analysis to the latter
case, for which new asymptotic MSE expressions are established under various
assumptions on the distributions of the pilot sequences and on the covariance
matrices. We link these expressions to those describing the estimation MSE of
conventional CSI acquisition with orthogonal pilot sequences of some equivalent
length. This analysis provides insights about how much the CSI acquisition
process can be overloaded (in the sense of allowing estimating CSI with
sufficient accuracy for more users than the number resource elements allocated
for training) when a covariance-aided approach is adopted, hinting at
potentially significant gains in the spectral efficiency of CSI acquisition in
Massive MIMO.",
merger and acquisition transaction costs,http://arxiv.org/abs/1709.05964v1,"Why Pay More When You Can Pay Less: A Joint Learning Framework for
  Active Feature Acquisition and Classification","We consider the problem of active feature acquisition, where we sequentially
select the subset of features in order to achieve the maximum prediction
performance in the most cost-effective way. In this work, we formulate this
active feature acquisition problem as a reinforcement learning problem, and
provide a novel framework for jointly learning both the RL agent and the
classifier (environment). We also introduce a more systematic way of encoding
subsets of features that can properly handle innate challenge with missing
entries in active feature acquisition problems, that uses the orderless
LSTM-based set encoding mechanism that readily fits in the joint learning
framework. We evaluate our model on a carefully designed synthetic dataset for
the active feature acquisition as well as several real datasets such as
electric health record (EHR) datasets, on which it outperforms all baselines in
terms of prediction performance as well feature acquisition cost.",
merger and acquisition transaction costs,http://arxiv.org/abs/1702.08478v1,"Risks and Transaction Costs of Distributed-Ledger Fintech: Boundary
  Effects and Consequences","Fintech business models based on distributed ledgers -- and their
smart-contract variants in particular -- offer the prospect of democratizing
access to faster, anywhere-accessible, lower cost, reliable-and-secure
high-quality financial services. In addition to holding great, economically
transformative promise, these business models pose new, little-studied risks
and transaction costs. However, these risks and transaction costs are not
evident during the demonstration and testing phases of development, when
adopters and users are drawn from the community of developers themselves, as
well as from among non-programmer fintech evangelists. Hence, when the new
risks and transaction costs become manifest -- as the fintech business models
are rolled out across the wider economy -- the consequences may also appear to
be new and surprising. The present study represents an effort to get ahead of
these developments by delineating risks and transaction costs inherent in
distributed-ledger- and smart-contracts-based fintech business models. The
analysis focuses on code risk and moral-hazard risk, as well as on
mixed-economy risks and the unintended consequences of replicating
bricks-and-mortar-generation contract forms within the ultra-low
transaction-cost environment of fintech.",
merger and acquisition transaction costs,http://arxiv.org/abs/1807.03662v1,"TrialChain: A Blockchain-Based Platform to Validate Data Integrity in
  Large, Biomedical Research Studies","The governance of data used for biomedical research and clinical trials is an
important requirement for generating accurate results. To improve the
visibility of data quality and analysis, we developed TrialChain, a
blockchain-based platform that can be used to validate data integrity from
large, biomedical research studies. We implemented a private blockchain using
the MultiChain platform and integrated it with a data science platform deployed
within a large research center. An administrative web application was built
with Python to manage the platform, which was built with a microservice
architecture using Docker. The TrialChain platform was integrated during data
acquisition into our existing data science platform. Using NiFi, data were
hashed and logged within the local blockchain infrastructure. To provide public
validation, the local blockchain state was periodically synchronized to the
public Ethereum network. The use of a combined private/public blockchain
platform allows for both public validation of results while maintaining
additional security and lower cost for blockchain transactions. Original data
and modifications due to downstream analysis can be logged within TrialChain
and data assets or results can be rapidly validated when needed using API calls
to the platform. The TrialChain platform provides a data governance solution to
audit the acquisition and analysis of biomedical research data. The platform
provides cryptographic assurance of data authenticity and can also be used to
document data analysis.",
merger and acquisition transaction costs,http://arxiv.org/abs/1409.6771v1,Mitigation of Delayed Management Costs in Transaction-Oriented Systems,"Abundant examples of complex transaction-oriented networks (TONs) can be
found in a variety of disciplines, including information and communication
technology, finances, commodity trading, and real estate. A transaction in a
TON is executed as a sequence of subtransactions associated with the network
nodes, and is committed if every subtransaction is committed. A subtransaction
incurs a two-fold overhead on the host node: the fixed transient operational
cost and the cost of long-term management (e.g. archiving and support) that
potentially grows exponentially with the transaction length. If the overall
cost exceeds the node capacity, the node fails and all subtransaction incident
to the node, and their parent distributed transactions, are aborted. A TON
resilience can be measured in terms of either external workloads or intrinsic
node fault rates that cause the TON to partially or fully choke. We demonstrate
that under certain conditions, these two measures are equivalent. We further
show that the exponential growth of the long-term management costs can be
mitigated by adjusting the effective operational cost: in other words, that the
future maintenance costs could be absorbed into the transient operational
costs.",
merger and acquisition transaction costs,http://arxiv.org/abs/1903.11331v2,Active Multi-Information Source Bayesian Quadrature,"Bayesian quadrature (BQ) is a sample-efficient probabilistic numerical method
to solve integrals of expensive-to-evaluate black-box functions, yet so
far,active BQ learning schemes focus merely on the integrand itself as
information source, and do not allow for information transfer from cheaper,
related functions. Here, we set the scene for active learning in BQ when
multiple related information sources of variable cost (in input and source) are
accessible. This setting arises for example when evaluating the integrand
requires a complex simulation to be run that can be approximated by simulating
at lower levels of sophistication and at lesser expense. We construct
meaningful cost-sensitive multi-source acquisition rates as an extension to
common utility functions from vanilla BQ (VBQ),and discuss pitfalls that arise
from blindly generalizing. Furthermore, we show that the VBQ acquisition policy
is a corner-case of all considered cost-sensitive acquisition schemes, which
collapse onto one single de-generate policy in the case of one source and
constant cost. In proof-of-concept experiments we scrutinize the behavior of
our generalized acquisition functions. On an epidemiological model, we
demonstrate that active multi-source BQ (AMS-BQ) allocates budget more
efficiently than VBQ for learning the integral to a good accuracy.",
merger and acquisition transaction costs,http://arxiv.org/abs/1401.3881v1,"Value of Information Lattice: Exploiting Probabilistic Independence for
  Effective Feature Subset Acquisition","We address the cost-sensitive feature acquisition problem, where
misclassifying an instance is costly but the expected misclassification cost
can be reduced by acquiring the values of the missing features. Because
acquiring the features is costly as well, the objective is to acquire the right
set of features so that the sum of the feature acquisition cost and
misclassification cost is minimized. We describe the Value of Information
Lattice (VOILA), an optimal and efficient feature subset acquisition framework.
Unlike the common practice, which is to acquire features greedily, VOILA can
reason with subsets of features. VOILA efficiently searches the space of
possible feature subsets by discovering and exploiting conditional independence
properties between the features and it reuses probabilistic inference
computations to further speed up the process. Through empirical evaluation on
five medical datasets, we show that the greedy strategy is often reluctant to
acquire features, as it cannot forecast the benefit of acquiring multiple
features in combination.","Journal Of Artificial Intelligence Research, Volume 41, pages
  69-95, 2011"
merger and acquisition transaction costs,http://arxiv.org/abs/1308.2309v1,"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms.",
merger and acquisition transaction costs,http://arxiv.org/abs/1308.2147v1,"Exploiting Locality in Lease-Based Replicated Transactional Memory via
  Task Migration","We present Lilac-TM, the first locality-aware Distributed Software
Transactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized
lease-based replicated DSTM. It employs a novel self- optimizing lease
circulation scheme based on the idea of dynamically determining whether to
migrate transactions to the nodes that own the leases required for their
validation, or to demand the acquisition of these leases by the node that
originated the transaction. Our experimental evaluation establishes that
Lilac-TM provides significant performance gains for distributed workloads
exhibiting data locality, while typically incurring no overhead for non-data
local workloads.",
merger and acquisition transaction costs,http://arxiv.org/abs/1706.05166v1,"Interference-Alignment and Soft-Space-Reuse Based Cooperative
  Transmission for Multi-cell Massive MIMO Networks","As a revolutionary wireless transmission strategy, interference alignment
(IA) can improve the capacity of the cell-edge users. However, the acquisition
of the global channel state information (CSI) for IA leads to unacceptable
overhead in the massive MIMO systems. To tackle this problem, in this paper, we
propose an IA and soft-space-reuse (IA-SSR) based cooperative transmission
scheme under the two-stage precoding framework. Specifically, the cell-center
and the cell-edge users are separately treated to fully exploit the spatial
degrees of freedoms (DoF). Then, the optimal power allocation policy is
developed to maximize the sum-capacity of the network. Next, a low-cost channel
estimator is designed for the proposed IA-SSR framework. Some practical issues
in IA-SSR implementation are also discussed. Finally, plenty of numerical
results are presented to show the efficiency of the proposed algorithm.",
merger and acquisition transaction costs,http://arxiv.org/abs/1806.03045v1,"System Level Framework for Assessing the Accuracy of Neonatal EEG
  Acquisition","Significant research has been conducted in recent years to design low-cost
alternatives to the current EEG monitoring systems used in healthcare
facilities. Testing such systems on a vulnerable population such as newborns is
complicated due to ethical and regulatory considerations that slow down the
technical development. This paper presents and validates a method for
quantifying the accuracy of neonatal EEG acquisition systems and electrode
technologies via clinical data simulations that do not require neonatal
participants. The proposed method uses an extensive neonatal EEG database to
simulate analogue signals, which are subsequently passed through electrical
models of the skin-electrode interface, which are developed using wet and dry
EEG electrode designs. The signal losses in the system are quantified at each
stage of the acquisition process for electrode and acquisition board losses.
SNR, correlation and noise values were calculated. The results verify that
low-cost EEG acquisition systems are capable of obtaining clinical grade EEG.
Although dry electrodes result in a significant increase in the skin-electrode
impedance, accurate EEG recordings are still achievable.",
merger and acquisition transaction costs,http://arxiv.org/abs/1604.00103v3,Effect of Bitcoin fee on transaction-confirmation process,"In Bitcoin system, transactions are prioritized according to transaction
fees. Transactions without fees are given low priority and likely to wait for
confirmation. Because the demand of micro payment in Bitcoin is expected to
increase due to low remittance cost, it is important to quantitatively
investigate how transactions with small fees of Bitcoin affect the
transaction-confirmation time. In this paper, we analyze the
transaction-confirmation time by queueing theory. We model the
transaction-confirmation process of Bitcoin as a priority queueing system with
batch service, deriving the mean transaction-confirmation time. Numerical
examples show how the demand of transactions with low fees affects the
transaction-confirmation time. We also consider the effect of the maximum block
size on the transaction-confirmation time.",
merger and acquisition transaction costs,http://arxiv.org/abs/1905.00553v1,Empirically Analyzing Ethereum's Gas Mechanism,"Ethereum's Gas mechanism attempts to set transaction fees in accordance with
the computational cost of transaction execution: a cost borne by default by
every node on the network to ensure correct smart contract execution. Gas
encourages users to author transactions that are efficient to execute and in so
doing encourages node diversity, allowing modestly resourced nodes to join and
contribute to the security of the network.
  However, the effectiveness of this scheme relies on Gas costs being correctly
aligned with observed computational costs in reality. In this work, we
performed the first large scale empirical study to understand to what degree
this alignment exists in practice, by collecting and analyzing Tera-bytes worth
of nanosecond-precision transaction execution traces. Besides confirming
potential denial-of-service vectors, our results also shed light on the role of
I/O in transaction costs which remains poorly captured by the current Gas cost
model. Finally, our results suggest that under the current Gas cost model,
nodes with modest computational resources are disadvantaged compared to their
better resourced peers, which we identify as an ongoing threat to node
diversity and network decentralization.",
merger and acquisition transaction costs,http://arxiv.org/abs/0804.1724v1,"Information Acquisition and Exploitation in Multichannel Wireless
  Networks","A wireless system with multiple channels is considered, where each channel
has several transmission states. A user learns about the instantaneous state of
an available channel by transmitting a control packet in it. Since probing all
channels consumes significant energy and time, a user needs to determine what
and how much information it needs to acquire about the instantaneous states of
the available channels so that it can maximize its transmission rate. This
motivates the study of the trade-off between the cost of information
acquisition and its value towards improving the transmission rate.
  A simple model is presented for studying this information acquisition and
exploitation trade-off when the channels are multi-state, with different
distributions and information acquisition costs. The objective is to maximize a
utility function which depends on both the cost and value of information.
Solution techniques are presented for computing near-optimal policies with
succinct representation in polynomial time. These policies provably achieve at
least a fixed constant factor of the optimal utility on any problem instance,
and in addition, have natural characterizations. The techniques are based on
exploiting the structure of the optimal policy, and use of Lagrangean
relaxations which simplify the space of approximately optimal solutions.",
merger and acquisition transaction costs,http://arxiv.org/abs/1603.02393v2,Microprocessor Optimizations for the Internet of Things: A Survey,"The Internet of Things (IoT) refers to a pervasive presence of interconnected
and uniquely identifiable physical devices. These devices' goal is to gather
data and drive actions in order to improve productivity, and ultimately reduce
or eliminate reliance on human intervention for data acquisition,
interpretation, and use. The proliferation of these connected low-power devices
will result in a data explosion that will significantly increase data
transmission costs with respect to energy consumption and latency. Edge
computing reduces these costs by performing computations at the edge nodes,
prior to data transmission, to interpret and/or utilize the data. While much
research has focused on the IoT's connected nature and communication
challenges, the challenges of IoT embedded computing with respect to device
microprocessors has received much less attention. This paper explores IoT
applications' execution characteristics from a microarchitectural perspective
and the microarchitectural characteristics that will enable efficient and
effective edge computing. To tractably represent a wide variety of
next-generation IoT applications, we present a broad IoT application
classification methodology based on application functions, to enable quicker
workload characterizations for IoT microprocessors. We then survey and discuss
potential microarchitectural optimizations and computing paradigms that will
enable the design of right-provisioned microprocessors that are efficient,
configurable, extensible, and scalable. This paper provides a foundation for
the analysis and design of a diverse set of microprocessor architectures for
next-generation IoT devices.",
merger and acquisition transaction costs,http://arxiv.org/abs/1511.03812v1,"Channel Acquisition for Massive MIMO-OFDM with Adjustable Phase Shift
  Pilots","We propose adjustable phase shift pilots (APSPs) for channel acquisition in
wideband massive multiple-input multiple-output (MIMO) systems employing
orthogonal frequency division multiplexing (OFDM) to reduce the pilot overhead.
Based on a physically motivated channel model, we first establish a
relationship between channel space-frequency correlations and the channel power
angle-delay spectrum in the massive antenna array regime, which reveals the
channel sparsity in massive MIMO-OFDM. With this channel model, we then
investigate channel acquisition, including channel estimation and channel
prediction, for massive MIMO-OFDM with APSPs. We show that channel acquisition
performance in terms of sum mean square error can be minimized if the user
terminals' channel power distributions in the angle-delay domain can be made
non-overlapping with proper phase shift scheduling. A simplified pilot phase
shift scheduling algorithm is developed based on this optimal channel
acquisition condition. The performance of APSPs is investigated for both one
symbol and multiple symbol data models. Simulations demonstrate that the
proposed APSP approach can provide substantial performance gains in terms of
achievable spectral efficiency over the conventional phase shift orthogonal
pilot approach in typical mobility scenarios.","IEEE Transactions on Signal Processing, vol. 64, no. 6, pp.
  1461--1476, Mar. 2016"
merger and acquisition transaction costs,http://arxiv.org/abs/1907.09163v2,"Full Reciprocity-Gap Waveform Inversion in the frequency domain,
  enabling sparse-source acquisition","We perform quantitative sub-surface Earth-imaging by setting up the Full
Reciprocity-gap Waveform Inversion (FRgWI ) method. The reconstruction relies
on iterative minimization of a misfit functional which is specifically designed
for data obtained from dual-sensors devices. In addition to the pressure field,
the dual-sensors devices are able to measure the normal velocity of the waves
and have been deployed in exploration geophysics. The use of reciprocity-based
misfit functional provides additional features compared to the more traditional
least-squares approaches with, in particular, that the observational and
computational acquisitions can be different. Therefore, the source positions
and wavelets that generate the measurements are not needed for the
reconstruction procedure and, in fact, the numerical acquisition (for the
simulations) can be arbitrarily chosen. We illustrate our method with
three-dimensional experiments, where we first show that the reciprocity-gap
inversion behaves better than the Full Waveform Inversion (FWI) in the same
context. Next, we investigate arbitrary numerical acquisitions in two ways:
firstly, when few measurements are given, we use a dense numerical acquisition
(compared to the observational one). On the other hand, with a dense
observational acquisition, we employ a sparse computational acquisition, with
multiple-point sources, to reduce the numerical cost. We highlight that the
reciprocity-gap functional is efficient in both situations and is more robust
with respect to cross-talk than shot-stacking.",
merger and acquisition transaction costs,http://arxiv.org/abs/1812.09640v1,"Estimating Rationally Inattentive Utility Functions with Deep Clustering
  for Framing - Applications in YouTube Engagement Dynamics","We consider a framework involving behavioral economics and machine learning.
Rationally inattentive Bayesian agents make decisions based on their posterior
distribution, utility function and information acquisition cost Renyi
divergence which generalizes Shannon mutual information). By observing these
decisions, how can an observer estimate the utility function and information
acquisition cost? Using deep learning, we estimate framing information
(essential extrinsic features) that determines the agent's attention strategy.
Then we present a preference based inverse reinforcement learning algorithm to
test for rational inattention: is the agent an utility maximizer, attention
maximizer, and does an information cost function exist that rationalizes the
data? The test imposes a Renyi mutual information constraint which impacts how
the agent can select attention strategies to maximize their expected utility.
The test provides constructive estimates of the utility function and
information acquisition cost of the agent. We illustrate these methods on a
massive YouTube dataset for characterizing the commenting behavior of users.",
merger and acquisition transaction costs,http://arxiv.org/abs/1611.04959v1,"Data Acquisition with GPUs: The DAQ for the Muon $g$-$2$ Experiment at
  Fermilab","Graphical Processing Units (GPUs) have recently become a valuable computing
tool for the acquisition of data at high rates and for a relatively low cost.
The devices work by parallelizing the code into thousands of threads, each
executing a simple process, such as identifying pulses from a waveform
digitizer. The CUDA programming library can be used to effectively write code
to parallelize such tasks on Nvidia GPUs, providing a significant upgrade in
performance over CPU based acquisition systems.
  The muon $g$-$2$ experiment at Fermilab is heavily relying on GPUs to process
its data. The data acquisition system for this experiment must have the ability
to create deadtime-free records from 700 $\mu$s muon spills at a raw data rate
18 GB per second. Data will be collected using 1296 channels of $\mu$TCA-based
800 MSPS, 12 bit waveform digitizers and processed in a layered array of
networked commodity processors with 24 GPUs working in parallel to perform a
fast recording of the muon decays during the spill. The described data
acquisition system is currently being constructed, and will be fully
operational before the start of the experiment in 2017.",
merger and acquisition transaction costs,http://arxiv.org/abs/1407.6876v2,On Partial Wait-Freedom in Transactional Memory,"Transactional memory (TM) is a convenient synchronization tool that allows
concurrent threads to declare sequences of instructions on shared data as
speculative \emph{transactions} with ""all-or-nothing"" semantics. It is known
that dynamic transactional memory cannot provide \emph{wait-free} progress in
the sense that every transaction commits in a finite number of its own steps.
In this paper, we explore the costs of providing wait-freedom to only a
\emph{subset} of transactions. Since most transactional workloads are believed
to be read-dominated, we require that read-only transactions commit in the
wait-free manner, while updating transactions are guaranteed to commit only if
they run in the absence of concurrency. We show that this kind of partial
wait-freedom, combined with attractive requirements like read invisibility or
disjoint-access parallelism, incurs considerable complexity costs.",
merger and acquisition transaction costs,http://arxiv.org/abs/1906.10385v1,"Ordinary Low Alpha Proportional Counter with Low Cost Commercial Data
  Acquisition System","In this study, we present a low cost and commercially available data
acquisition system (DAQ) for an ordinary low alpha proportional counter. By
employing this DAQ system and aid of a simple physical model, we can easily
rule out the common disadvantage of proportional type low alpha counters, such
as sensitive to electromagnetic interference and vibration. The obtained
results demonstrated that this method has improved the capability of an
ordinary low alpha counter and even makes it easier to operate in a worse
ground-loop laboratory.",
merger and acquisition transaction costs,http://arxiv.org/abs/1902.07102v2,Cost-Sensitive Diagnosis and Learning Leveraging Public Health Data,"Traditionally, machine learning algorithms rely on the assumption that all
features of a given dataset are available for free. However, there are many
concerns such as monetary data collection costs, patient discomfort in medical
procedures, and privacy impacts of data collection that require careful
consideration in any real-world health analytics system. An efficient solution
would only acquire a subset of features based on the value it provides while
considering acquisition costs. Moreover, datasets that provide feature costs
are very limited, especially in healthcare. In this paper, we provide a health
dataset as well as a method for assigning feature costs based on the total
level of inconvenience asking for each feature entails. Furthermore, based on
the suggested dataset, we provide a comparison of recent and state-of-the-art
approaches to cost-sensitive feature acquisition and learning. Specifically, we
analyze the performance of major sensitivity-based and reinforcement learning
based methods in the literature on three different problems in the health
domain, including diabetes, heart disease, and hypertension classification.",
merger and acquisition transaction costs,http://arxiv.org/abs/1811.12204v1,"Chiller: Contention-centric Transaction Execution and Data Partitioning
  for Fast Networks","Distributed transactions on high-overhead TCP/IP-based networks were
conventionally considered to be prohibitively expensive and thus were avoided
at all costs. To that end, the primary goal of almost any existing partitioning
scheme is to minimize the number of cross-partition transactions. However, with
the next generation of fast RDMA-enabled networks, this assumption is no longer
valid. In fact, recent work has shown that distributed databases can scale even
when the majority of transactions are cross-partition.
  In this paper, we first make the case that the new bottleneck which hinders
truly scalable transaction processing in modern RDMA-enabled databases is data
contention, and that optimizing for data contention leads to different
partitioning layouts than optimizing for the number of distributed
transactions. We then present Chiller, a new approach to data partitioning and
transaction execution, which minimizes data contention for both local and
distributed transactions. Finally, we evaluate Chiller using TPC-C and a
real-world workload, and show that our partitioning and execution strategy
outperforms traditional partitioning techniques which try to avoid distributed
transactions, by up to a factor of 2 under the same conditions.",
merger and acquisition transaction costs,http://arxiv.org/abs/1607.03691v1,Sequential Cost-Sensitive Feature Acquisition,"We propose a reinforcement learning based approach to tackle the
cost-sensitive learning problem where each input feature has a specific cost.
The acquisition process is handled through a stochastic policy which allows
features to be acquired in an adaptive way. The general architecture of our
approach relies on representation learning to enable performing prediction on
any partially observed sample, whatever the set of its observed features are.
The resulting model is an original mix of representation learning and of
reinforcement learning ideas. It is learned with policy gradient techniques to
minimize a budgeted inference cost. We demonstrate the effectiveness of our
proposed method with several experiments on a variety of datasets for the
sparse prediction problem where all features have the same cost, but also for
some cost-sensitive settings.",
merger and acquisition transaction costs,http://arxiv.org/abs/1909.03600v1,Cost-aware Multi-objective Bayesian optimisation,"The notion of expense in Bayesian optimisation generally refers to the
uniformly expensive cost of function evaluations over the whole search space.
However, in some scenarios, the cost of evaluation for black-box objective
functions is non-uniform since different inputs from search space may incur
different costs for function evaluations. We introduce a cost-aware
multi-objective Bayesian optimisation with non-uniform evaluation cost over
objective functions by defining cost-aware constraints over the search space.
The cost-aware constraints are a sorted tuple of indexes that demonstrate the
ordering of dimensions of the search space based on the user's prior knowledge
about their cost of usage. We formulate a new multi-objective Bayesian
optimisation acquisition function with detailed analysis of the convergence
that incorporates this cost-aware constraints while optimising the objective
functions. We demonstrate our algorithm based on synthetic and real-world
problems in hyperparameter tuning of neural networks and random forests.",
merger and acquisition transaction costs,http://arxiv.org/abs/1703.02722v1,"Scaling Distributed Transaction Processing and Recovery based on
  Dependency Logging","DGCC protocol has been shown to achieve good performance on multi-core
in-memory system. However, distributed transactions complicate the dependency
resolution, and therefore, an effective transaction partitioning strategy is
essential to reduce expensive multi-node distributed transactions. During
failure recovery, log must be examined from the last checkpoint onwards and the
affected transactions are re-executed based on the way they are partitioned and
executed. Existing approaches treat both transaction management and recovery as
two separate problems, even though recovery is dependent on the sequence in
which transactions are executed.
  In this paper, we propose to treat the transaction management and recovery
problems as one. We first propose an efficient Distributed Dependency Graph
based Concurrency Control (DistDGCC) protocol for handling transactions
spanning multiple nodes, and propose a new novel and efficient logging protocol
called Dependency Logging that also makes use of dependency graphs for
efficient logging and recovery. DistDGCC optimizes the average cost for each
distributed transaction by processing transactions in batch. Moreover, it also
reduces the effects of thread blocking caused by distributed transactions and
consequently improves the runtime performance. Further, dependency logging
exploits the same data structure that is used by DistDGCC to reduce the logging
overhead, as well as the logical dependency information to improve the recovery
parallelism. Extensive experiments are conducted to evaluate the performance of
our proposed technique against state-of-the-art techniques. Experimental
results show that DistDGCC is efficient and scalable, and dependency logging
supports fast recovery with marginal runtime overhead. Hence, the overall
system performance is significantly improved as a result.",
merger and acquisition transaction costs,http://arxiv.org/abs/1711.07617v2,Dynamic Distributed Storage for Scaling Blockchains,"Blockchain uses the idea of storing transaction data in the form of a
distributed ledger wherein each node in the network stores a current copy of
the sequence of transactions in the form of a hash chain. This requirement of
storing the entire ledger incurs a high storage cost that grows undesirably
large for high transaction rates and large networks. In this work we use the
ideas of secret key sharing, private key encryption, and distributed storage to
design a coding scheme such that each node stores only a part of the entire
transaction thereby reducing the storage cost to a fraction of its original
cost. When further using dynamic zone allocation, we show the coding scheme can
also improve the integrity of the transaction data in the network over current
schemes. Further, block validation (bitcoin mining) consumes a significant
amount of energy as it is necessary to determine a hash value satisfying a
specific set of constraints; we show that using dynamic distributed storage
reduces these energy costs.",
merger and acquisition transaction costs,http://arxiv.org/abs/1806.08554v1,Learning-to-Ask: Knowledge Acquisition via 20 Questions,"Almost all the knowledge empowered applications rely upon accurate knowledge,
which has to be either collected manually with high cost, or extracted
automatically with unignorable errors. In this paper, we study 20 Questions, an
online interactive game where each question-response pair corresponds to a fact
of the target entity, to acquire highly accurate knowledge effectively with
nearly zero labor cost. Knowledge acquisition via 20 Questions predominantly
presents two challenges to the intelligent agent playing games with human
players. The first one is to seek enough information and identify the target
entity with as few questions as possible, while the second one is to leverage
the remaining questioning opportunities to acquire valuable knowledge
effectively, both of which count on good questioning strategies. To address
these challenges, we propose the Learning-to-Ask (LA) framework, within which
the agent learns smart questioning strategies for information seeking and
knowledge acquisition by means of deep reinforcement learning and generalized
matrix factorization respectively. In addition, a Bayesian approach to
represent knowledge is adopted to ensure robustness to noisy user responses.
Simulating experiments on real data show that LA is able to equip the agent
with effective questioning strategies, which result in high winning rates and
rapid knowledge acquisition. Moreover, the questioning strategies for
information seeking and knowledge acquisition boost the performance of each
other, allowing the agent to start with a relatively small knowledge set and
quickly improve its knowledge base in the absence of constant human
supervision.",
merger and acquisition transaction costs,http://arxiv.org/abs/1906.04077v1,"An Aristotelian view on MR-based attenuation correction (ARISTOMRAC):
  combining the four elements","MR-based attenuation correction (MRAC) is important for accurate
quantification of the uptake of PET tracers in combined PET/MR scanners.
However, current techniques for MRAC usually require multiple acquisitions or
complex post-processing to discriminate the different tissues. Inspired by the
ancient Greeks, who believed that matter was made of the combination of four
elements (earth, water, air and fire), we formulated a multi-component Magnetic
Resonance (MR) Fingerprinting framework, where every voxel was considered a
weighted combination of four base elements: bone, water, air and fat. We named
our approach Aristotelian MR based attenuation correction (ARISTOMRAC). We used
a 3D radial acquisition scheme at 1.5T, acquiring a transient-state spoiled
acquisition with variable flip angles and echo times (TE), with the shortest
TEs being ultra-short echo times (UTE). We simulated a multi-tissue MR signal
model using the Bloch equations and used dictionary matching to extract tissue
fraction maps for bone water and fat, while air fractions were obtained by
thresholding the UTE parts of our acquisitions at higher spatial resolution.
Compared to previous methods for MR-based Attenuation Correction (MRAC), our
approach used a full multi-component signal model, including multiple tissues
per voxel. For this reason, rather than reconstructing high resolutions images,
MR data can be acquired more efficiently, directly at the resolution needed for
PET attenuation maps. The ARISTOMRAC method allows to accurately estimate the
air, water, bone and fat fractions (Concordance Correlation Coefficient =
0.81/0.91/0.98 for bone, water and fat respectively). Attenuation maps could be
obtained in the head and neck with a single 1-minute acquisition.",
merger and acquisition transaction costs,http://arxiv.org/abs/1404.3768v1,Changing Bases: Multistage Optimization for Matroids and Matchings,"This paper is motivated by the fact that many systems need to be maintained
continually while the underlying costs change over time. The challenge is to
continually maintain near-optimal solutions to the underlying optimization
problems, without creating too much churn in the solution itself. We model this
as a multistage combinatorial optimization problem where the input is a
sequence of cost functions (one for each time step); while we can change the
solution from step to step, we incur an additional cost for every such change.
We study the multistage matroid maintenance problem, where we need to maintain
a base of a matroid in each time step under the changing cost functions and
acquisition costs for adding new elements. The online version of this problem
generalizes online paging. E.g., given a graph, we need to maintain a spanning
tree $T_t$ at each step: we pay $c_t(T_t)$ for the cost of the tree at time
$t$, and also $| T_t\setminus T_{t-1} |$ for the number of edges changed at
this step. Our main result is an $O(\log m \log r)$-approximation, where $m$ is
the number of elements/edges and $r$ is the rank of the matroid. We also give
an $O(\log m)$ approximation for the offline version of the problem. These
bounds hold when the acquisition costs are non-uniform, in which caseboth these
results are the best possible unless P=NP.
  We also study the perfect matching version of the problem, where we must
maintain a perfect matching at each step under changing cost functions and
costs for adding new elements. Surprisingly, the hardness drastically
increases: for any constant $\epsilon>0$, there is no
$O(n^{1-\epsilon})$-approximation to the multistage matching maintenance
problem, even in the offline case.",
regression analysis,http://arxiv.org/abs/1208.0219v1,Functional Mechanism: Regression Analysis under Differential Privacy,"\epsilon-differential privacy is the state-of-the-art model for releasing
sensitive information while protecting privacy. Numerous methods have been
proposed to enforce epsilon-differential privacy in various analytical tasks,
e.g., regression analysis. Existing solutions for regression analysis, however,
are either limited to non-standard types of regression or unable to produce
accurate regression results. Motivated by this, we propose the Functional
Mechanism, a differentially private method designed for a large class of
optimization-based analyses. The main idea is to enforce epsilon-differential
privacy by perturbing the objective function of the optimization problem,
rather than its results. As case studies, we apply the functional mechanism to
address two most widely used regression models, namely, linear regression and
logistic regression. Both theoretical analysis and thorough experimental
evaluations show that the functional mechanism is highly effective and
efficient, and it significantly outperforms existing solutions.","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1364-1375 (2012)"
regression analysis,http://arxiv.org/abs/1108.5592v1,"A Performance Study of Data Mining Techniques: Multiple Linear
  Regression vs. Factor Analysis","The growing volume of data usually creates an interesting challenge for the
need of data analysis tools that discover regularities in these data. Data
mining has emerged as disciplines that contribute tools for data analysis,
discovery of hidden knowledge, and autonomous decision making in many
application domains. The purpose of this study is to compare the performance of
two data mining techniques viz., factor analysis and multiple linear regression
for different sample sizes on three unique sets of data. The performance of the
two data mining techniques is compared on following parameters like mean square
error (MSE), R-square, R-Square adjusted, condition number, root mean square
error(RMSE), number of variables included in the prediction model, modified
coefficient of efficiency, F-value, and test of normality. These parameters
have been computed using various data mining tools like SPSS, XLstat, Stata,
and MS-Excel. It is seen that for all the given dataset, factor analysis
outperform multiple linear regression. But the absolute value of prediction
accuracy varied between the three datasets indicating that the data
distribution and data characteristics play a major role in choosing the correct
prediction technique.",
regression analysis,http://arxiv.org/abs/0908.0570v1,The Infinite Hierarchical Factor Regression Model,"We propose a nonparametric Bayesian factor regression model that accounts for
uncertainty in the number of factors, and the relationship between factors. To
accomplish this, we propose a sparse variant of the Indian Buffet Process and
couple this with a hierarchical model over factors, based on Kingman's
coalescent. We apply this model to two problems (factor analysis and factor
regression) in gene-expression data analysis.",NIPS 2008
regression analysis,http://arxiv.org/abs/0910.4627v1,Self-concordant analysis for logistic regression,"Most of the non-asymptotic theoretical work in regression is carried out for
the square loss, where estimators can be obtained through closed-form
expressions. In this paper, we use and extend tools from the convex
optimization literature, namely self-concordant functions, to provide simple
extensions of theoretical results for the square loss to the logistic loss. We
apply the extension techniques to logistic regression with regularization by
the $\ell_2$-norm and regularization by the $\ell_1$-norm, showing that new
results for binary classification through logistic regression can be easily
derived from corresponding results for least-squares regression.",
regression analysis,http://arxiv.org/abs/1307.1903v1,"Achieving greater Explanatory Power and Forecasting Accuracy with
  Non-uniform spread Fuzzy Linear Regression","Fuzzy regression models have been applied to several Operations Research
applications viz., forecasting and prediction. Earlier works on fuzzy
regression analysis obtain crisp regression coefficients for eliminating the
problem of increasing spreads for the estimated fuzzy responses as the
magnitude of the independent variable increases. But they cannot deal with the
problem of non-uniform spreads. In this work, a three-phase approach is
discussed to construct the fuzzy regression model with non-uniform spreads to
deal with this problem. The first phase constructs the membership functions of
the least-squares estimates of regression coefficients based on extension
principle to completely conserve the fuzziness of observations. They are then
defuzzified by the centre of area method to obtain crisp regression
coefficients in the second phase. Finally, the error terms of the method are
determined by setting each estimated spread equal to its corresponding observed
spread. The Tagaki-Sugeno inference system is used for improving the accuracy
of forecasts. The simulation example demonstrates the strength of fuzzy linear
regression model in terms of higher explanatory power and forecasting
performance.",
regression analysis,http://arxiv.org/abs/1405.1207v1,"Nuclear Norm based Matrix Regression with Applications to Face
  Recognition with Occlusion and Illumination Changes","Recently regression analysis becomes a popular tool for face recognition. The
existing regression methods all use the one-dimensional pixel-based error
model, which characterizes the representation error pixel by pixel individually
and thus neglects the whole structure of the error image. We observe that
occlusion and illumination changes generally lead to a low-rank error image. To
make use of this low-rank structural information, this paper presents a
two-dimensional image matrix based error model, i.e. matrix regression, for
face representation and classification. Our model uses the minimal nuclear norm
of representation error image as a criterion, and the alternating direction
method of multipliers method to calculate the regression coefficients. Compared
with the current regression methods, the proposed Nuclear Norm based Matrix
Regression (NMR) model is more robust for alleviating the effect of
illumination, and more intuitive and powerful for removing the structural noise
caused by occlusion. We experiment using four popular face image databases, the
Extended Yale B database, the AR database, the Multi-PIE and the FRGC database.
Experimental results demonstrate the performance advantage of NMR over the
state-of-the-art regression based face recognition methods.",
regression analysis,http://arxiv.org/abs/1412.5272v1,Consistency Analysis of an Empirical Minimum Error Entropy Algorithm,"In this paper we study the consistency of an empirical minimum error entropy
(MEE) algorithm in a regression setting. We introduce two types of consistency.
The error entropy consistency, which requires the error entropy of the learned
function to approximate the minimum error entropy, is shown to be always true
if the bandwidth parameter tends to 0 at an appropriate rate. The regression
consistency, which requires the learned function to approximate the regression
function, however, is a complicated issue. We prove that the error entropy
consistency implies the regression consistency for homoskedastic models where
the noise is independent of the input variable. But for heteroskedastic models,
a counterexample is used to show that the two types of consistency do not
coincide. A surprising result is that the regression consistency is always
true, provided that the bandwidth parameter tends to infinity at an appropriate
rate. Regression consistency of two classes of special models is shown to hold
with fixed bandwidth parameter, which further illustrates the complexity of
regression consistency of MEE. Fourier transform plays crucial roles in our
analysis.",
regression analysis,http://arxiv.org/abs/1708.02052v1,VART: A Tool for the Automatic Detection of Regression Faults,"In this paper we present VART, a tool for automatically revealing regression
faults missed by regression test suites. Interestingly, VART is not limited to
faults causing crashing or exceptions, but can reveal faults that cause the
violation of application-specific correctness properties. VART achieves this
goal by combining static and dynamic program analysis.",
regression analysis,http://arxiv.org/abs/0904.0814v1,"Stability Analysis and Learning Bounds for Transductive Regression
  Algorithms","This paper uses the notion of algorithmic stability to derive novel
generalization bounds for several families of transductive regression
algorithms, both by using convexity and closed-form solutions. Our analysis
helps compare the stability of these algorithms. It also shows that a number of
widely used transductive regression algorithms are in fact unstable. Finally,
it reports the results of experiments with local transductive regression
demonstrating the benefit of our stability bounds for model selection, for one
of the algorithms, in particular for determining the radius of the local
neighborhood used by the algorithm.",
regression analysis,http://arxiv.org/abs/1109.5311v1,Bias Plus Variance Decomposition for Survival Analysis Problems,"Bias - variance decomposition of the expected error defined for regression
and classification problems is an important tool to study and compare different
algorithms, to find the best areas for their application. Here the
decomposition is introduced for the survival analysis problem. In our
experiments, we study bias -variance parts of the expected error for two
algorithms: original Cox proportional hazard regression and CoxPath, path
algorithm for L1-regularized Cox regression, on the series of increased
training sets. The experiments demonstrate that, contrary expectations, CoxPath
does not necessarily have an advantage over Cox regression.",
regression analysis,http://arxiv.org/abs/1612.05740v1,"Machine Learning, Linear and Bayesian Models for Logistic Regression in
  Failure Detection Problems","In this work, we study the use of logistic regression in manufacturing
failures detection. As a data set for the analysis, we used the data from
Kaggle competition Bosch Production Line Performance. We considered the use of
machine learning, linear and Bayesian models. For machine learning approach, we
analyzed XGBoost tree based classifier to obtain high scored classification.
Using the generalized linear model for logistic regression makes it possible to
analyze the influence of the factors under study. The Bayesian approach for
logistic regression gives the statistical distribution for the parameters of
the model. It can be useful in the probabilistic analysis, e.g. risk
assessment.",
regression analysis,http://arxiv.org/abs/1701.01218v1,Overlapping Cover Local Regression Machines,"We present the Overlapping Domain Cover (ODC) notion for kernel machines, as
a set of overlapping subsets of the data that covers the entire training set
and optimized to be spatially cohesive as possible. We show how this notion
benefit the speed of local kernel machines for regression in terms of both
speed while achieving while minimizing the prediction error. We propose an
efficient ODC framework, which is applicable to various regression models and
in particular reduces the complexity of Twin Gaussian Processes (TGP)
regression from cubic to quadratic. Our notion is also applicable to several
kernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as
shown in our experiments). We also theoretically justified the idea behind our
method to improve local prediction by the overlapping cover. We validated and
analyzed our method on three benchmark human pose estimation datasets and
interesting findings are discussed.",
regression analysis,http://arxiv.org/abs/1712.09473v1,Sketching for Kronecker Product Regression and P-splines,"TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later
used in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was
shown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace
embedding, and therefore can be used for canonical correlation analysis, low
rank approximation, and principal component regression for the polynomial
kernel. We take TensorSketch outside of the context of polynomials kernels, and
show its utility in applications in which the underlying design matrix is a
Kronecker product of smaller matrices. This allows us to solve Kronecker
product regression and non-negative Kronecker product regression, as well as
regularized spline regression. Our main technical result is then in extending
TensorSketch to other norms. That is, TensorSketch only provides input sparsity
time for Kronecker product regression with respect to the $2$-norm. We show how
to solve Kronecker product regression with respect to the $1$-norm in time
sublinear in the time required for computing the Kronecker product, as well as
for more general $p$-norms.",
regression analysis,http://arxiv.org/abs/1804.06504v2,Learning how to be robust: Deep polynomial regression,"Polynomial regression is a recurrent problem with a large number of
applications. In computer vision it often appears in motion analysis. Whatever
the application, standard methods for regression of polynomial models tend to
deliver biased results when the input data is heavily contaminated by outliers.
Moreover, the problem is even harder when outliers have strong structure.
Departing from problem-tailored heuristics for robust estimation of parametric
models, we explore deep convolutional neural networks. Our work aims to find a
generic approach for training deep regression models without the explicit need
of supervised annotation. We bypass the need for a tailored loss function on
the regression parameters by attaching to our model a differentiable hard-wired
decoder corresponding to the polynomial operation at hand. We demonstrate the
value of our findings by comparing with standard robust regression methods.
Furthermore, we demonstrate how to use such models for a real computer vision
problem, i.e., video stabilization. The qualitative and quantitative
experiments show that neural networks are able to learn robustness for general
polynomial regression, with results that well overpass scores of traditional
robust estimation methods.",
regression analysis,http://arxiv.org/abs/1805.04737v1,"Offline EEG-Based Driver Drowsiness Estimation Using Enhanced Batch-Mode
  Active Learning (EBMAL) for Regression","There are many important regression problems in real-world brain-computer
interface (BCI) applications, e.g., driver drowsiness estimation from EEG
signals. This paper considers offline analysis: given a pool of unlabeled EEG
epochs recorded during driving, how do we optimally select a small number of
them to label so that an accurate regression model can be built from them to
label the rest? Active learning is a promising solution to this problem, but
interestingly, to our best knowledge, it has not been used for regression
problems in BCI so far. This paper proposes a novel enhanced batch-mode active
learning (EBMAL) approach for regression, which improves upon a baseline active
learning algorithm by increasing the reliability, representativeness and
diversity of the selected samples to achieve better regression performance. We
validate its effectiveness using driver drowsiness estimation from EEG signals.
However, EBMAL is a general approach that can also be applied to many other
offline regression problems beyond BCI.",
regression analysis,http://arxiv.org/abs/1901.07643v1,"Solving All Regression Models For Learning Gaussian Networks Using
  Givens Rotations","Score based learning (SBL) is a promising approach for learning Bayesian
networks. The initial step in the majority of the SBL algorithms consists of
computing the scores of all possible child and parent-set combinations for the
variables. For Bayesian networks with continuous variables, a particular score
is usually calculated as a function of the regression of the child over the
variables in the parent-set. The sheer number of regressions models to be
solved necessitates the design of efficient numerical algorithms. In this
paper, we propose an algorithm for an efficient and exact calculation of
regressions for all child and parent-set combinations. In the proposed
algorithm, we use QR decompositions (QRDs) to capture the dependencies between
the regressions for different families and Givens rotations to efficiently
traverse through the space of QRDs such that all the regression models are
accounted for in the shortest path possible. We compare the complexity of the
suggested method with different algorithms, mainly those arising in all subset
regression problems, and show that our algorithm has the smallest algorithmic
complexity. We also explain how to parallelize the proposed method so as to
decrease the runtime by a factor proportional to the number of processors
utilized.",
regression analysis,http://arxiv.org/abs/1312.2789v1,"Performance Analysis Of Regularized Linear Regression Models For
  Oxazolines And Oxazoles Derivitive Descriptor Dataset","Regularized regression techniques for linear regression have been created the
last few ten years to reduce the flaws of ordinary least squares regression
with regard to prediction accuracy. In this paper, new methods for using
regularized regression in model choice are introduced, and we distinguish the
conditions in which regularized regression develops our ability to discriminate
models. We applied all the five methods that use penalty-based (regularization)
shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with
far more predictors than observations. The lasso, ridge, elasticnet, lars and
relaxed lasso further possess the desirable property that they simultaneously
select relevant predictive descriptors and optimally estimate their effects.
Here, we comparatively evaluate the performance of five regularized linear
regression methods The assessment of the performance of each model by means of
benchmark experiments is an established exercise. Cross-validation and
resampling methods are generally used to arrive point evaluates the
efficiencies which are compared to recognize methods with acceptable features.
Predictive accuracy was evaluated using the root mean squared error (RMSE) and
Square of usual correlation between predictors and observed mean inhibitory
concentration of antitubercular activity (R square). We found that all five
regularized regression models were able to produce feasible models and
efficient capturing the linearity in the data. The elastic net and lars had
similar accuracies as well as lasso and relaxed lasso had similar accuracies
but outperformed ridge regression in terms of the RMSE and R square metrics.","published International Journal of Computational Science and
  Information Technology (IJCSITY) Vol.1, No.4, November 2013"
regression analysis,http://arxiv.org/abs/1704.06498v3,Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces,"Graph models are relevant in many fields, such as distributed computing,
intelligent tutoring systems or social network analysis. In many cases, such
models need to take changes in the graph structure into account, i.e. a varying
number of nodes or edges. Predicting such changes within graphs can be expected
to yield important insight with respect to the underlying dynamics, e.g. with
respect to user behaviour. However, predictive techniques in the past have
almost exclusively focused on single edges or nodes. In this contribution, we
attempt to predict the future state of a graph as a whole. We propose to phrase
time series prediction as a regression problem and apply dissimilarity- or
kernel-based regression techniques, such as 1-nearest neighbor, kernel
regression and Gaussian process regression, which can be applied to graphs via
graph kernels. The output of the regression is a point embedded in a
pseudo-Euclidean space, which can be analyzed using subsequent dissimilarity-
or kernel-based processing methods. We discuss strategies to speed up Gaussian
Processes regression from cubic to linear time and evaluate our approach on two
well-established theoretical models of graph evolution as well as two real data
sets from the domain of intelligent tutoring systems. We find that simple
regression methods, such as kernel regression, are sufficient to capture the
dynamics in the theoretical models, but that Gaussian process regression
significantly improves the prediction error for real-world data.",Neural Processing Letters 48 (2018) 669-689
regression analysis,http://arxiv.org/abs/1705.07853v2,Nonparametric Online Regression while Learning the Metric,"We study algorithms for online nonparametric regression that learn the
directions along which the regression function is smoother. Our algorithm
learns the Mahalanobis metric based on the gradient outer product matrix
$\boldsymbol{G}$ of the regression function (automatically adapting to the
effective rank of this matrix), while simultaneously bounding the regret ---on
the same data sequence--- in terms of the spectrum of $\boldsymbol{G}$. As a
preliminary step in our analysis, we extend a nonparametric online learning
algorithm by Hazan and Megiddo enabling it to compete against functions whose
Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.",
regression analysis,http://arxiv.org/abs/1908.08855v1,A Robust Regression Approach for Robot Model Learning,"Machine learning and data analysis have been used in many robotics fields,
especially for modelling. Data are usually the result of sensor measurements
and, as such, they might be subjected to noise and outliers. The presence of
outliers has a huge impact on modelling the acquired data, resulting in
inappropriate models. In this work a novel approach for outlier detection and
rejection for input/output mapping in regression problems is presented. The
robustness of the method is shown both through simulated data for linear and
nonlinear regression, and real sensory data. Despite being validated by using
artificial neural networks, the method can be generalized to any other
regression method",
regression analysis,http://arxiv.org/abs/1402.5466v1,"Predictive Comparative QSAR analysis of Sulfathiazole Analogues as
  Mycobacterium Tuberculosis H37RV Inhabitors","Antitubercular activity of Sulfathiazole Derivitives series were subjected to
Quantitative Structure Activity Relationship (QSAR) Analysis with an attempt to
derive and understand a correlation between the Biologically Activity as
dependent variable and various descriptors as independent variables. QSAR
models generated using 28 compounds. Several statistical regression expressions
were obtained using Partial Least Squares (PLS) Regression, Multiple Linear
Regression (MLR) and Principal Component Regression (PCR) methods. The among
these methods, Partial Least Square Regression (PLS) method has shown very
promising result as compare to other two methods. A QSAR model was generated by
a training set of 18 molecules with correlation coefficient r (r square) of
0.9191, significant cross validated correlation coefficient (q square) of
0.8300, F test of 53.5783, r square for external test set pred_r square
-3.6132, coefficient of correlation of predicted data set pred_r_se square
1.4859 and degree of freedom 14 by Partial Least Squares Regression Method.",published 2012
regression analysis,http://arxiv.org/abs/1708.01960v1,"Learning Theory of Distributed Regression with Bias Corrected
  Regularization Kernel Network","Distributed learning is an effective way to analyze big data. In distributed
regression, a typical approach is to divide the big data into multiple blocks,
apply a base regression algorithm on each of them, and then simply average the
output functions learnt from these blocks. Since the average process will
decrease the variance, not the bias, bias correction is expected to improve the
learning performance if the base regression algorithm is a biased one.
Regularization kernel network is an effective and widely used method for
nonlinear regression analysis. In this paper we will investigate a bias
corrected version of regularization kernel network. We derive the error bounds
when it is applied to a single data set and when it is applied as a base
algorithm in distributed regression. We show that, under certain appropriate
conditions, the optimal learning rates can be reached in both situations.",
regression analysis,http://arxiv.org/abs/1811.01506v3,"Theoretical and Experimental Analysis on the Generalizability of
  Distribution Regression Network","There is emerging interest in performing regression between distributions. In
contrast to prediction on single instances, these machine learning methods can
be useful for population-based studies or on problems that are inherently
statistical in nature. The recently proposed distribution regression network
(DRN) has shown superior performance for the distribution-to-distribution
regression task compared to conventional neural networks. However, in Kou et
al. (2018) and some other works on distribution regression, there is a lack of
comprehensive comparative study on both theoretical basis and generalization
abilities of the methods. We derive some mathematical properties of DRN and
qualitatively compare it to conventional neural networks. We also perform
comprehensive experiments to study the generalizability of distribution
regression models, by studying their robustness to limited training data, data
sampling noise and task difficulty. DRN consistently outperforms conventional
neural networks, requiring fewer training data and maintaining robust
performance with noise. Furthermore, the theoretical properties of DRN can be
used to provide some explanation on the ability of DRN to achieve better
generalization performance than conventional neural networks.",
Tobin's Q,http://arxiv.org/abs/1511.01561v3,"Strong Scaling for Numerical Weather Prediction at Petascale with the
  Atmospheric Model NUMA","Numerical weather prediction (NWP) has proven to be computationally
challenging due to its inherent multiscale nature. Currently, the highest
resolution NWP models use a horizontal resolution of about 10km. In order to
increase the resolution of NWP models highly scalable atmospheric models are
needed.
  The Non-hydrostatic Unified Model of the Atmosphere (NUMA), developed by the
authors at the Naval Postgraduate School, was designed to achieve this purpose.
NUMA is used by the Naval Research Laboratory, Monterey as the engine inside
its next generation weather prediction system NEPTUNE. NUMA solves the fully
compressible Navier-Stokes equations by means of high-order Galerkin methods
(both spectral element as well as discontinuous Galerkin methods can be used).
Mesh generation is done using the p4est library. NUMA is capable of running
middle and upper atmosphere simulations since it does not make use of the
shallow-atmosphere approximation.
  This paper presents the performance analysis and optimization of the spectral
element version of NUMA. The performance at different optimization stages is
analyzed using a theoretical performance model as well as measurements via
hardware counters. Machine independent optimization is compared to machine
specific optimization using BG/Q vector intrinsics. By using vector intrinsics
the main computations reach 1.2 PFlops on the entire machine Mira (12% of the
theoretical peak performance). The paper also presents scalability studies for
two idealized test cases that are relevant for NWP applications. The
atmospheric model NUMA delivers an excellent strong scaling efficiency of 99%
on the entire supercomputer Mira using a mesh with 1.8 billion grid points.
This allows to run a global forecast of a baroclinic wave test case at 3km
uniform horizontal resolution and double precision within the time frame
required for operational weather prediction.",
Tobin's Q,http://arxiv.org/abs/1405.4572v1,A Kernel-Based Calculation of Information on a Metric Space,"Kernel density estimation is a technique for approximating probability
distributions. Here, it is applied to the calculation of mutual information on
a metric space. This is motivated by the problem in neuroscience of calculating
the mutual information between stimuli and spiking responses; the space of
these responses is a metric space. It is shown that kernel density estimation
on a metric space resembles the k-nearest-neighbor approach. This approach is
applied to a toy dataset designed to mimic electrophysiological data.","Entropy 2013, 15(10), 4540-4552"
Tobin's Q,http://arxiv.org/abs/1508.02470v1,Support for Non-conformal Meshes in PETSc's DMPlex Interface,"PETSc's DMPlex interface for unstructured meshes has been extended to support
non-conformal meshes. The topological construct that DMPlex implements---the
CW-complex---is by definition conformal, so representing non- conformal meshes
in a way that hides complexity requires careful attention to the interface
between DMPlex and numerical methods such as the finite element method. Our
approach---which combines a tree structure for subset- superset relationships
and a ""reference tree"" describing the types of non-conformal
interfaces---allows finite element code written for conformal meshes to extend
automatically: in particular, all ""hanging-node"" constraint calculations are
handled behind the scenes. We give example code demonstrating the use of this
extension, and use it to convert forests of quadtrees and forests of octrees
from the p4est library to DMPlex meshes.",
Tobin's Q,http://arxiv.org/abs/1703.06907v1,"Domain Randomization for Transferring Deep Neural Networks from
  Simulation to the Real World","Bridging the 'reality gap' that separates simulated robotics from experiments
on hardware could accelerate robotic research through improved data
availability. This paper explores domain randomization, a simple technique for
training models on simulated images that transfer to real images by randomizing
rendering in the simulator. With enough variability in the simulator, the real
world may appear to the model as just another variation. We focus on the task
of object localization, which is a stepping stone to general robotic
manipulation skills. We find that it is possible to train a real-world object
detector that is accurate to $1.5$cm and robust to distractors and partial
occlusions using only data from a simulator with non-realistic random textures.
To demonstrate the capabilities of our detectors, we show they can be used to
perform grasping in a cluttered environment. To our knowledge, this is the
first successful transfer of a deep neural network trained only on simulated
RGB images (without pre-training on real images) to the real world for the
purpose of robotic control.",
Tobin's Q,http://arxiv.org/abs/1911.04554v1,Geometry-Aware Neural Rendering,"Understanding the 3-dimensional structure of the world is a core challenge in
computer vision and robotics. Neural rendering approaches learn an implicit 3D
model by predicting what a camera would see from an arbitrary viewpoint. We
extend existing neural rendering to more complex, higher dimensional scenes
than previously possible. We propose Epipolar Cross Attention (ECA), an
attention mechanism that leverages the geometry of the scene to perform
efficient non-local operations, requiring only $O(n)$ comparisons per spatial
dimension instead of $O(n^2)$. We introduce three new simulated datasets
inspired by real-world robotics and demonstrate that ECA significantly improves
the quantitative and qualitative performance of Generative Query Networks
(GQN).","33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada"
Tobin's Q,http://arxiv.org/abs/1110.2815v2,DC readout experiment in Enhanced LIGO,"The two 4 km long gravitational wave detectors operated by the Laser
Interferometer Gravitational-wave Observatory (LIGO) were modified in 2008 to
read out the gravitational wave channel using the DC readout form of homodyne
detection and to include an optical filter cavity at the output of the
detector. As part of the upgrade to Enhanced LIGO, these modifications replaced
the radio-frequency (RF) heterodyne system used previously. We describe the
motivations for and the implementation of DC readout and the output mode
cleaner in Enhanced LIGO. We present characterizations of the system, including
measurements and models of the couplings of the noises from the laser source to
the gravitational wave readout channel. We show that noise couplings using DC
readout are improved over those for RF readout, and we find that the achieved
shot-noise-limited sensitivity is consistent with modeled results.",Class. Quantum Grav. 29 (2012) 065005
Tobin's Q,http://arxiv.org/abs/1710.06425v2,Domain Randomization and Generative Models for Robotic Grasping,"Deep learning-based robotic grasping has made significant progress thanks to
algorithmic improvements and increased data availability. However,
state-of-the-art models are often trained on as few as hundreds or thousands of
unique object instances, and as a result generalization can be a challenge.
  In this work, we explore a novel data generation pipeline for training a deep
neural network to perform grasp planning that applies the idea of domain
randomization to object synthesis. We generate millions of unique, unrealistic
procedurally generated objects, and train a deep neural network to perform
grasp planning on these objects.
  Since the distribution of successful grasps for a given object can be highly
multimodal, we propose an autoregressive grasp planning model that maps sensor
inputs of a scene to a probability distribution over possible grasps. This
model allows us to sample grasps efficiently at test time (or avoid sampling
entirely).
  We evaluate our model architecture and data generation pipeline in simulation
and the real world. We find we can achieve a $>$90% success rate on previously
unseen realistic objects at test time in simulation despite having only been
trained on random objects. We also demonstrate an 80% success rate on
real-world grasp attempts despite having only been trained on random simulated
objects.",
Tobin's Q,http://arxiv.org/abs/1711.00189v1,A generalized concatenation construction for q-ary 1-perfect codes,"We consider perfect 1-error correcting codes over a finite field with $q$
elements (briefly $q$-ary 1-perfect codes). In this paper, a generalized
concatenation construction for $q$-ary 1-perfect codes is presented that allows
us to construct $q$-ary 1-perfect codes of length $(q - 1)nm + n + m$ from the
given $q$-ary 1-perfect codes of length $n =(q^{s_1} - 1) / (q - 1)$ and $m =
(q^{s_2} - 1) / (q - 1)$, where $s_1, s_2$ are natural numbers not less than
two. This construction allows us to also construct $q$-ary codes with
parameters $(q^{s_1 + s_2}, q^{q^{s_1 + s_2} - (s_1 + s_2) - 1}, 3)_q$ and can
be regarded as a $q$-ary analogue of the well-known Phelps construction.",
Tobin's Q,http://arxiv.org/abs/1711.01569v1,"Double Q($σ$) and Q($σ, λ$): Unifying Reinforcement
  Learning Control Algorithms","Temporal-difference (TD) learning is an important field in reinforcement
learning. Sarsa and Q-Learning are among the most used TD algorithms. The
Q($\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper
extends the Q($\sigma$) algorithm to an online multi-step algorithm Q($\sigma,
\lambda$) using eligibility traces and introduces Double Q($\sigma$) as the
extension of Q($\sigma$) to double learning. Experiments suggest that the new
Q($\sigma, \lambda$) algorithm can outperform the classical TD control methods
Sarsa($\lambda$), Q($\lambda$) and Q($\sigma$).",
Tobin's Q,http://arxiv.org/abs/1903.05265v1,MDS codes over finite fields,"The mds (maximum distance separable) conjecture claims that a nontrivial
linear mds $[n,k]$ code over the finite field $GF(q)$ satisfies $n \leq (q +
1)$, except when $q$ is even and $k = 3$ or $k = q- 1$ in which case it
satisfies $n \leq (q + 2)$.
  For given field $GF(q)$ and any given $k$, series of mds $[q+1,k]$ codes are
constructed.
  Any $[n,3]$ mds or $[n,n-3]$ mds code over $GF(q)$ must satisfy $n\leq (q+1)$
for $q$ odd and $n\leq (q+2)$ for $q$ even. For even $q$, mds $[q+2,3]$ and mds
$[q+2, q-1]$ codes are constructed over $GF(q)$.
  The codes constructed have efficient encoding and decoding algorithms.",
Tobin's Q,http://arxiv.org/abs/1610.06357v1,Another $q$-Polynomial Approach to Cyclic Codes,"Recently, a $q$-polynomial approach to the construction and analysis of
cyclic codes over $\gf(q)$ was given by Ding and Ling. The objective of this
paper is to give another $q$-polynomial approach to all cyclic codes over
$\gf(q)$.",
